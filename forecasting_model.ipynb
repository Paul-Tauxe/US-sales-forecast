{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sasdate</th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>DNDGRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>UMCSENTx</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/1959</td>\n",
       "      <td>2583.560</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>15.188</td>\n",
       "      <td>2.766768e+05</td>\n",
       "      <td>18235.77392</td>\n",
       "      <td>21.9616</td>\n",
       "      <td>23.3868</td>\n",
       "      <td>22.2620</td>\n",
       "      <td>31.6664</td>\n",
       "      <td>...</td>\n",
       "      <td>18.294</td>\n",
       "      <td>10.152</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>84.2043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/1/1959</td>\n",
       "      <td>2593.596</td>\n",
       "      <td>2434.8</td>\n",
       "      <td>15.346</td>\n",
       "      <td>2.787140e+05</td>\n",
       "      <td>18369.56308</td>\n",
       "      <td>22.3917</td>\n",
       "      <td>23.7024</td>\n",
       "      <td>22.4549</td>\n",
       "      <td>31.8987</td>\n",
       "      <td>...</td>\n",
       "      <td>18.302</td>\n",
       "      <td>10.167</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>83.5280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/1/1959</td>\n",
       "      <td>2610.396</td>\n",
       "      <td>2452.7</td>\n",
       "      <td>15.491</td>\n",
       "      <td>2.777753e+05</td>\n",
       "      <td>18523.05762</td>\n",
       "      <td>22.7142</td>\n",
       "      <td>23.8459</td>\n",
       "      <td>22.5651</td>\n",
       "      <td>31.8987</td>\n",
       "      <td>...</td>\n",
       "      <td>18.289</td>\n",
       "      <td>10.185</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6508.00</td>\n",
       "      <td>12349.00</td>\n",
       "      <td>81.6405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/1/1959</td>\n",
       "      <td>2627.446</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>15.435</td>\n",
       "      <td>2.833627e+05</td>\n",
       "      <td>18534.46600</td>\n",
       "      <td>23.1981</td>\n",
       "      <td>24.1903</td>\n",
       "      <td>22.8957</td>\n",
       "      <td>32.4019</td>\n",
       "      <td>...</td>\n",
       "      <td>18.300</td>\n",
       "      <td>10.221</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6620.00</td>\n",
       "      <td>12484.00</td>\n",
       "      <td>81.8099</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/1/1959</td>\n",
       "      <td>2642.720</td>\n",
       "      <td>2486.4</td>\n",
       "      <td>15.622</td>\n",
       "      <td>2.853072e+05</td>\n",
       "      <td>18679.66354</td>\n",
       "      <td>23.5476</td>\n",
       "      <td>24.3911</td>\n",
       "      <td>23.1161</td>\n",
       "      <td>32.5567</td>\n",
       "      <td>...</td>\n",
       "      <td>18.280</td>\n",
       "      <td>10.238</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.08</td>\n",
       "      <td>95.3</td>\n",
       "      <td>6753.00</td>\n",
       "      <td>12646.00</td>\n",
       "      <td>80.7315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>9/1/2024</td>\n",
       "      <td>19993.464</td>\n",
       "      <td>16283.1</td>\n",
       "      <td>121.690</td>\n",
       "      <td>1.541305e+06</td>\n",
       "      <td>716388.00000</td>\n",
       "      <td>102.5873</td>\n",
       "      <td>100.4044</td>\n",
       "      <td>100.1100</td>\n",
       "      <td>102.0602</td>\n",
       "      <td>...</td>\n",
       "      <td>119.220</td>\n",
       "      <td>128.682</td>\n",
       "      <td>31.45</td>\n",
       "      <td>36.18</td>\n",
       "      <td>28.01</td>\n",
       "      <td>70.1</td>\n",
       "      <td>553347.06</td>\n",
       "      <td>934283.59</td>\n",
       "      <td>5368.5671</td>\n",
       "      <td>17.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>10/1/2024</td>\n",
       "      <td>20067.376</td>\n",
       "      <td>16340.0</td>\n",
       "      <td>121.904</td>\n",
       "      <td>1.538666e+06</td>\n",
       "      <td>720393.00000</td>\n",
       "      <td>102.1219</td>\n",
       "      <td>99.6821</td>\n",
       "      <td>99.0178</td>\n",
       "      <td>101.4336</td>\n",
       "      <td>...</td>\n",
       "      <td>119.218</td>\n",
       "      <td>129.176</td>\n",
       "      <td>31.53</td>\n",
       "      <td>36.27</td>\n",
       "      <td>28.07</td>\n",
       "      <td>70.5</td>\n",
       "      <td>554951.25</td>\n",
       "      <td>938525.34</td>\n",
       "      <td>5407.2449</td>\n",
       "      <td>19.9478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>11/1/2024</td>\n",
       "      <td>20111.246</td>\n",
       "      <td>16385.1</td>\n",
       "      <td>122.435</td>\n",
       "      <td>1.544822e+06</td>\n",
       "      <td>725079.00000</td>\n",
       "      <td>101.9736</td>\n",
       "      <td>99.5645</td>\n",
       "      <td>99.0025</td>\n",
       "      <td>101.0038</td>\n",
       "      <td>...</td>\n",
       "      <td>119.230</td>\n",
       "      <td>129.390</td>\n",
       "      <td>31.59</td>\n",
       "      <td>36.26</td>\n",
       "      <td>28.22</td>\n",
       "      <td>71.8</td>\n",
       "      <td>556075.09</td>\n",
       "      <td>941204.79</td>\n",
       "      <td>5382.5669</td>\n",
       "      <td>15.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>12/1/2024</td>\n",
       "      <td>20136.069</td>\n",
       "      <td>16407.9</td>\n",
       "      <td>123.103</td>\n",
       "      <td>1.555153e+06</td>\n",
       "      <td>730300.00000</td>\n",
       "      <td>102.9833</td>\n",
       "      <td>100.2940</td>\n",
       "      <td>99.6550</td>\n",
       "      <td>101.3436</td>\n",
       "      <td>...</td>\n",
       "      <td>119.746</td>\n",
       "      <td>129.875</td>\n",
       "      <td>31.73</td>\n",
       "      <td>36.46</td>\n",
       "      <td>28.33</td>\n",
       "      <td>74.0</td>\n",
       "      <td>558854.68</td>\n",
       "      <td>946489.00</td>\n",
       "      <td>5370.9871</td>\n",
       "      <td>15.6997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>1/1/2025</td>\n",
       "      <td>20248.025</td>\n",
       "      <td>16464.7</td>\n",
       "      <td>122.519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>723853.00000</td>\n",
       "      <td>103.5110</td>\n",
       "      <td>101.1431</td>\n",
       "      <td>100.7228</td>\n",
       "      <td>102.1889</td>\n",
       "      <td>...</td>\n",
       "      <td>120.453</td>\n",
       "      <td>130.196</td>\n",
       "      <td>31.90</td>\n",
       "      <td>36.54</td>\n",
       "      <td>28.55</td>\n",
       "      <td>71.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5372.8381</td>\n",
       "      <td>16.8122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sasdate        RPI  W875RX1  DPCERA3M086SBEA     CMRMTSPLx  \\\n",
       "0     1/1/1959   2583.560   2426.0           15.188  2.766768e+05   \n",
       "1     2/1/1959   2593.596   2434.8           15.346  2.787140e+05   \n",
       "2     3/1/1959   2610.396   2452.7           15.491  2.777753e+05   \n",
       "3     4/1/1959   2627.446   2470.0           15.435  2.833627e+05   \n",
       "4     5/1/1959   2642.720   2486.4           15.622  2.853072e+05   \n",
       "..         ...        ...      ...              ...           ...   \n",
       "788   9/1/2024  19993.464  16283.1          121.690  1.541305e+06   \n",
       "789  10/1/2024  20067.376  16340.0          121.904  1.538666e+06   \n",
       "790  11/1/2024  20111.246  16385.1          122.435  1.544822e+06   \n",
       "791  12/1/2024  20136.069  16407.9          123.103  1.555153e+06   \n",
       "792   1/1/2025  20248.025  16464.7          122.519           NaN   \n",
       "\n",
       "          RETAILx    INDPRO   IPFPNSS   IPFINAL   IPCONGD  ...  \\\n",
       "0     18235.77392   21.9616   23.3868   22.2620   31.6664  ...   \n",
       "1     18369.56308   22.3917   23.7024   22.4549   31.8987  ...   \n",
       "2     18523.05762   22.7142   23.8459   22.5651   31.8987  ...   \n",
       "3     18534.46600   23.1981   24.1903   22.8957   32.4019  ...   \n",
       "4     18679.66354   23.5476   24.3911   23.1161   32.5567  ...   \n",
       "..            ...       ...       ...       ...       ...  ...   \n",
       "788  716388.00000  102.5873  100.4044  100.1100  102.0602  ...   \n",
       "789  720393.00000  102.1219   99.6821   99.0178  101.4336  ...   \n",
       "790  725079.00000  101.9736   99.5645   99.0025  101.0038  ...   \n",
       "791  730300.00000  102.9833  100.2940   99.6550  101.3436  ...   \n",
       "792  723853.00000  103.5110  101.1431  100.7228  102.1889  ...   \n",
       "\n",
       "     DNDGRG3M086SBEA  DSERRG3M086SBEA  CES0600000008  CES2000000008  \\\n",
       "0             18.294           10.152           2.13           2.45   \n",
       "1             18.302           10.167           2.14           2.46   \n",
       "2             18.289           10.185           2.15           2.45   \n",
       "3             18.300           10.221           2.16           2.47   \n",
       "4             18.280           10.238           2.17           2.48   \n",
       "..               ...              ...            ...            ...   \n",
       "788          119.220          128.682          31.45          36.18   \n",
       "789          119.218          129.176          31.53          36.27   \n",
       "790          119.230          129.390          31.59          36.26   \n",
       "791          119.746          129.875          31.73          36.46   \n",
       "792          120.453          130.196          31.90          36.54   \n",
       "\n",
       "     CES3000000008  UMCSENTx  DTCOLNVHFNM   DTCTHFNM     INVEST  VIXCLSx  \n",
       "0             2.04       NaN      6476.00   12298.00    84.2043      NaN  \n",
       "1             2.05       NaN      6476.00   12298.00    83.5280      NaN  \n",
       "2             2.07       NaN      6508.00   12349.00    81.6405      NaN  \n",
       "3             2.08       NaN      6620.00   12484.00    81.8099      NaN  \n",
       "4             2.08      95.3      6753.00   12646.00    80.7315      NaN  \n",
       "..             ...       ...          ...        ...        ...      ...  \n",
       "788          28.01      70.1    553347.06  934283.59  5368.5671  17.6597  \n",
       "789          28.07      70.5    554951.25  938525.34  5407.2449  19.9478  \n",
       "790          28.22      71.8    556075.09  941204.79  5382.5669  15.9822  \n",
       "791          28.33      74.0    558854.68  946489.00  5370.9871  15.6997  \n",
       "792          28.55      71.7          NaN        NaN  5372.8381  16.8122  \n",
       "\n",
       "[793 rows x 127 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/your_path', delimiter = ',')\n",
    "df = df.iloc[1:].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42    19.5715\n",
       "Name: VIXCLSx, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first non NaN value of the VIX\n",
    "df.loc[df['sasdate']=='7/1/1962','VIXCLSx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate step to work with time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>DNDGRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>UMCSENTx</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-01-01</th>\n",
       "      <td>2583.560</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>15.188</td>\n",
       "      <td>2.766768e+05</td>\n",
       "      <td>18235.77392</td>\n",
       "      <td>21.9616</td>\n",
       "      <td>23.3868</td>\n",
       "      <td>22.2620</td>\n",
       "      <td>31.6664</td>\n",
       "      <td>18.9498</td>\n",
       "      <td>...</td>\n",
       "      <td>18.294</td>\n",
       "      <td>10.152</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>84.2043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-02-01</th>\n",
       "      <td>2593.596</td>\n",
       "      <td>2434.8</td>\n",
       "      <td>15.346</td>\n",
       "      <td>2.787140e+05</td>\n",
       "      <td>18369.56308</td>\n",
       "      <td>22.3917</td>\n",
       "      <td>23.7024</td>\n",
       "      <td>22.4549</td>\n",
       "      <td>31.8987</td>\n",
       "      <td>19.0492</td>\n",
       "      <td>...</td>\n",
       "      <td>18.302</td>\n",
       "      <td>10.167</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>83.5280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-03-01</th>\n",
       "      <td>2610.396</td>\n",
       "      <td>2452.7</td>\n",
       "      <td>15.491</td>\n",
       "      <td>2.777753e+05</td>\n",
       "      <td>18523.05762</td>\n",
       "      <td>22.7142</td>\n",
       "      <td>23.8459</td>\n",
       "      <td>22.5651</td>\n",
       "      <td>31.8987</td>\n",
       "      <td>19.4223</td>\n",
       "      <td>...</td>\n",
       "      <td>18.289</td>\n",
       "      <td>10.185</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6508.00</td>\n",
       "      <td>12349.00</td>\n",
       "      <td>81.6405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-04-01</th>\n",
       "      <td>2627.446</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>15.435</td>\n",
       "      <td>2.833627e+05</td>\n",
       "      <td>18534.46600</td>\n",
       "      <td>23.1981</td>\n",
       "      <td>24.1903</td>\n",
       "      <td>22.8957</td>\n",
       "      <td>32.4019</td>\n",
       "      <td>19.5466</td>\n",
       "      <td>...</td>\n",
       "      <td>18.300</td>\n",
       "      <td>10.221</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6620.00</td>\n",
       "      <td>12484.00</td>\n",
       "      <td>81.8099</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-05-01</th>\n",
       "      <td>2642.720</td>\n",
       "      <td>2486.4</td>\n",
       "      <td>15.622</td>\n",
       "      <td>2.853072e+05</td>\n",
       "      <td>18679.66354</td>\n",
       "      <td>23.5476</td>\n",
       "      <td>24.3911</td>\n",
       "      <td>23.1161</td>\n",
       "      <td>32.5567</td>\n",
       "      <td>19.9445</td>\n",
       "      <td>...</td>\n",
       "      <td>18.280</td>\n",
       "      <td>10.238</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.08</td>\n",
       "      <td>95.3</td>\n",
       "      <td>6753.00</td>\n",
       "      <td>12646.00</td>\n",
       "      <td>80.7315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-01</th>\n",
       "      <td>19993.464</td>\n",
       "      <td>16283.1</td>\n",
       "      <td>121.690</td>\n",
       "      <td>1.541305e+06</td>\n",
       "      <td>716388.00000</td>\n",
       "      <td>102.5873</td>\n",
       "      <td>100.4044</td>\n",
       "      <td>100.1100</td>\n",
       "      <td>102.0602</td>\n",
       "      <td>103.6095</td>\n",
       "      <td>...</td>\n",
       "      <td>119.220</td>\n",
       "      <td>128.682</td>\n",
       "      <td>31.45</td>\n",
       "      <td>36.18</td>\n",
       "      <td>28.01</td>\n",
       "      <td>70.1</td>\n",
       "      <td>553347.06</td>\n",
       "      <td>934283.59</td>\n",
       "      <td>5368.5671</td>\n",
       "      <td>17.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>20067.376</td>\n",
       "      <td>16340.0</td>\n",
       "      <td>121.904</td>\n",
       "      <td>1.538666e+06</td>\n",
       "      <td>720393.00000</td>\n",
       "      <td>102.1219</td>\n",
       "      <td>99.6821</td>\n",
       "      <td>99.0178</td>\n",
       "      <td>101.4336</td>\n",
       "      <td>100.6884</td>\n",
       "      <td>...</td>\n",
       "      <td>119.218</td>\n",
       "      <td>129.176</td>\n",
       "      <td>31.53</td>\n",
       "      <td>36.27</td>\n",
       "      <td>28.07</td>\n",
       "      <td>70.5</td>\n",
       "      <td>554951.25</td>\n",
       "      <td>938525.34</td>\n",
       "      <td>5407.2449</td>\n",
       "      <td>19.9478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01</th>\n",
       "      <td>20111.246</td>\n",
       "      <td>16385.1</td>\n",
       "      <td>122.435</td>\n",
       "      <td>1.544822e+06</td>\n",
       "      <td>725079.00000</td>\n",
       "      <td>101.9736</td>\n",
       "      <td>99.5645</td>\n",
       "      <td>99.0025</td>\n",
       "      <td>101.0038</td>\n",
       "      <td>102.5986</td>\n",
       "      <td>...</td>\n",
       "      <td>119.230</td>\n",
       "      <td>129.390</td>\n",
       "      <td>31.59</td>\n",
       "      <td>36.26</td>\n",
       "      <td>28.22</td>\n",
       "      <td>71.8</td>\n",
       "      <td>556075.09</td>\n",
       "      <td>941204.79</td>\n",
       "      <td>5382.5669</td>\n",
       "      <td>15.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-01</th>\n",
       "      <td>20136.069</td>\n",
       "      <td>16407.9</td>\n",
       "      <td>123.103</td>\n",
       "      <td>1.555153e+06</td>\n",
       "      <td>730300.00000</td>\n",
       "      <td>102.9833</td>\n",
       "      <td>100.2940</td>\n",
       "      <td>99.6550</td>\n",
       "      <td>101.3436</td>\n",
       "      <td>101.2420</td>\n",
       "      <td>...</td>\n",
       "      <td>119.746</td>\n",
       "      <td>129.875</td>\n",
       "      <td>31.73</td>\n",
       "      <td>36.46</td>\n",
       "      <td>28.33</td>\n",
       "      <td>74.0</td>\n",
       "      <td>558854.68</td>\n",
       "      <td>946489.00</td>\n",
       "      <td>5370.9871</td>\n",
       "      <td>15.6997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>20248.025</td>\n",
       "      <td>16464.7</td>\n",
       "      <td>122.519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>723853.00000</td>\n",
       "      <td>103.5110</td>\n",
       "      <td>101.1431</td>\n",
       "      <td>100.7228</td>\n",
       "      <td>102.1889</td>\n",
       "      <td>98.2527</td>\n",
       "      <td>...</td>\n",
       "      <td>120.453</td>\n",
       "      <td>130.196</td>\n",
       "      <td>31.90</td>\n",
       "      <td>36.54</td>\n",
       "      <td>28.55</td>\n",
       "      <td>71.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5372.8381</td>\n",
       "      <td>16.8122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  RPI  W875RX1  DPCERA3M086SBEA     CMRMTSPLx       RETAILx  \\\n",
       "sasdate                                                                       \n",
       "1959-01-01   2583.560   2426.0           15.188  2.766768e+05   18235.77392   \n",
       "1959-02-01   2593.596   2434.8           15.346  2.787140e+05   18369.56308   \n",
       "1959-03-01   2610.396   2452.7           15.491  2.777753e+05   18523.05762   \n",
       "1959-04-01   2627.446   2470.0           15.435  2.833627e+05   18534.46600   \n",
       "1959-05-01   2642.720   2486.4           15.622  2.853072e+05   18679.66354   \n",
       "...               ...      ...              ...           ...           ...   \n",
       "2024-09-01  19993.464  16283.1          121.690  1.541305e+06  716388.00000   \n",
       "2024-10-01  20067.376  16340.0          121.904  1.538666e+06  720393.00000   \n",
       "2024-11-01  20111.246  16385.1          122.435  1.544822e+06  725079.00000   \n",
       "2024-12-01  20136.069  16407.9          123.103  1.555153e+06  730300.00000   \n",
       "2025-01-01  20248.025  16464.7          122.519           NaN  723853.00000   \n",
       "\n",
       "              INDPRO   IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  \\\n",
       "sasdate                                                       ...   \n",
       "1959-01-01   21.9616   23.3868   22.2620   31.6664   18.9498  ...   \n",
       "1959-02-01   22.3917   23.7024   22.4549   31.8987   19.0492  ...   \n",
       "1959-03-01   22.7142   23.8459   22.5651   31.8987   19.4223  ...   \n",
       "1959-04-01   23.1981   24.1903   22.8957   32.4019   19.5466  ...   \n",
       "1959-05-01   23.5476   24.3911   23.1161   32.5567   19.9445  ...   \n",
       "...              ...       ...       ...       ...       ...  ...   \n",
       "2024-09-01  102.5873  100.4044  100.1100  102.0602  103.6095  ...   \n",
       "2024-10-01  102.1219   99.6821   99.0178  101.4336  100.6884  ...   \n",
       "2024-11-01  101.9736   99.5645   99.0025  101.0038  102.5986  ...   \n",
       "2024-12-01  102.9833  100.2940   99.6550  101.3436  101.2420  ...   \n",
       "2025-01-01  103.5110  101.1431  100.7228  102.1889   98.2527  ...   \n",
       "\n",
       "            DNDGRG3M086SBEA  DSERRG3M086SBEA  CES0600000008  CES2000000008  \\\n",
       "sasdate                                                                      \n",
       "1959-01-01           18.294           10.152           2.13           2.45   \n",
       "1959-02-01           18.302           10.167           2.14           2.46   \n",
       "1959-03-01           18.289           10.185           2.15           2.45   \n",
       "1959-04-01           18.300           10.221           2.16           2.47   \n",
       "1959-05-01           18.280           10.238           2.17           2.48   \n",
       "...                     ...              ...            ...            ...   \n",
       "2024-09-01          119.220          128.682          31.45          36.18   \n",
       "2024-10-01          119.218          129.176          31.53          36.27   \n",
       "2024-11-01          119.230          129.390          31.59          36.26   \n",
       "2024-12-01          119.746          129.875          31.73          36.46   \n",
       "2025-01-01          120.453          130.196          31.90          36.54   \n",
       "\n",
       "            CES3000000008  UMCSENTx  DTCOLNVHFNM   DTCTHFNM     INVEST  \\\n",
       "sasdate                                                                  \n",
       "1959-01-01           2.04       NaN      6476.00   12298.00    84.2043   \n",
       "1959-02-01           2.05       NaN      6476.00   12298.00    83.5280   \n",
       "1959-03-01           2.07       NaN      6508.00   12349.00    81.6405   \n",
       "1959-04-01           2.08       NaN      6620.00   12484.00    81.8099   \n",
       "1959-05-01           2.08      95.3      6753.00   12646.00    80.7315   \n",
       "...                   ...       ...          ...        ...        ...   \n",
       "2024-09-01          28.01      70.1    553347.06  934283.59  5368.5671   \n",
       "2024-10-01          28.07      70.5    554951.25  938525.34  5407.2449   \n",
       "2024-11-01          28.22      71.8    556075.09  941204.79  5382.5669   \n",
       "2024-12-01          28.33      74.0    558854.68  946489.00  5370.9871   \n",
       "2025-01-01          28.55      71.7          NaN        NaN  5372.8381   \n",
       "\n",
       "            VIXCLSx  \n",
       "sasdate              \n",
       "1959-01-01      NaN  \n",
       "1959-02-01      NaN  \n",
       "1959-03-01      NaN  \n",
       "1959-04-01      NaN  \n",
       "1959-05-01      NaN  \n",
       "...             ...  \n",
       "2024-09-01  17.6597  \n",
       "2024-10-01  19.9478  \n",
       "2024-11-01  15.9822  \n",
       "2024-12-01  15.6997  \n",
       "2025-01-01  16.8122  \n",
       "\n",
       "[793 rows x 126 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sasdate'] = pd.to_datetime(df['sasdate'])\n",
    "df.set_index('sasdate', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your DataFrame is sorted chronologically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the series are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 793 entries, 1959-01-01 to 2025-01-01\n",
      "Columns: 126 entries, RPI to VIXCLSx\n",
      "dtypes: float64(103), int64(23)\n",
      "memory usage: 786.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = df['CMRMTSPLx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Log for positive series except for variables that are already in rates or percentage units (based on the appendix of FRED-MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_vars = [\n",
    "    'CUMFNS', 'HWI', 'HWIURATIO', 'UNRATE', 'UEMPMEAN', 'CES0600000007',\n",
    "    'AWHMAN', 'ISRATIOx', 'UMCSENTx', 'NONBORRES', 'CONSPI', 'FEDFUNDS',\n",
    "    'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA',\n",
    "    'COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM',\n",
    "    'AAAFFM', 'BAAFFM', 'S&P div yield', 'VIXCLSx'\n",
    "]\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in excluded_vars and df[col].min() > 0:  # Ensure all values are positive\n",
    "        df[col] = np.log(df[col])  # Apply log transformation directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Check for Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPI: ✅ Stationary (p-value: 0.0471)\n",
      "W875RX1: ❌ Non-stationary (p-value: 0.3414)\n",
      "DPCERA3M086SBEA: ❌ Non-stationary (p-value: 0.1409)\n",
      "CMRMTSPLx: ❌ Non-stationary (p-value: 0.6666)\n",
      "RETAILx: ❌ Non-stationary (p-value: 0.3549)\n",
      "INDPRO: ❌ Non-stationary (p-value: 0.2049)\n",
      "IPFPNSS: ✅ Stationary (p-value: 0.0410)\n",
      "IPFINAL: ✅ Stationary (p-value: 0.0051)\n",
      "IPCONGD: ✅ Stationary (p-value: 0.0041)\n",
      "IPDCONGD: ❌ Non-stationary (p-value: 0.2085)\n",
      "IPNCONGD: ✅ Stationary (p-value: 0.0000)\n",
      "IPBUSEQ: ❌ Non-stationary (p-value: 0.3436)\n",
      "IPMAT: ❌ Non-stationary (p-value: 0.2549)\n",
      "IPDMAT: ❌ Non-stationary (p-value: 0.7293)\n",
      "IPNMAT: ✅ Stationary (p-value: 0.0002)\n",
      "IPMANSICS: ❌ Non-stationary (p-value: 0.2049)\n",
      "IPB51222S: ✅ Stationary (p-value: 0.0000)\n",
      "IPFUELS: ❌ Non-stationary (p-value: 0.1460)\n",
      "CUMFNS: ✅ Stationary (p-value: 0.0026)\n",
      "HWI: ❌ Non-stationary (p-value: 0.4078)\n",
      "HWIURATIO: ❌ Non-stationary (p-value: 0.1165)\n",
      "CLF16OV: ✅ Stationary (p-value: 0.0001)\n",
      "CE16OV: ❌ Non-stationary (p-value: 0.2339)\n",
      "UNRATE: ✅ Stationary (p-value: 0.0109)\n",
      "UEMPMEAN: ❌ Non-stationary (p-value: 0.3320)\n",
      "UEMPLT5: ❌ Non-stationary (p-value: 0.1995)\n",
      "UEMP5TO14: ❌ Non-stationary (p-value: 0.0511)\n",
      "UEMP15OV: ❌ Non-stationary (p-value: 0.1175)\n",
      "UEMP15T26: ❌ Non-stationary (p-value: 0.0517)\n",
      "UEMP27OV: ✅ Stationary (p-value: 0.0440)\n",
      "CLAIMSx: ✅ Stationary (p-value: 0.0002)\n",
      "PAYEMS: ❌ Non-stationary (p-value: 0.0884)\n",
      "USGOOD: ❌ Non-stationary (p-value: 0.1556)\n",
      "CES1021000001: ❌ Non-stationary (p-value: 0.1595)\n",
      "USCONS: ❌ Non-stationary (p-value: 0.7637)\n",
      "MANEMP: ❌ Non-stationary (p-value: 0.8065)\n",
      "DMANEMP: ❌ Non-stationary (p-value: 0.6555)\n",
      "NDMANEMP: ❌ Non-stationary (p-value: 0.9402)\n",
      "SRVPRD: ✅ Stationary (p-value: 0.0076)\n",
      "USTPU: ❌ Non-stationary (p-value: 0.0603)\n",
      "USWTRADE: ❌ Non-stationary (p-value: 0.1190)\n",
      "USTRADE: ✅ Stationary (p-value: 0.0004)\n",
      "USFIRE: ❌ Non-stationary (p-value: 0.0703)\n",
      "USGOVT: ✅ Stationary (p-value: 0.0001)\n",
      "CES0600000007: ✅ Stationary (p-value: 0.0282)\n",
      "AWOTMAN: ✅ Stationary (p-value: 0.0462)\n",
      "AWHMAN: ✅ Stationary (p-value: 0.0076)\n",
      "HOUST: ✅ Stationary (p-value: 0.0155)\n",
      "HOUSTNE: ❌ Non-stationary (p-value: 0.1580)\n",
      "HOUSTMW: ❌ Non-stationary (p-value: 0.1258)\n",
      "HOUSTS: ✅ Stationary (p-value: 0.0142)\n",
      "HOUSTW: ✅ Stationary (p-value: 0.0046)\n",
      "PERMIT: ✅ Stationary (p-value: 0.0027)\n",
      "PERMITNE: ✅ Stationary (p-value: 0.0401)\n",
      "PERMITMW: ❌ Non-stationary (p-value: 0.0951)\n",
      "PERMITS: ✅ Stationary (p-value: 0.0079)\n",
      "PERMITW: ✅ Stationary (p-value: 0.0003)\n",
      "ACOGNO: ❌ Non-stationary (p-value: 0.6647)\n",
      "AMDMNOx: ❌ Non-stationary (p-value: 0.3094)\n",
      "ANDENOx: ❌ Non-stationary (p-value: 0.2193)\n",
      "AMDMUOx: ❌ Non-stationary (p-value: 0.4540)\n",
      "BUSINVx: ❌ Non-stationary (p-value: 0.1794)\n",
      "ISRATIOx: ❌ Non-stationary (p-value: 0.1376)\n",
      "M1SL: ❌ Non-stationary (p-value: 0.9961)\n",
      "M2SL: ❌ Non-stationary (p-value: 0.5139)\n",
      "M2REAL: ❌ Non-stationary (p-value: 0.8442)\n",
      "BOGMBASE: ❌ Non-stationary (p-value: 0.9897)\n",
      "TOTRESNS: ❌ Non-stationary (p-value: 0.9436)\n",
      "NONBORRES: ❌ Non-stationary (p-value: 0.9870)\n",
      "BUSLOANS: ❌ Non-stationary (p-value: 0.2403)\n",
      "REALLN: ❌ Non-stationary (p-value: 0.1811)\n",
      "NONREVSL: ❌ Non-stationary (p-value: 0.5038)\n",
      "CONSPI: ❌ Non-stationary (p-value: 0.4003)\n",
      "S&P 500: ❌ Non-stationary (p-value: 0.9858)\n",
      "S&P div yield: ❌ Non-stationary (p-value: 0.7765)\n",
      "S&P PE ratio: ✅ Stationary (p-value: 0.0287)\n",
      "FEDFUNDS: ✅ Stationary (p-value: 0.0435)\n",
      "CP3Mx: ❌ Non-stationary (p-value: 0.0978)\n",
      "TB3MS: ❌ Non-stationary (p-value: 0.1596)\n",
      "TB6MS: ❌ Non-stationary (p-value: 0.1976)\n",
      "GS1: ❌ Non-stationary (p-value: 0.2645)\n",
      "GS5: ❌ Non-stationary (p-value: 0.5763)\n",
      "GS10: ❌ Non-stationary (p-value: 0.6367)\n",
      "AAA: ❌ Non-stationary (p-value: 0.4921)\n",
      "BAA: ❌ Non-stationary (p-value: 0.4425)\n",
      "COMPAPFFx: ✅ Stationary (p-value: 0.0000)\n",
      "TB3SMFFM: ✅ Stationary (p-value: 0.0039)\n",
      "TB6SMFFM: ✅ Stationary (p-value: 0.0000)\n",
      "T1YFFM: ✅ Stationary (p-value: 0.0000)\n",
      "T5YFFM: ✅ Stationary (p-value: 0.0001)\n",
      "T10YFFM: ✅ Stationary (p-value: 0.0000)\n",
      "AAAFFM: ✅ Stationary (p-value: 0.0000)\n",
      "BAAFFM: ✅ Stationary (p-value: 0.0000)\n",
      "TWEXAFEGSMTHx: ❌ Non-stationary (p-value: 0.3251)\n",
      "EXSZUSx: ❌ Non-stationary (p-value: 0.6890)\n",
      "EXJPUSx: ❌ Non-stationary (p-value: 0.4968)\n",
      "EXUSUKx: ❌ Non-stationary (p-value: 0.3932)\n",
      "EXCAUSx: ❌ Non-stationary (p-value: 0.2658)\n",
      "WPSFD49207: ❌ Non-stationary (p-value: 0.7480)\n",
      "WPSFD49502: ❌ Non-stationary (p-value: 0.8054)\n",
      "WPSID61: ❌ Non-stationary (p-value: 0.7317)\n",
      "WPSID62: ❌ Non-stationary (p-value: 0.7564)\n",
      "OILPRICEx: ❌ Non-stationary (p-value: 0.5919)\n",
      "PPICMM: ❌ Non-stationary (p-value: 0.6637)\n",
      "CPIAUCSL: ❌ Non-stationary (p-value: 0.5766)\n",
      "CPIAPPSL: ❌ Non-stationary (p-value: 0.1535)\n",
      "CPITRNSL: ❌ Non-stationary (p-value: 0.6873)\n",
      "CPIMEDSL: ❌ Non-stationary (p-value: 0.2584)\n",
      "CUSR0000SAC: ❌ Non-stationary (p-value: 0.4507)\n",
      "CUSR0000SAD: ❌ Non-stationary (p-value: 0.3514)\n",
      "CUSR0000SAS: ❌ Non-stationary (p-value: 0.6186)\n",
      "CPIULFSL: ❌ Non-stationary (p-value: 0.5543)\n",
      "CUSR0000SA0L2: ❌ Non-stationary (p-value: 0.4927)\n",
      "CUSR0000SA0L5: ❌ Non-stationary (p-value: 0.5940)\n",
      "PCEPI: ❌ Non-stationary (p-value: 0.5163)\n",
      "DDURRG3M086SBEA: ❌ Non-stationary (p-value: 0.3612)\n",
      "DNDGRG3M086SBEA: ❌ Non-stationary (p-value: 0.4911)\n",
      "DSERRG3M086SBEA: ❌ Non-stationary (p-value: 0.6599)\n",
      "CES0600000008: ❌ Non-stationary (p-value: 0.1926)\n",
      "CES2000000008: ❌ Non-stationary (p-value: 0.4085)\n",
      "CES3000000008: ❌ Non-stationary (p-value: 0.5078)\n",
      "UMCSENTx: ✅ Stationary (p-value: 0.0494)\n",
      "DTCOLNVHFNM: ❌ Non-stationary (p-value: 0.8881)\n",
      "DTCTHFNM: ❌ Non-stationary (p-value: 0.2250)\n",
      "INVEST: ❌ Non-stationary (p-value: 0.9040)\n",
      "VIXCLSx: ✅ Stationary (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def check_stationarity_all(df):\n",
    "    for col in df.columns:\n",
    "        series = df[col].dropna()  # Remove NaNs to avoid issues\n",
    "        try:\n",
    "            result = adfuller(series)\n",
    "            p_value = result[1]\n",
    "            status = \"✅ Stationary\" if p_value < 0.05 else \"❌ Non-stationary\"\n",
    "            print(f\"{col}: {status} (p-value: {p_value:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"{col}: ❌ Could not test (Error: {e})\")\n",
    "\n",
    "# Run stationarity check on all series in df\n",
    "check_stationarity_all(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPI: ✅ Stationary (no differencing needed)\n",
      "W875RX1: 1st Difference Applied ✅ Stationary\n",
      "DPCERA3M086SBEA: 1st Difference Applied ✅ Stationary\n",
      "CMRMTSPLx: 1st Difference Applied ✅ Stationary\n",
      "RETAILx: 1st Difference Applied ✅ Stationary\n",
      "INDPRO: 1st Difference Applied ✅ Stationary\n",
      "IPFPNSS: ✅ Stationary (no differencing needed)\n",
      "IPFINAL: ✅ Stationary (no differencing needed)\n",
      "IPCONGD: ✅ Stationary (no differencing needed)\n",
      "IPDCONGD: 1st Difference Applied ✅ Stationary\n",
      "IPNCONGD: ✅ Stationary (no differencing needed)\n",
      "IPBUSEQ: 1st Difference Applied ✅ Stationary\n",
      "IPMAT: 1st Difference Applied ✅ Stationary\n",
      "IPDMAT: 1st Difference Applied ✅ Stationary\n",
      "IPNMAT: ✅ Stationary (no differencing needed)\n",
      "IPMANSICS: 1st Difference Applied ✅ Stationary\n",
      "IPB51222S: ✅ Stationary (no differencing needed)\n",
      "IPFUELS: 1st Difference Applied ✅ Stationary\n",
      "CUMFNS: ✅ Stationary (no differencing needed)\n",
      "HWI: 1st Difference Applied ✅ Stationary\n",
      "HWIURATIO: 1st Difference Applied ✅ Stationary\n",
      "CLF16OV: ✅ Stationary (no differencing needed)\n",
      "CE16OV: 1st Difference Applied ✅ Stationary\n",
      "UNRATE: ✅ Stationary (no differencing needed)\n",
      "UEMPMEAN: 1st Difference Applied ✅ Stationary\n",
      "UEMPLT5: 1st Difference Applied ✅ Stationary\n",
      "UEMP5TO14: 1st Difference Applied ✅ Stationary\n",
      "UEMP15OV: 1st Difference Applied ✅ Stationary\n",
      "UEMP15T26: 1st Difference Applied ✅ Stationary\n",
      "UEMP27OV: ✅ Stationary (no differencing needed)\n",
      "CLAIMSx: ✅ Stationary (no differencing needed)\n",
      "PAYEMS: 1st Difference Applied ✅ Stationary\n",
      "USGOOD: 1st Difference Applied ✅ Stationary\n",
      "CES1021000001: 1st Difference Applied ✅ Stationary\n",
      "USCONS: 1st Difference Applied ✅ Stationary\n",
      "MANEMP: 1st Difference Applied ✅ Stationary\n",
      "DMANEMP: 1st Difference Applied ✅ Stationary\n",
      "NDMANEMP: 1st Difference Applied ✅ Stationary\n",
      "SRVPRD: ✅ Stationary (no differencing needed)\n",
      "USTPU: 1st Difference Applied ✅ Stationary\n",
      "USWTRADE: 1st Difference Applied ✅ Stationary\n",
      "USTRADE: ✅ Stationary (no differencing needed)\n",
      "USFIRE: 1st Difference Applied ✅ Stationary\n",
      "USGOVT: ✅ Stationary (no differencing needed)\n",
      "CES0600000007: ✅ Stationary (no differencing needed)\n",
      "AWOTMAN: ✅ Stationary (no differencing needed)\n",
      "AWHMAN: ✅ Stationary (no differencing needed)\n",
      "HOUST: ✅ Stationary (no differencing needed)\n",
      "HOUSTNE: 1st Difference Applied ✅ Stationary\n",
      "HOUSTMW: 1st Difference Applied ✅ Stationary\n",
      "HOUSTS: ✅ Stationary (no differencing needed)\n",
      "HOUSTW: ✅ Stationary (no differencing needed)\n",
      "PERMIT: ✅ Stationary (no differencing needed)\n",
      "PERMITNE: ✅ Stationary (no differencing needed)\n",
      "PERMITMW: 1st Difference Applied ✅ Stationary\n",
      "PERMITS: ✅ Stationary (no differencing needed)\n",
      "PERMITW: ✅ Stationary (no differencing needed)\n",
      "ACOGNO: 1st Difference Applied ✅ Stationary\n",
      "AMDMNOx: 1st Difference Applied ✅ Stationary\n",
      "ANDENOx: 1st Difference Applied ✅ Stationary\n",
      "AMDMUOx: 1st Difference Applied ✅ Stationary\n",
      "BUSINVx: 1st Difference Applied ✅ Stationary\n",
      "ISRATIOx: 1st Difference Applied ✅ Stationary\n",
      "M1SL: 1st Difference Applied ✅ Stationary\n",
      "M2SL: 1st Difference Applied ✅ Stationary\n",
      "M2REAL: 1st Difference Applied ✅ Stationary\n",
      "BOGMBASE: 1st Difference Applied ✅ Stationary\n",
      "TOTRESNS: 1st Difference Applied ✅ Stationary\n",
      "NONBORRES: 1st Difference Applied ✅ Stationary\n",
      "BUSLOANS: 1st Difference Applied ✅ Stationary\n",
      "REALLN: 1st Difference Applied ✅ Stationary\n",
      "NONREVSL: 1st Difference Applied ✅ Stationary\n",
      "CONSPI: 1st Difference Applied ✅ Stationary\n",
      "S&P 500: 1st Difference Applied ✅ Stationary\n",
      "S&P div yield: 1st Difference Applied ✅ Stationary\n",
      "S&P PE ratio: ✅ Stationary (no differencing needed)\n",
      "FEDFUNDS: ✅ Stationary (no differencing needed)\n",
      "CP3Mx: 1st Difference Applied ✅ Stationary\n",
      "TB3MS: 1st Difference Applied ✅ Stationary\n",
      "TB6MS: 1st Difference Applied ✅ Stationary\n",
      "GS1: 1st Difference Applied ✅ Stationary\n",
      "GS5: 1st Difference Applied ✅ Stationary\n",
      "GS10: 1st Difference Applied ✅ Stationary\n",
      "AAA: 1st Difference Applied ✅ Stationary\n",
      "BAA: 1st Difference Applied ✅ Stationary\n",
      "COMPAPFFx: ✅ Stationary (no differencing needed)\n",
      "TB3SMFFM: ✅ Stationary (no differencing needed)\n",
      "TB6SMFFM: ✅ Stationary (no differencing needed)\n",
      "T1YFFM: ✅ Stationary (no differencing needed)\n",
      "T5YFFM: ✅ Stationary (no differencing needed)\n",
      "T10YFFM: ✅ Stationary (no differencing needed)\n",
      "AAAFFM: ✅ Stationary (no differencing needed)\n",
      "BAAFFM: ✅ Stationary (no differencing needed)\n",
      "TWEXAFEGSMTHx: 1st Difference Applied ✅ Stationary\n",
      "EXSZUSx: 1st Difference Applied ✅ Stationary\n",
      "EXJPUSx: 1st Difference Applied ✅ Stationary\n",
      "EXUSUKx: 1st Difference Applied ✅ Stationary\n",
      "EXCAUSx: 1st Difference Applied ✅ Stationary\n",
      "WPSFD49207: 1st Difference Applied ✅ Stationary\n",
      "WPSFD49502: 1st Difference Applied ✅ Stationary\n",
      "WPSID61: 1st Difference Applied ✅ Stationary\n",
      "WPSID62: 1st Difference Applied ✅ Stationary\n",
      "OILPRICEx: 1st Difference Applied ✅ Stationary\n",
      "PPICMM: 1st Difference Applied ✅ Stationary\n",
      "CPIAUCSL: 1st Difference Applied ✅ Stationary\n",
      "CPIAPPSL: 1st Difference Applied ✅ Stationary\n",
      "CPITRNSL: 1st Difference Applied ✅ Stationary\n",
      "CPIMEDSL: 1st Difference Applied ✅ Stationary\n",
      "CUSR0000SAC: 1st Difference Applied ✅ Stationary\n",
      "CUSR0000SAD: 1st Difference Applied ✅ Stationary\n",
      "CUSR0000SAS: 1st Difference Applied ✅ Stationary\n",
      "CPIULFSL: 1st Difference Applied ✅ Stationary\n",
      "CUSR0000SA0L2: 1st Difference Applied ✅ Stationary\n",
      "CUSR0000SA0L5: 1st Difference Applied ✅ Stationary\n",
      "PCEPI: 1st Difference Applied ✅ Stationary\n",
      "DDURRG3M086SBEA: 1st Difference Applied ✅ Stationary\n",
      "DNDGRG3M086SBEA: 1st Difference Applied ✅ Stationary\n",
      "DSERRG3M086SBEA: 2nd Difference Applied ✅ Stationary\n",
      "CES0600000008: 1st Difference Applied ✅ Stationary\n",
      "CES2000000008: 2nd Difference Applied ✅ Stationary\n",
      "CES3000000008: 2nd Difference Applied ✅ Stationary\n",
      "UMCSENTx: ✅ Stationary (no differencing needed)\n",
      "DTCOLNVHFNM: 1st Difference Applied ✅ Stationary\n",
      "DTCTHFNM: 1st Difference Applied ✅ Stationary\n",
      "INVEST: 1st Difference Applied ✅ Stationary\n",
      "VIXCLSx: ✅ Stationary (no differencing needed)\n"
     ]
    }
   ],
   "source": [
    "def check_and_difference(df):\n",
    "    for col in df.columns:\n",
    "        # Step 1: Check if series is stationary (ADF test)\n",
    "        series = df[col].dropna()  # Remove NaNs\n",
    "        result = adfuller(series)\n",
    "        p_value = result[1]\n",
    "        \n",
    "        if p_value < 0.05:  # If p-value is less than 0.05, the series is stationary\n",
    "            print(f\"{col}: ✅ Stationary (no differencing needed)\")\n",
    "        else:\n",
    "            # First differencing\n",
    "            df[col] = df[col].diff()\n",
    "            series = df[col].dropna()  # Re-run ADF test after first differencing\n",
    "            result = adfuller(series)\n",
    "            p_value = result[1]\n",
    "            \n",
    "            if p_value < 0.05:  # If now stationary after first difference\n",
    "                print(f\"{col}: 1st Difference Applied ✅ Stationary\")\n",
    "            else:\n",
    "                # Second differencing\n",
    "                df[col] = df[col].diff()  # Apply second differencing\n",
    "                series = df[col].dropna()  # Re-run ADF test after second differencing\n",
    "                result = adfuller(series)\n",
    "                p_value = result[1]\n",
    "                \n",
    "                if p_value < 0.05:  # If now stationary after second differencing\n",
    "                    print(f\"{col}: 2nd Difference Applied ✅ Stationary\")\n",
    "                else:\n",
    "                    print(f\"{col}: ❌ Still Non-Stationary after 2nd Difference\")\n",
    "\n",
    "check_and_difference(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPI: ✅ Stationary (p-value: 0.0471)\n",
      "W875RX1: ✅ Stationary (p-value: 0.0000)\n",
      "DPCERA3M086SBEA: ✅ Stationary (p-value: 0.0000)\n",
      "CMRMTSPLx: ✅ Stationary (p-value: 0.0000)\n",
      "RETAILx: ✅ Stationary (p-value: 0.0000)\n",
      "INDPRO: ✅ Stationary (p-value: 0.0000)\n",
      "IPFPNSS: ✅ Stationary (p-value: 0.0410)\n",
      "IPFINAL: ✅ Stationary (p-value: 0.0051)\n",
      "IPCONGD: ✅ Stationary (p-value: 0.0041)\n",
      "IPDCONGD: ✅ Stationary (p-value: 0.0000)\n",
      "IPNCONGD: ✅ Stationary (p-value: 0.0000)\n",
      "IPBUSEQ: ✅ Stationary (p-value: 0.0000)\n",
      "IPMAT: ✅ Stationary (p-value: 0.0000)\n",
      "IPDMAT: ✅ Stationary (p-value: 0.0000)\n",
      "IPNMAT: ✅ Stationary (p-value: 0.0002)\n",
      "IPMANSICS: ✅ Stationary (p-value: 0.0000)\n",
      "IPB51222S: ✅ Stationary (p-value: 0.0000)\n",
      "IPFUELS: ✅ Stationary (p-value: 0.0000)\n",
      "CUMFNS: ✅ Stationary (p-value: 0.0026)\n",
      "HWI: ✅ Stationary (p-value: 0.0000)\n",
      "HWIURATIO: ✅ Stationary (p-value: 0.0000)\n",
      "CLF16OV: ✅ Stationary (p-value: 0.0001)\n",
      "CE16OV: ✅ Stationary (p-value: 0.0000)\n",
      "UNRATE: ✅ Stationary (p-value: 0.0109)\n",
      "UEMPMEAN: ✅ Stationary (p-value: 0.0000)\n",
      "UEMPLT5: ✅ Stationary (p-value: 0.0000)\n",
      "UEMP5TO14: ✅ Stationary (p-value: 0.0000)\n",
      "UEMP15OV: ✅ Stationary (p-value: 0.0000)\n",
      "UEMP15T26: ✅ Stationary (p-value: 0.0000)\n",
      "UEMP27OV: ✅ Stationary (p-value: 0.0440)\n",
      "CLAIMSx: ✅ Stationary (p-value: 0.0002)\n",
      "PAYEMS: ✅ Stationary (p-value: 0.0000)\n",
      "USGOOD: ✅ Stationary (p-value: 0.0000)\n",
      "CES1021000001: ✅ Stationary (p-value: 0.0000)\n",
      "USCONS: ✅ Stationary (p-value: 0.0000)\n",
      "MANEMP: ✅ Stationary (p-value: 0.0000)\n",
      "DMANEMP: ✅ Stationary (p-value: 0.0000)\n",
      "NDMANEMP: ✅ Stationary (p-value: 0.0000)\n",
      "SRVPRD: ✅ Stationary (p-value: 0.0076)\n",
      "USTPU: ✅ Stationary (p-value: 0.0000)\n",
      "USWTRADE: ✅ Stationary (p-value: 0.0000)\n",
      "USTRADE: ✅ Stationary (p-value: 0.0004)\n",
      "USFIRE: ✅ Stationary (p-value: 0.0004)\n",
      "USGOVT: ✅ Stationary (p-value: 0.0001)\n",
      "CES0600000007: ✅ Stationary (p-value: 0.0282)\n",
      "AWOTMAN: ✅ Stationary (p-value: 0.0462)\n",
      "AWHMAN: ✅ Stationary (p-value: 0.0076)\n",
      "HOUST: ✅ Stationary (p-value: 0.0155)\n",
      "HOUSTNE: ✅ Stationary (p-value: 0.0000)\n",
      "HOUSTMW: ✅ Stationary (p-value: 0.0000)\n",
      "HOUSTS: ✅ Stationary (p-value: 0.0142)\n",
      "HOUSTW: ✅ Stationary (p-value: 0.0046)\n",
      "PERMIT: ✅ Stationary (p-value: 0.0027)\n",
      "PERMITNE: ✅ Stationary (p-value: 0.0401)\n",
      "PERMITMW: ✅ Stationary (p-value: 0.0000)\n",
      "PERMITS: ✅ Stationary (p-value: 0.0079)\n",
      "PERMITW: ✅ Stationary (p-value: 0.0003)\n",
      "ACOGNO: ✅ Stationary (p-value: 0.0000)\n",
      "AMDMNOx: ✅ Stationary (p-value: 0.0000)\n",
      "ANDENOx: ✅ Stationary (p-value: 0.0000)\n",
      "AMDMUOx: ✅ Stationary (p-value: 0.0000)\n",
      "BUSINVx: ✅ Stationary (p-value: 0.0000)\n",
      "ISRATIOx: ✅ Stationary (p-value: 0.0000)\n",
      "M1SL: ✅ Stationary (p-value: 0.0000)\n",
      "M2SL: ✅ Stationary (p-value: 0.0000)\n",
      "M2REAL: ✅ Stationary (p-value: 0.0000)\n",
      "BOGMBASE: ✅ Stationary (p-value: 0.0000)\n",
      "TOTRESNS: ✅ Stationary (p-value: 0.0000)\n",
      "NONBORRES: ✅ Stationary (p-value: 0.0000)\n",
      "BUSLOANS: ✅ Stationary (p-value: 0.0000)\n",
      "REALLN: ✅ Stationary (p-value: 0.0001)\n",
      "NONREVSL: ✅ Stationary (p-value: 0.0000)\n",
      "CONSPI: ✅ Stationary (p-value: 0.0000)\n",
      "S&P 500: ✅ Stationary (p-value: 0.0000)\n",
      "S&P div yield: ✅ Stationary (p-value: 0.0000)\n",
      "S&P PE ratio: ✅ Stationary (p-value: 0.0287)\n",
      "FEDFUNDS: ✅ Stationary (p-value: 0.0435)\n",
      "CP3Mx: ✅ Stationary (p-value: 0.0000)\n",
      "TB3MS: ✅ Stationary (p-value: 0.0000)\n",
      "TB6MS: ✅ Stationary (p-value: 0.0000)\n",
      "GS1: ✅ Stationary (p-value: 0.0000)\n",
      "GS5: ✅ Stationary (p-value: 0.0000)\n",
      "GS10: ✅ Stationary (p-value: 0.0000)\n",
      "AAA: ✅ Stationary (p-value: 0.0000)\n",
      "BAA: ✅ Stationary (p-value: 0.0000)\n",
      "COMPAPFFx: ✅ Stationary (p-value: 0.0000)\n",
      "TB3SMFFM: ✅ Stationary (p-value: 0.0039)\n",
      "TB6SMFFM: ✅ Stationary (p-value: 0.0000)\n",
      "T1YFFM: ✅ Stationary (p-value: 0.0000)\n",
      "T5YFFM: ✅ Stationary (p-value: 0.0001)\n",
      "T10YFFM: ✅ Stationary (p-value: 0.0000)\n",
      "AAAFFM: ✅ Stationary (p-value: 0.0000)\n",
      "BAAFFM: ✅ Stationary (p-value: 0.0000)\n",
      "TWEXAFEGSMTHx: ✅ Stationary (p-value: 0.0000)\n",
      "EXSZUSx: ✅ Stationary (p-value: 0.0000)\n",
      "EXJPUSx: ✅ Stationary (p-value: 0.0000)\n",
      "EXUSUKx: ✅ Stationary (p-value: 0.0000)\n",
      "EXCAUSx: ✅ Stationary (p-value: 0.0000)\n",
      "WPSFD49207: ✅ Stationary (p-value: 0.0001)\n",
      "WPSFD49502: ✅ Stationary (p-value: 0.0000)\n",
      "WPSID61: ✅ Stationary (p-value: 0.0000)\n",
      "WPSID62: ✅ Stationary (p-value: 0.0000)\n",
      "OILPRICEx: ✅ Stationary (p-value: 0.0000)\n",
      "PPICMM: ✅ Stationary (p-value: 0.0000)\n",
      "CPIAUCSL: ✅ Stationary (p-value: 0.0272)\n",
      "CPIAPPSL: ✅ Stationary (p-value: 0.0170)\n",
      "CPITRNSL: ✅ Stationary (p-value: 0.0000)\n",
      "CPIMEDSL: ✅ Stationary (p-value: 0.0016)\n",
      "CUSR0000SAC: ✅ Stationary (p-value: 0.0002)\n",
      "CUSR0000SAD: ✅ Stationary (p-value: 0.0287)\n",
      "CUSR0000SAS: ✅ Stationary (p-value: 0.0171)\n",
      "CPIULFSL: ✅ Stationary (p-value: 0.0129)\n",
      "CUSR0000SA0L2: ✅ Stationary (p-value: 0.0035)\n",
      "CUSR0000SA0L5: ✅ Stationary (p-value: 0.0178)\n",
      "PCEPI: ✅ Stationary (p-value: 0.0427)\n",
      "DDURRG3M086SBEA: ✅ Stationary (p-value: 0.0047)\n",
      "DNDGRG3M086SBEA: ✅ Stationary (p-value: 0.0001)\n",
      "DSERRG3M086SBEA: ✅ Stationary (p-value: 0.0000)\n",
      "CES0600000008: ✅ Stationary (p-value: 0.0354)\n",
      "CES2000000008: ✅ Stationary (p-value: 0.0000)\n",
      "CES3000000008: ✅ Stationary (p-value: 0.0000)\n",
      "UMCSENTx: ✅ Stationary (p-value: 0.0494)\n",
      "DTCOLNVHFNM: ✅ Stationary (p-value: 0.0001)\n",
      "DTCTHFNM: ✅ Stationary (p-value: 0.0000)\n",
      "INVEST: ✅ Stationary (p-value: 0.0000)\n",
      "VIXCLSx: ✅ Stationary (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def check_stationarity_all(df):\n",
    "    for col in df.columns:\n",
    "        series = df[col].dropna()  # Remove NaNs to avoid issues\n",
    "        try:\n",
    "            result = adfuller(series)\n",
    "            p_value = result[1]\n",
    "            status = \"✅ Stationary\" if p_value < 0.05 else \"❌ Non-stationary\"\n",
    "            print(f\"{col}: {status} (p-value: {p_value:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"{col}: ❌ Could not test (Error: {e})\")\n",
    "\n",
    "\n",
    "check_stationarity_all(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Standardize the series (mean 0 and variance 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = df['CMRMTSPLx'].mean()\n",
    "std_diff = df['CMRMTSPLx'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = (df[col] - df[col].mean()) / df[col].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Screening for outliers (ie. obs exceeding 10 times the interquartile range from the median are replaced by missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected in column: W875RX1\n",
      "Outliers detected in column: DPCERA3M086SBEA\n",
      "Outliers detected in column: CMRMTSPLx\n",
      "Outliers detected in column: RETAILx\n",
      "Outliers detected in column: INDPRO\n",
      "Outliers detected in column: IPDCONGD\n",
      "Outliers detected in column: IPBUSEQ\n",
      "Outliers detected in column: IPMAT\n",
      "Outliers detected in column: IPDMAT\n",
      "Outliers detected in column: IPMANSICS\n",
      "Outliers detected in column: HWIURATIO\n",
      "Outliers detected in column: CE16OV\n",
      "Outliers detected in column: UEMPMEAN\n",
      "Outliers detected in column: UEMPLT5\n",
      "Outliers detected in column: UEMP5TO14\n",
      "Outliers detected in column: UEMP15OV\n",
      "Outliers detected in column: UEMP15T26\n",
      "Outliers detected in column: PAYEMS\n",
      "Outliers detected in column: USGOOD\n",
      "Outliers detected in column: CES1021000001\n",
      "Outliers detected in column: USCONS\n",
      "Outliers detected in column: MANEMP\n",
      "Outliers detected in column: DMANEMP\n",
      "Outliers detected in column: NDMANEMP\n",
      "Outliers detected in column: USTPU\n",
      "Outliers detected in column: USWTRADE\n",
      "Outliers detected in column: USFIRE\n",
      "Outliers detected in column: BUSINVx\n",
      "Outliers detected in column: ISRATIOx\n",
      "Outliers detected in column: M1SL\n",
      "Outliers detected in column: M2SL\n",
      "Outliers detected in column: M2REAL\n",
      "Outliers detected in column: BOGMBASE\n",
      "Outliers detected in column: TOTRESNS\n",
      "Outliers detected in column: NONBORRES\n",
      "Outliers detected in column: BUSLOANS\n",
      "Outliers detected in column: NONREVSL\n",
      "Outliers detected in column: CONSPI\n",
      "Outliers detected in column: CP3Mx\n",
      "Outliers detected in column: TB3MS\n",
      "Outliers detected in column: TB6MS\n",
      "Outliers detected in column: GS1\n",
      "Outliers detected in column: OILPRICEx\n",
      "Outliers detected in column: CPITRNSL\n",
      "Outliers detected in column: DNDGRG3M086SBEA\n",
      "Outliers detected in column: DTCOLNVHFNM\n",
      "Outliers detected in column: DTCTHFNM\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    # Calculate the IQR and median for the current series\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    median = df[col].median()\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = median - 10 * IQR\n",
    "    upper_bound = median + 10 * IQR\n",
    "\n",
    "    # Identify the outliers in the series\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "\n",
    "    # Print the series where outliers are detected\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers detected in column: {col}\")\n",
    "\n",
    "    # Replace outliers with NaN\n",
    "    df[col] = df[col].apply(lambda x: np.nan if x < lower_bound or x > upper_bound else x)\n",
    "# .apply() is a method in pandas used to apply a function (in this case, a lambda function) to each element of the column\n",
    "# lambda x defines an anonymous function that takes an argument x, where x represents each individual value in the column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: W875RX1, NaN Count: 3\n",
      "Column: DPCERA3M086SBEA, NaN Count: 4\n",
      "Column: CMRMTSPLx, NaN Count: 3\n",
      "Column: RETAILx, NaN Count: 3\n",
      "Column: INDPRO, NaN Count: 2\n",
      "Column: IPDCONGD, NaN Count: 3\n",
      "Column: IPBUSEQ, NaN Count: 2\n",
      "Column: IPMAT, NaN Count: 2\n",
      "Column: IPDMAT, NaN Count: 3\n",
      "Column: IPMANSICS, NaN Count: 2\n",
      "Column: IPFUELS, NaN Count: 1\n",
      "Column: HWI, NaN Count: 2\n",
      "Column: HWIURATIO, NaN Count: 3\n",
      "Column: CE16OV, NaN Count: 3\n",
      "Column: UEMPMEAN, NaN Count: 2\n",
      "Column: UEMPLT5, NaN Count: 3\n",
      "Column: UEMP5TO14, NaN Count: 4\n",
      "Column: UEMP15OV, NaN Count: 2\n",
      "Column: UEMP15T26, NaN Count: 2\n",
      "Column: PAYEMS, NaN Count: 3\n",
      "Column: USGOOD, NaN Count: 2\n",
      "Column: CES1021000001, NaN Count: 7\n",
      "Column: USCONS, NaN Count: 2\n",
      "Column: MANEMP, NaN Count: 2\n",
      "Column: DMANEMP, NaN Count: 3\n",
      "Column: NDMANEMP, NaN Count: 2\n",
      "Column: USTPU, NaN Count: 3\n",
      "Column: USWTRADE, NaN Count: 2\n",
      "Column: USFIRE, NaN Count: 2\n",
      "Column: HOUSTNE, NaN Count: 1\n",
      "Column: HOUSTMW, NaN Count: 1\n",
      "Column: PERMIT, NaN Count: 12\n",
      "Column: PERMITNE, NaN Count: 12\n",
      "Column: PERMITMW, NaN Count: 13\n",
      "Column: PERMITS, NaN Count: 12\n",
      "Column: PERMITW, NaN Count: 12\n",
      "Column: ACOGNO, NaN Count: 399\n",
      "Column: AMDMNOx, NaN Count: 2\n",
      "Column: ANDENOx, NaN Count: 111\n",
      "Column: AMDMUOx, NaN Count: 2\n",
      "Column: BUSINVx, NaN Count: 3\n",
      "Column: ISRATIOx, NaN Count: 3\n",
      "Column: M1SL, NaN Count: 3\n",
      "Column: M2SL, NaN Count: 3\n",
      "Column: M2REAL, NaN Count: 2\n",
      "Column: BOGMBASE, NaN Count: 5\n",
      "Column: TOTRESNS, NaN Count: 6\n",
      "Column: NONBORRES, NaN Count: 176\n",
      "Column: BUSLOANS, NaN Count: 2\n",
      "Column: REALLN, NaN Count: 1\n",
      "Column: NONREVSL, NaN Count: 4\n",
      "Column: CONSPI, NaN Count: 7\n",
      "Column: S&P 500, NaN Count: 1\n",
      "Column: S&P div yield, NaN Count: 2\n",
      "Column: S&P PE ratio, NaN Count: 3\n",
      "Column: CP3Mx, NaN Count: 10\n",
      "Column: TB3MS, NaN Count: 8\n",
      "Column: TB6MS, NaN Count: 5\n",
      "Column: GS1, NaN Count: 3\n",
      "Column: GS5, NaN Count: 1\n",
      "Column: GS10, NaN Count: 1\n",
      "Column: AAA, NaN Count: 1\n",
      "Column: BAA, NaN Count: 1\n",
      "Column: COMPAPFFx, NaN Count: 1\n",
      "Column: TWEXAFEGSMTHx, NaN Count: 169\n",
      "Column: EXSZUSx, NaN Count: 1\n",
      "Column: EXJPUSx, NaN Count: 1\n",
      "Column: EXUSUKx, NaN Count: 1\n",
      "Column: EXCAUSx, NaN Count: 1\n",
      "Column: WPSFD49207, NaN Count: 1\n",
      "Column: WPSFD49502, NaN Count: 1\n",
      "Column: WPSID61, NaN Count: 1\n",
      "Column: WPSID62, NaN Count: 1\n",
      "Column: OILPRICEx, NaN Count: 5\n",
      "Column: PPICMM, NaN Count: 1\n",
      "Column: CPIAUCSL, NaN Count: 1\n",
      "Column: CPIAPPSL, NaN Count: 1\n",
      "Column: CPITRNSL, NaN Count: 2\n",
      "Column: CPIMEDSL, NaN Count: 1\n",
      "Column: CUSR0000SAC, NaN Count: 1\n",
      "Column: CUSR0000SAD, NaN Count: 1\n",
      "Column: CUSR0000SAS, NaN Count: 1\n",
      "Column: CPIULFSL, NaN Count: 1\n",
      "Column: CUSR0000SA0L2, NaN Count: 1\n",
      "Column: CUSR0000SA0L5, NaN Count: 1\n",
      "Column: PCEPI, NaN Count: 1\n",
      "Column: DDURRG3M086SBEA, NaN Count: 1\n",
      "Column: DNDGRG3M086SBEA, NaN Count: 2\n",
      "Column: DSERRG3M086SBEA, NaN Count: 2\n",
      "Column: CES0600000008, NaN Count: 1\n",
      "Column: CES2000000008, NaN Count: 2\n",
      "Column: CES3000000008, NaN Count: 2\n",
      "Column: UMCSENTx, NaN Count: 154\n",
      "Column: DTCOLNVHFNM, NaN Count: 4\n",
      "Column: DTCTHFNM, NaN Count: 5\n",
      "Column: INVEST, NaN Count: 1\n",
      "Column: VIXCLSx, NaN Count: 42\n"
     ]
    }
   ],
   "source": [
    "nan_counts = df.isna().sum()\n",
    "\n",
    "# Filter columns that have NaN values (greater than 0)\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Print the column names and number of NaN values\n",
    "for column, count in nan_columns.items():\n",
    "    print(f\"Column: {column}, NaN Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset (with missing values)\n",
    "\n",
    "It is the dataset named df.\n",
    "\n",
    "### Creation of the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Continuous balanced panel created with shape: (692, 106)\n"
     ]
    }
   ],
   "source": [
    "def create_continuous_balanced_panel(df, period_ratio):\n",
    "\n",
    "    \"\"\"=========================================================================\n",
    "    Creates a continuous balanced panel by keeping at least a specified percentage of time periods, \n",
    "    maximizing the number of series, and in case of a tie between two data set with the same dimensions,\n",
    "    retain the one that has the most recent values.\n",
    "\n",
    "    ----------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "\n",
    "            - df: DataFrame with time series data (indexed by time)\n",
    "            - period_ratio: Minimum percentage of original time periods to retain\n",
    "\n",
    "    ----------------------------------------------------------------------------\n",
    "    Returns:\n",
    "\n",
    "            - balanced_df: The largest continuous panel that satisfies the period constraint.\n",
    "    =========================================================================\"\"\"\n",
    "    \n",
    "    # Step 1: Determine the target number of periods\n",
    "    total_periods = df.shape[0]  # Get total number of time periods in the dataset\n",
    "    target_periods = int(period_ratio * total_periods)  # Compute number of periods to retain\n",
    "\n",
    "    # Step 2: Find all possible continuous time blocks\n",
    "    time_index = df.index  # Extract the time index (dates)\n",
    "    best_panel = None  # Placeholder for the best balanced panel\n",
    "    max_series_retained = 0  # Track the highest number of retained series\n",
    "    max_periods_retained = 0  # Track the highest number of periods retained\n",
    "    best_current_end_idx = 0  # Track the best end index for tie-breaking\n",
    "\n",
    "    # Step 3: Slide over all possible continuous windows of `target_periods` length\n",
    "    for start_idx in range(total_periods - target_periods + 1):  # Iterate through all start points\n",
    "        # Define the initial time slice\n",
    "        end_idx = start_idx + target_periods  # Compute the end index for initial window\n",
    "        time_window = time_index[start_idx:end_idx]  # Get the corresponding time period\n",
    "\n",
    "        # Keep only rows in this initial time window\n",
    "        temp_df = df.loc[time_window]  # Select only the data within this initial window\n",
    "\n",
    "        # Count non-missing values per column (each series)\n",
    "        available_series = temp_df.notna().sum(axis=0)  # Count how many values are not NaN\n",
    "\n",
    "        # Keep only series that have no missing values in this initial window\n",
    "        valid_series = available_series[available_series == target_periods].index\n",
    "        temp_df = temp_df[valid_series]  # Keep only the selected valid series\n",
    "\n",
    "        # Step 4: Expand the window progressively one row at a time\n",
    "        current_end_idx = end_idx  # Start with the original end index\n",
    "        while current_end_idx < total_periods:\n",
    "            current_end_idx += 1  # Expand the window by one row\n",
    "\n",
    "            # Get the updated time window with the expanded period\n",
    "            expanded_time_window = time_index[start_idx:current_end_idx]\n",
    "            expanded_df = df.loc[expanded_time_window]\n",
    "\n",
    "            # Count the valid series in the expanded window\n",
    "            available_series_expanded = expanded_df.notna().sum(axis=0)\n",
    "            valid_series_expanded = available_series_expanded[available_series_expanded == current_end_idx - start_idx].index\n",
    "\n",
    "            # If the number of valid series stays the same, keep expanding\n",
    "            if len(valid_series_expanded) == len(valid_series):\n",
    "                temp_df = expanded_df[valid_series_expanded]  # Update the temporary panel\n",
    "            else:\n",
    "                break  # Stop expanding if the number of valid series decreases\n",
    "\n",
    "        # Step 5: Check if this panel is better based on the new criteria\n",
    "        if (temp_df.shape[1] > max_series_retained) or \\\n",
    "           (temp_df.shape[1] == max_series_retained and temp_df.shape[0] > max_periods_retained) or \\\n",
    "           (temp_df.shape[1] == max_series_retained and temp_df.shape[0] == max_periods_retained and current_end_idx > best_current_end_idx):\n",
    "            best_panel = temp_df  # Store this panel as the best one\n",
    "            max_series_retained = temp_df.shape[1]  # Update the max retained series\n",
    "            max_periods_retained = temp_df.shape[0]  # Update the max retained periods\n",
    "            best_current_end_idx = current_end_idx  # Update the most up-to-date end index\n",
    "\n",
    "    print(f\"✅ Continuous balanced panel created with shape: {best_panel.shape}\")  # Print result\n",
    "    return best_panel  # Return the best balanced panel\n",
    "\n",
    "# Example usage: Keep 80% of the original time periods\n",
    "balanced = create_continuous_balanced_panel(df, period_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>IPNCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>CUSR0000SA0L2</th>\n",
       "      <th>CUSR0000SA0L5</th>\n",
       "      <th>PCEPI</th>\n",
       "      <th>DDURRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-07-01</th>\n",
       "      <td>-1.748613</td>\n",
       "      <td>0.072551</td>\n",
       "      <td>-0.615446</td>\n",
       "      <td>0.715483</td>\n",
       "      <td>0.765023</td>\n",
       "      <td>-1.973617</td>\n",
       "      <td>-1.955113</td>\n",
       "      <td>-2.059552</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>-2.128613</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.584209</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-1.312770</td>\n",
       "      <td>0.031852</td>\n",
       "      <td>-0.520108</td>\n",
       "      <td>0.254280</td>\n",
       "      <td>0.781746</td>\n",
       "      <td>1.877859</td>\n",
       "      <td>-0.099429</td>\n",
       "      <td>0.039390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>-1.745851</td>\n",
       "      <td>0.020216</td>\n",
       "      <td>1.681654</td>\n",
       "      <td>0.100189</td>\n",
       "      <td>-0.090381</td>\n",
       "      <td>-1.973617</td>\n",
       "      <td>-1.959689</td>\n",
       "      <td>-2.093244</td>\n",
       "      <td>-0.483186</td>\n",
       "      <td>-2.161061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081715</td>\n",
       "      <td>0.090797</td>\n",
       "      <td>-0.494186</td>\n",
       "      <td>-0.146021</td>\n",
       "      <td>-0.058863</td>\n",
       "      <td>0.248540</td>\n",
       "      <td>-1.176048</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.422974</td>\n",
       "      <td>-0.495456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>-1.745045</td>\n",
       "      <td>1.062232</td>\n",
       "      <td>-1.454784</td>\n",
       "      <td>-0.107582</td>\n",
       "      <td>0.438811</td>\n",
       "      <td>-1.959528</td>\n",
       "      <td>-1.950547</td>\n",
       "      <td>-2.074819</td>\n",
       "      <td>0.326349</td>\n",
       "      <td>-2.139393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906896</td>\n",
       "      <td>0.087596</td>\n",
       "      <td>0.906261</td>\n",
       "      <td>0.062750</td>\n",
       "      <td>-0.116100</td>\n",
       "      <td>-1.120352</td>\n",
       "      <td>0.389672</td>\n",
       "      <td>-0.935949</td>\n",
       "      <td>-0.157978</td>\n",
       "      <td>-0.138552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>-1.735994</td>\n",
       "      <td>-0.952220</td>\n",
       "      <td>1.418380</td>\n",
       "      <td>0.706761</td>\n",
       "      <td>-0.091163</td>\n",
       "      <td>-1.966561</td>\n",
       "      <td>-1.948264</td>\n",
       "      <td>-2.080945</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-2.153821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751247</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-1.489622</td>\n",
       "      <td>-1.512358</td>\n",
       "      <td>0.285591</td>\n",
       "      <td>-1.120352</td>\n",
       "      <td>-0.389134</td>\n",
       "      <td>0.930325</td>\n",
       "      <td>1.243957</td>\n",
       "      <td>0.944973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>-1.729678</td>\n",
       "      <td>1.325876</td>\n",
       "      <td>0.692543</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.224158</td>\n",
       "      <td>-1.952524</td>\n",
       "      <td>-1.939170</td>\n",
       "      <td>-2.062598</td>\n",
       "      <td>-0.030449</td>\n",
       "      <td>-2.139393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751247</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-0.802510</td>\n",
       "      <td>-0.078988</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.242849</td>\n",
       "      <td>0.775077</td>\n",
       "      <td>-0.931856</td>\n",
       "      <td>-0.553800</td>\n",
       "      <td>-0.357883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>1.336587</td>\n",
       "      <td>-0.322076</td>\n",
       "      <td>-1.086475</td>\n",
       "      <td>-0.100294</td>\n",
       "      <td>-1.056195</td>\n",
       "      <td>0.917435</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>0.814720</td>\n",
       "      <td>-1.210923</td>\n",
       "      <td>0.718669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>-0.230922</td>\n",
       "      <td>-0.362669</td>\n",
       "      <td>-0.284042</td>\n",
       "      <td>0.350859</td>\n",
       "      <td>-0.067443</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.381369</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>-0.597801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>1.342504</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.240135</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.357385</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>0.971771</td>\n",
       "      <td>0.860001</td>\n",
       "      <td>1.840469</td>\n",
       "      <td>0.725875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>-0.069329</td>\n",
       "      <td>-0.664476</td>\n",
       "      <td>-0.993822</td>\n",
       "      <td>-0.396922</td>\n",
       "      <td>-0.463997</td>\n",
       "      <td>0.078521</td>\n",
       "      <td>0.378289</td>\n",
       "      <td>0.115401</td>\n",
       "      <td>-0.976736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>1.334565</td>\n",
       "      <td>-0.127958</td>\n",
       "      <td>-0.515598</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>-0.400964</td>\n",
       "      <td>0.938540</td>\n",
       "      <td>0.961971</td>\n",
       "      <td>0.836717</td>\n",
       "      <td>-0.551102</td>\n",
       "      <td>0.706722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147283</td>\n",
       "      <td>-0.075272</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>-1.494266</td>\n",
       "      <td>1.255018</td>\n",
       "      <td>-0.072909</td>\n",
       "      <td>0.273817</td>\n",
       "      <td>-0.289127</td>\n",
       "      <td>-0.485797</td>\n",
       "      <td>-0.804671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>1.352504</td>\n",
       "      <td>0.126202</td>\n",
       "      <td>-0.274933</td>\n",
       "      <td>-0.244196</td>\n",
       "      <td>-0.794993</td>\n",
       "      <td>0.922740</td>\n",
       "      <td>0.933475</td>\n",
       "      <td>0.814779</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.661632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504065</td>\n",
       "      <td>-0.313399</td>\n",
       "      <td>-0.646589</td>\n",
       "      <td>-0.125531</td>\n",
       "      <td>-0.749275</td>\n",
       "      <td>-0.597883</td>\n",
       "      <td>-0.275785</td>\n",
       "      <td>-0.665177</td>\n",
       "      <td>-0.556101</td>\n",
       "      <td>-0.742185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>1.359983</td>\n",
       "      <td>-0.562549</td>\n",
       "      <td>0.176148</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.949170</td>\n",
       "      <td>0.842781</td>\n",
       "      <td>0.747830</td>\n",
       "      <td>0.676775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968081</td>\n",
       "      <td>-0.803189</td>\n",
       "      <td>-0.608203</td>\n",
       "      <td>1.031017</td>\n",
       "      <td>-0.122003</td>\n",
       "      <td>0.182190</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.567083</td>\n",
       "      <td>0.548102</td>\n",
       "      <td>0.046356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPI  DPCERA3M086SBEA  CMRMTSPLx   RETAILx    INDPRO  \\\n",
       "sasdate                                                                \n",
       "1962-07-01 -1.748613         0.072551  -0.615446  0.715483  0.765023   \n",
       "1962-08-01 -1.745851         0.020216   1.681654  0.100189 -0.090381   \n",
       "1962-09-01 -1.745045         1.062232  -1.454784 -0.107582  0.438811   \n",
       "1962-10-01 -1.735994        -0.952220   1.418380  0.706761 -0.091163   \n",
       "1962-11-01 -1.729678         1.325876   0.692543  0.036096  0.224158   \n",
       "...              ...              ...        ...       ...       ...   \n",
       "2019-10-01  1.336587        -0.322076  -1.086475 -0.100294 -1.056195   \n",
       "2019-11-01  1.342504         0.341565   0.240135  0.111723  0.357385   \n",
       "2019-12-01  1.334565        -0.127958  -0.515598 -0.074221 -0.400964   \n",
       "2020-01-01  1.352504         0.126202  -0.274933 -0.244196 -0.794993   \n",
       "2020-02-01  1.359983        -0.562549   0.176148 -0.304257  0.134579   \n",
       "\n",
       "             IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  IPNCONGD  ...  \\\n",
       "sasdate                                                       ...   \n",
       "1962-07-01 -1.973617 -1.955113 -2.059552  0.587361 -2.128613  ...   \n",
       "1962-08-01 -1.973617 -1.959689 -2.093244 -0.483186 -2.161061  ...   \n",
       "1962-09-01 -1.959528 -1.950547 -2.074819  0.326349 -2.139393  ...   \n",
       "1962-10-01 -1.966561 -1.948264 -2.080945  0.066084 -2.153821  ...   \n",
       "1962-11-01 -1.952524 -1.939170 -2.062598 -0.030449 -2.139393  ...   \n",
       "...              ...       ...       ...       ...       ...  ...   \n",
       "2019-10-01  0.917435  0.939624  0.814720 -1.210923  0.718669  ...   \n",
       "2019-11-01  0.942423  0.971771  0.860001  1.840469  0.725875  ...   \n",
       "2019-12-01  0.938540  0.961971  0.836717 -0.551102  0.706722  ...   \n",
       "2020-01-01  0.922740  0.933475  0.814779  0.277744  0.661632  ...   \n",
       "2020-02-01  0.935713  0.949170  0.842781  0.747830  0.676775  ...   \n",
       "\n",
       "            CUSR0000SA0L2  CUSR0000SA0L5     PCEPI  DDURRG3M086SBEA  \\\n",
       "sasdate                                                               \n",
       "1962-07-01      -1.584209      -0.896592 -1.312770         0.031852   \n",
       "1962-08-01       0.081715       0.090797 -0.494186        -0.146021   \n",
       "1962-09-01       0.906896       0.087596  0.906261         0.062750   \n",
       "1962-10-01      -0.751247      -0.896592 -1.489622        -1.512358   \n",
       "1962-11-01      -0.751247      -0.896592 -0.802510        -0.078988   \n",
       "...                   ...            ...       ...              ...   \n",
       "2019-10-01       0.198502      -0.230922 -0.362669        -0.284042   \n",
       "2019-11-01       0.062068      -0.069329 -0.664476        -0.993822   \n",
       "2019-12-01       0.147283      -0.075272  0.020996        -1.494266   \n",
       "2020-01-01      -0.504065      -0.313399 -0.646589        -0.125531   \n",
       "2020-02-01      -0.968081      -0.803189 -0.608203         1.031017   \n",
       "\n",
       "            DSERRG3M086SBEA  CES0600000008  CES2000000008  CES3000000008  \\\n",
       "sasdate                                                                    \n",
       "1962-07-01        -0.520108       0.254280       0.781746       1.877859   \n",
       "1962-08-01        -0.058863       0.248540      -1.176048      -0.004894   \n",
       "1962-09-01        -0.116100      -1.120352       0.389672      -0.935949   \n",
       "1962-10-01         0.285591      -1.120352      -0.389134       0.930325   \n",
       "1962-11-01         0.227337       0.242849       0.775077      -0.931856   \n",
       "...                     ...            ...            ...            ...   \n",
       "2019-10-01         0.350859      -0.067443      -0.000229       0.381369   \n",
       "2019-11-01        -0.396922      -0.463997       0.078521       0.378289   \n",
       "2019-12-01         1.255018      -0.072909       0.273817      -0.289127   \n",
       "2020-01-01        -0.749275      -0.597883      -0.275785      -0.665177   \n",
       "2020-02-01        -0.122003       0.182190       0.310500       0.567083   \n",
       "\n",
       "              INVEST   VIXCLSx  \n",
       "sasdate                         \n",
       "1962-07-01 -0.099429  0.039390  \n",
       "1962-08-01 -0.422974 -0.495456  \n",
       "1962-09-01 -0.157978 -0.138552  \n",
       "1962-10-01  1.243957  0.944973  \n",
       "1962-11-01 -0.553800 -0.357883  \n",
       "...              ...       ...  \n",
       "2019-10-01 -0.489205 -0.597801  \n",
       "2019-11-01  0.115401 -0.976736  \n",
       "2019-12-01 -0.485797 -0.804671  \n",
       "2020-01-01 -0.556101 -0.742185  \n",
       "2020-02-01  0.548102  0.046356  \n",
       "\n",
       "[692 rows x 106 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When retaining at least 80% of the periods, we end up studting 57,7 years and 106 series. Unfortunately, the oil price is not included in this data set. The finding is similar when at least 70% percent of the periods are retained. This is due to how the very few (5 nans) missing values are spread accros time for oil price ('1959-01-01', '1974-01-01', '2020-03-01', '2020-04-01', '2020-05-01'). In order to benefit from the asymptotic properties of the Diffusion Index forecast, we choose to work with a balanced panel including 57.7 years and 106 series, and excluding the price of oil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the stacked balanced dataset (with the first lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>IPNCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>CUSR0000SA0L2_lag</th>\n",
       "      <th>CUSR0000SA0L5_lag</th>\n",
       "      <th>PCEPI_lag</th>\n",
       "      <th>DDURRG3M086SBEA_lag</th>\n",
       "      <th>DSERRG3M086SBEA_lag</th>\n",
       "      <th>CES0600000008_lag</th>\n",
       "      <th>CES2000000008_lag</th>\n",
       "      <th>CES3000000008_lag</th>\n",
       "      <th>INVEST_lag</th>\n",
       "      <th>VIXCLSx_lag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>-1.745851</td>\n",
       "      <td>0.020216</td>\n",
       "      <td>1.681654</td>\n",
       "      <td>0.100189</td>\n",
       "      <td>-0.090381</td>\n",
       "      <td>-1.973617</td>\n",
       "      <td>-1.959689</td>\n",
       "      <td>-2.093244</td>\n",
       "      <td>-0.483186</td>\n",
       "      <td>-2.161061</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.584209</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-1.312770</td>\n",
       "      <td>0.031852</td>\n",
       "      <td>-0.520108</td>\n",
       "      <td>0.254280</td>\n",
       "      <td>0.781746</td>\n",
       "      <td>1.877859</td>\n",
       "      <td>-0.099429</td>\n",
       "      <td>0.039390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>-1.745045</td>\n",
       "      <td>1.062232</td>\n",
       "      <td>-1.454784</td>\n",
       "      <td>-0.107582</td>\n",
       "      <td>0.438811</td>\n",
       "      <td>-1.959528</td>\n",
       "      <td>-1.950547</td>\n",
       "      <td>-2.074819</td>\n",
       "      <td>0.326349</td>\n",
       "      <td>-2.139393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081715</td>\n",
       "      <td>0.090797</td>\n",
       "      <td>-0.494186</td>\n",
       "      <td>-0.146021</td>\n",
       "      <td>-0.058863</td>\n",
       "      <td>0.248540</td>\n",
       "      <td>-1.176048</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.422974</td>\n",
       "      <td>-0.495456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>-1.735994</td>\n",
       "      <td>-0.952220</td>\n",
       "      <td>1.418380</td>\n",
       "      <td>0.706761</td>\n",
       "      <td>-0.091163</td>\n",
       "      <td>-1.966561</td>\n",
       "      <td>-1.948264</td>\n",
       "      <td>-2.080945</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-2.153821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906896</td>\n",
       "      <td>0.087596</td>\n",
       "      <td>0.906261</td>\n",
       "      <td>0.062750</td>\n",
       "      <td>-0.116100</td>\n",
       "      <td>-1.120352</td>\n",
       "      <td>0.389672</td>\n",
       "      <td>-0.935949</td>\n",
       "      <td>-0.157978</td>\n",
       "      <td>-0.138552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>-1.729678</td>\n",
       "      <td>1.325876</td>\n",
       "      <td>0.692543</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.224158</td>\n",
       "      <td>-1.952524</td>\n",
       "      <td>-1.939170</td>\n",
       "      <td>-2.062598</td>\n",
       "      <td>-0.030449</td>\n",
       "      <td>-2.139393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751247</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-1.489622</td>\n",
       "      <td>-1.512358</td>\n",
       "      <td>0.285591</td>\n",
       "      <td>-1.120352</td>\n",
       "      <td>-0.389134</td>\n",
       "      <td>0.930325</td>\n",
       "      <td>1.243957</td>\n",
       "      <td>0.944973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-12-01</th>\n",
       "      <td>-1.721641</td>\n",
       "      <td>0.368742</td>\n",
       "      <td>-2.421834</td>\n",
       "      <td>-0.313232</td>\n",
       "      <td>-0.196769</td>\n",
       "      <td>-1.950192</td>\n",
       "      <td>-1.932378</td>\n",
       "      <td>-2.050435</td>\n",
       "      <td>0.160851</td>\n",
       "      <td>-2.121448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751247</td>\n",
       "      <td>-0.896592</td>\n",
       "      <td>-0.802510</td>\n",
       "      <td>-0.078988</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.242849</td>\n",
       "      <td>0.775077</td>\n",
       "      <td>-0.931856</td>\n",
       "      <td>-0.553800</td>\n",
       "      <td>-0.357883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>1.336587</td>\n",
       "      <td>-0.322076</td>\n",
       "      <td>-1.086475</td>\n",
       "      <td>-0.100294</td>\n",
       "      <td>-1.056195</td>\n",
       "      <td>0.917435</td>\n",
       "      <td>0.939624</td>\n",
       "      <td>0.814720</td>\n",
       "      <td>-1.210923</td>\n",
       "      <td>0.718669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.582022</td>\n",
       "      <td>-0.445786</td>\n",
       "      <td>-0.835723</td>\n",
       "      <td>-0.105463</td>\n",
       "      <td>-0.067963</td>\n",
       "      <td>-0.196275</td>\n",
       "      <td>-0.159221</td>\n",
       "      <td>-0.192868</td>\n",
       "      <td>1.471991</td>\n",
       "      <td>-0.504207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>1.342504</td>\n",
       "      <td>0.341565</td>\n",
       "      <td>0.240135</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.357385</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>0.971771</td>\n",
       "      <td>0.860001</td>\n",
       "      <td>1.840469</td>\n",
       "      <td>0.725875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>-0.230922</td>\n",
       "      <td>-0.362669</td>\n",
       "      <td>-0.284042</td>\n",
       "      <td>0.350859</td>\n",
       "      <td>-0.067443</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.381369</td>\n",
       "      <td>-0.489205</td>\n",
       "      <td>-0.597801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>1.334565</td>\n",
       "      <td>-0.127958</td>\n",
       "      <td>-0.515598</td>\n",
       "      <td>-0.074221</td>\n",
       "      <td>-0.400964</td>\n",
       "      <td>0.938540</td>\n",
       "      <td>0.961971</td>\n",
       "      <td>0.836717</td>\n",
       "      <td>-0.551102</td>\n",
       "      <td>0.706722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>-0.069329</td>\n",
       "      <td>-0.664476</td>\n",
       "      <td>-0.993822</td>\n",
       "      <td>-0.396922</td>\n",
       "      <td>-0.463997</td>\n",
       "      <td>0.078521</td>\n",
       "      <td>0.378289</td>\n",
       "      <td>0.115401</td>\n",
       "      <td>-0.976736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>1.352504</td>\n",
       "      <td>0.126202</td>\n",
       "      <td>-0.274933</td>\n",
       "      <td>-0.244196</td>\n",
       "      <td>-0.794993</td>\n",
       "      <td>0.922740</td>\n",
       "      <td>0.933475</td>\n",
       "      <td>0.814779</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.661632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147283</td>\n",
       "      <td>-0.075272</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>-1.494266</td>\n",
       "      <td>1.255018</td>\n",
       "      <td>-0.072909</td>\n",
       "      <td>0.273817</td>\n",
       "      <td>-0.289127</td>\n",
       "      <td>-0.485797</td>\n",
       "      <td>-0.804671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>1.359983</td>\n",
       "      <td>-0.562549</td>\n",
       "      <td>0.176148</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>0.935713</td>\n",
       "      <td>0.949170</td>\n",
       "      <td>0.842781</td>\n",
       "      <td>0.747830</td>\n",
       "      <td>0.676775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504065</td>\n",
       "      <td>-0.313399</td>\n",
       "      <td>-0.646589</td>\n",
       "      <td>-0.125531</td>\n",
       "      <td>-0.749275</td>\n",
       "      <td>-0.597883</td>\n",
       "      <td>-0.275785</td>\n",
       "      <td>-0.665177</td>\n",
       "      <td>-0.556101</td>\n",
       "      <td>-0.742185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPI  DPCERA3M086SBEA  CMRMTSPLx   RETAILx    INDPRO  \\\n",
       "sasdate                                                                \n",
       "1962-08-01 -1.745851         0.020216   1.681654  0.100189 -0.090381   \n",
       "1962-09-01 -1.745045         1.062232  -1.454784 -0.107582  0.438811   \n",
       "1962-10-01 -1.735994        -0.952220   1.418380  0.706761 -0.091163   \n",
       "1962-11-01 -1.729678         1.325876   0.692543  0.036096  0.224158   \n",
       "1962-12-01 -1.721641         0.368742  -2.421834 -0.313232 -0.196769   \n",
       "...              ...              ...        ...       ...       ...   \n",
       "2019-10-01  1.336587        -0.322076  -1.086475 -0.100294 -1.056195   \n",
       "2019-11-01  1.342504         0.341565   0.240135  0.111723  0.357385   \n",
       "2019-12-01  1.334565        -0.127958  -0.515598 -0.074221 -0.400964   \n",
       "2020-01-01  1.352504         0.126202  -0.274933 -0.244196 -0.794993   \n",
       "2020-02-01  1.359983        -0.562549   0.176148 -0.304257  0.134579   \n",
       "\n",
       "             IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  IPNCONGD  ...  \\\n",
       "sasdate                                                       ...   \n",
       "1962-08-01 -1.973617 -1.959689 -2.093244 -0.483186 -2.161061  ...   \n",
       "1962-09-01 -1.959528 -1.950547 -2.074819  0.326349 -2.139393  ...   \n",
       "1962-10-01 -1.966561 -1.948264 -2.080945  0.066084 -2.153821  ...   \n",
       "1962-11-01 -1.952524 -1.939170 -2.062598 -0.030449 -2.139393  ...   \n",
       "1962-12-01 -1.950192 -1.932378 -2.050435  0.160851 -2.121448  ...   \n",
       "...              ...       ...       ...       ...       ...  ...   \n",
       "2019-10-01  0.917435  0.939624  0.814720 -1.210923  0.718669  ...   \n",
       "2019-11-01  0.942423  0.971771  0.860001  1.840469  0.725875  ...   \n",
       "2019-12-01  0.938540  0.961971  0.836717 -0.551102  0.706722  ...   \n",
       "2020-01-01  0.922740  0.933475  0.814779  0.277744  0.661632  ...   \n",
       "2020-02-01  0.935713  0.949170  0.842781  0.747830  0.676775  ...   \n",
       "\n",
       "            CUSR0000SA0L2_lag  CUSR0000SA0L5_lag  PCEPI_lag  \\\n",
       "sasdate                                                       \n",
       "1962-08-01          -1.584209          -0.896592  -1.312770   \n",
       "1962-09-01           0.081715           0.090797  -0.494186   \n",
       "1962-10-01           0.906896           0.087596   0.906261   \n",
       "1962-11-01          -0.751247          -0.896592  -1.489622   \n",
       "1962-12-01          -0.751247          -0.896592  -0.802510   \n",
       "...                       ...                ...        ...   \n",
       "2019-10-01          -0.582022          -0.445786  -0.835723   \n",
       "2019-11-01           0.198502          -0.230922  -0.362669   \n",
       "2019-12-01           0.062068          -0.069329  -0.664476   \n",
       "2020-01-01           0.147283          -0.075272   0.020996   \n",
       "2020-02-01          -0.504065          -0.313399  -0.646589   \n",
       "\n",
       "            DDURRG3M086SBEA_lag  DSERRG3M086SBEA_lag  CES0600000008_lag  \\\n",
       "sasdate                                                                   \n",
       "1962-08-01             0.031852            -0.520108           0.254280   \n",
       "1962-09-01            -0.146021            -0.058863           0.248540   \n",
       "1962-10-01             0.062750            -0.116100          -1.120352   \n",
       "1962-11-01            -1.512358             0.285591          -1.120352   \n",
       "1962-12-01            -0.078988             0.227337           0.242849   \n",
       "...                         ...                  ...                ...   \n",
       "2019-10-01            -0.105463            -0.067963          -0.196275   \n",
       "2019-11-01            -0.284042             0.350859          -0.067443   \n",
       "2019-12-01            -0.993822            -0.396922          -0.463997   \n",
       "2020-01-01            -1.494266             1.255018          -0.072909   \n",
       "2020-02-01            -0.125531            -0.749275          -0.597883   \n",
       "\n",
       "            CES2000000008_lag  CES3000000008_lag  INVEST_lag  VIXCLSx_lag  \n",
       "sasdate                                                                    \n",
       "1962-08-01           0.781746           1.877859   -0.099429     0.039390  \n",
       "1962-09-01          -1.176048          -0.004894   -0.422974    -0.495456  \n",
       "1962-10-01           0.389672          -0.935949   -0.157978    -0.138552  \n",
       "1962-11-01          -0.389134           0.930325    1.243957     0.944973  \n",
       "1962-12-01           0.775077          -0.931856   -0.553800    -0.357883  \n",
       "...                       ...                ...         ...          ...  \n",
       "2019-10-01          -0.159221          -0.192868    1.471991    -0.504207  \n",
       "2019-11-01          -0.000229           0.381369   -0.489205    -0.597801  \n",
       "2019-12-01           0.078521           0.378289    0.115401    -0.976736  \n",
       "2020-01-01           0.273817          -0.289127   -0.485797    -0.804671  \n",
       "2020-02-01          -0.275785          -0.665177   -0.556101    -0.742185  \n",
       "\n",
       "[691 rows x 212 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the lagged version of the balanced panel (using pandas .shift method)\n",
    "stacked_balanced = balanced.copy()  # Start with the original data\n",
    "\n",
    "# Create a list of the lagged columns\n",
    "lagged_columns = [balanced[col].shift(1).rename(f'{col}_lag') for col in balanced.columns]\n",
    "\n",
    "# Concatenate the lagged columns to the original data (axis=1 means column-wise concatenation)\n",
    "stacked_balanced = pd.concat([stacked_balanced] + lagged_columns, axis=1)\n",
    "\n",
    "# Remove rows with NaN values generated due to lagging (first row will have NaN in the lag columns)\n",
    "stacked_balanced = stacked_balanced.dropna()\n",
    "\n",
    "stacked_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion index forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced panel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA with function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just once, I want to compare the result using sklearn PCA to the manual PCA (written by hand). I do this because I find the optimal number of factor using the manual PCA. You will see that results between both methods are similar in terms of magnitude, which is what matter because the signs of the figures are the signs of the eigenvectors which are **sign-invariant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PCA()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA without limiting the number of components\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the data (this will keep all components)\n",
    "pca.fit(balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five components explain 50% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the first 8 components based on the scree plot\n",
    "num_factors = 8\n",
    "pca_12 = PCA(n_components=8)\n",
    "F_train_12 = pca_12.fit_transform(balanced) # It is an array\n",
    "F_train_12_df = pd.DataFrame(F_train_12, index=balanced.index, columns=[f'Factor_{i+1}' for i in range(num_factors)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "      <th>Factor_6</th>\n",
       "      <th>Factor_7</th>\n",
       "      <th>Factor_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-07-01</th>\n",
       "      <td>-2.430850</td>\n",
       "      <td>3.288939</td>\n",
       "      <td>-6.468471</td>\n",
       "      <td>1.410870</td>\n",
       "      <td>0.772791</td>\n",
       "      <td>-0.029055</td>\n",
       "      <td>-0.901984</td>\n",
       "      <td>0.438726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>-3.106295</td>\n",
       "      <td>2.064480</td>\n",
       "      <td>-5.589702</td>\n",
       "      <td>1.433361</td>\n",
       "      <td>0.190168</td>\n",
       "      <td>-0.888419</td>\n",
       "      <td>-1.985162</td>\n",
       "      <td>1.216483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>-4.347294</td>\n",
       "      <td>1.728258</td>\n",
       "      <td>-4.388553</td>\n",
       "      <td>2.466327</td>\n",
       "      <td>1.120924</td>\n",
       "      <td>-0.994633</td>\n",
       "      <td>-2.222960</td>\n",
       "      <td>-0.462809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>-2.098093</td>\n",
       "      <td>2.928615</td>\n",
       "      <td>-6.965651</td>\n",
       "      <td>-0.585348</td>\n",
       "      <td>0.612791</td>\n",
       "      <td>0.217091</td>\n",
       "      <td>-0.523194</td>\n",
       "      <td>-0.836990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>-2.945525</td>\n",
       "      <td>2.541101</td>\n",
       "      <td>-6.111085</td>\n",
       "      <td>0.473879</td>\n",
       "      <td>-0.572175</td>\n",
       "      <td>-1.052905</td>\n",
       "      <td>-1.874775</td>\n",
       "      <td>2.344298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2.071037</td>\n",
       "      <td>-0.744045</td>\n",
       "      <td>2.416860</td>\n",
       "      <td>-2.496280</td>\n",
       "      <td>-0.196736</td>\n",
       "      <td>0.045330</td>\n",
       "      <td>-2.475431</td>\n",
       "      <td>0.572766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>2.433603</td>\n",
       "      <td>1.629193</td>\n",
       "      <td>2.740348</td>\n",
       "      <td>-0.809214</td>\n",
       "      <td>0.480843</td>\n",
       "      <td>-0.660092</td>\n",
       "      <td>-0.844145</td>\n",
       "      <td>1.125069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>2.357629</td>\n",
       "      <td>0.541123</td>\n",
       "      <td>2.969504</td>\n",
       "      <td>-1.220142</td>\n",
       "      <td>-0.364273</td>\n",
       "      <td>-0.748274</td>\n",
       "      <td>-2.351318</td>\n",
       "      <td>0.885292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>2.946509</td>\n",
       "      <td>1.313604</td>\n",
       "      <td>1.638149</td>\n",
       "      <td>-2.237702</td>\n",
       "      <td>-0.466551</td>\n",
       "      <td>-0.857865</td>\n",
       "      <td>-1.031139</td>\n",
       "      <td>1.382650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>3.125814</td>\n",
       "      <td>1.590556</td>\n",
       "      <td>0.510654</td>\n",
       "      <td>-3.957114</td>\n",
       "      <td>0.544546</td>\n",
       "      <td>-0.096441</td>\n",
       "      <td>1.335229</td>\n",
       "      <td>-0.103891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Factor_1  Factor_2  Factor_3  Factor_4  Factor_5  Factor_6  \\\n",
       "sasdate                                                                  \n",
       "1962-07-01 -2.430850  3.288939 -6.468471  1.410870  0.772791 -0.029055   \n",
       "1962-08-01 -3.106295  2.064480 -5.589702  1.433361  0.190168 -0.888419   \n",
       "1962-09-01 -4.347294  1.728258 -4.388553  2.466327  1.120924 -0.994633   \n",
       "1962-10-01 -2.098093  2.928615 -6.965651 -0.585348  0.612791  0.217091   \n",
       "1962-11-01 -2.945525  2.541101 -6.111085  0.473879 -0.572175 -1.052905   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2019-10-01  2.071037 -0.744045  2.416860 -2.496280 -0.196736  0.045330   \n",
       "2019-11-01  2.433603  1.629193  2.740348 -0.809214  0.480843 -0.660092   \n",
       "2019-12-01  2.357629  0.541123  2.969504 -1.220142 -0.364273 -0.748274   \n",
       "2020-01-01  2.946509  1.313604  1.638149 -2.237702 -0.466551 -0.857865   \n",
       "2020-02-01  3.125814  1.590556  0.510654 -3.957114  0.544546 -0.096441   \n",
       "\n",
       "            Factor_7  Factor_8  \n",
       "sasdate                         \n",
       "1962-07-01 -0.901984  0.438726  \n",
       "1962-08-01 -1.985162  1.216483  \n",
       "1962-09-01 -2.222960 -0.462809  \n",
       "1962-10-01 -0.523194 -0.836990  \n",
       "1962-11-01 -1.874775  2.344298  \n",
       "...              ...       ...  \n",
       "2019-10-01 -2.475431  0.572766  \n",
       "2019-11-01 -0.844145  1.125069  \n",
       "2019-12-01 -2.351318  0.885292  \n",
       "2020-01-01 -1.031139  1.382650  \n",
       "2020-02-01  1.335229 -0.103891  \n",
       "\n",
       "[692 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_train_12_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "      <th>Factor_6</th>\n",
       "      <th>Factor_7</th>\n",
       "      <th>Factor_8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-07-01</th>\n",
       "      <td>2.416050</td>\n",
       "      <td>-3.188035</td>\n",
       "      <td>6.305992</td>\n",
       "      <td>-1.348435</td>\n",
       "      <td>0.736512</td>\n",
       "      <td>0.065724</td>\n",
       "      <td>-0.767882</td>\n",
       "      <td>-0.401182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>3.091496</td>\n",
       "      <td>-1.963577</td>\n",
       "      <td>5.427223</td>\n",
       "      <td>-1.370929</td>\n",
       "      <td>0.153887</td>\n",
       "      <td>0.925096</td>\n",
       "      <td>-1.851201</td>\n",
       "      <td>-1.178756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>4.332494</td>\n",
       "      <td>-1.627355</td>\n",
       "      <td>4.226074</td>\n",
       "      <td>-2.403894</td>\n",
       "      <td>1.084650</td>\n",
       "      <td>1.031296</td>\n",
       "      <td>-2.088563</td>\n",
       "      <td>0.499153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>2.083294</td>\n",
       "      <td>-2.827711</td>\n",
       "      <td>6.803172</td>\n",
       "      <td>0.647782</td>\n",
       "      <td>0.576512</td>\n",
       "      <td>-0.180411</td>\n",
       "      <td>-0.389203</td>\n",
       "      <td>0.875287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>2.930725</td>\n",
       "      <td>-2.440197</td>\n",
       "      <td>5.948606</td>\n",
       "      <td>-0.411447</td>\n",
       "      <td>-0.608458</td>\n",
       "      <td>1.089573</td>\n",
       "      <td>-1.740696</td>\n",
       "      <td>-2.307449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>-2.085837</td>\n",
       "      <td>0.844949</td>\n",
       "      <td>-2.579339</td>\n",
       "      <td>2.558711</td>\n",
       "      <td>-0.233017</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>-2.341466</td>\n",
       "      <td>-0.534855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>-2.448402</td>\n",
       "      <td>-1.528289</td>\n",
       "      <td>-2.902827</td>\n",
       "      <td>0.871648</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>0.696752</td>\n",
       "      <td>-0.710093</td>\n",
       "      <td>-1.088009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>-2.372429</td>\n",
       "      <td>-0.440219</td>\n",
       "      <td>-3.131984</td>\n",
       "      <td>1.282575</td>\n",
       "      <td>-0.400553</td>\n",
       "      <td>0.784951</td>\n",
       "      <td>-2.217382</td>\n",
       "      <td>-0.847214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>-2.961308</td>\n",
       "      <td>-1.212701</td>\n",
       "      <td>-1.800629</td>\n",
       "      <td>2.300131</td>\n",
       "      <td>-0.502825</td>\n",
       "      <td>0.894571</td>\n",
       "      <td>-0.897259</td>\n",
       "      <td>-1.343543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>-3.140613</td>\n",
       "      <td>-1.489653</td>\n",
       "      <td>-0.673133</td>\n",
       "      <td>4.019548</td>\n",
       "      <td>0.508266</td>\n",
       "      <td>0.133121</td>\n",
       "      <td>1.469186</td>\n",
       "      <td>0.142538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Factor_1  Factor_2  Factor_3  Factor_4  Factor_5  Factor_6  \\\n",
       "sasdate                                                                  \n",
       "1962-07-01  2.416050 -3.188035  6.305992 -1.348435  0.736512  0.065724   \n",
       "1962-08-01  3.091496 -1.963577  5.427223 -1.370929  0.153887  0.925096   \n",
       "1962-09-01  4.332494 -1.627355  4.226074 -2.403894  1.084650  1.031296   \n",
       "1962-10-01  2.083294 -2.827711  6.803172  0.647782  0.576512 -0.180411   \n",
       "1962-11-01  2.930725 -2.440197  5.948606 -0.411447 -0.608458  1.089573   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2019-10-01 -2.085837  0.844949 -2.579339  2.558711 -0.233017 -0.008641   \n",
       "2019-11-01 -2.448402 -1.528289 -2.902827  0.871648  0.444558  0.696752   \n",
       "2019-12-01 -2.372429 -0.440219 -3.131984  1.282575 -0.400553  0.784951   \n",
       "2020-01-01 -2.961308 -1.212701 -1.800629  2.300131 -0.502825  0.894571   \n",
       "2020-02-01 -3.140613 -1.489653 -0.673133  4.019548  0.508266  0.133121   \n",
       "\n",
       "            Factor_7  Factor_8  \n",
       "sasdate                         \n",
       "1962-07-01 -0.767882 -0.401182  \n",
       "1962-08-01 -1.851201 -1.178756  \n",
       "1962-09-01 -2.088563  0.499153  \n",
       "1962-10-01 -0.389203  0.875287  \n",
       "1962-11-01 -1.740696 -2.307449  \n",
       "...              ...       ...  \n",
       "2019-10-01 -2.341466 -0.534855  \n",
       "2019-11-01 -0.710093 -1.088009  \n",
       "2019-12-01 -2.217382 -0.847214  \n",
       "2020-01-01 -0.897259 -1.343543  \n",
       "2020-02-01  1.469186  0.142538  \n",
       "\n",
       "[692 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_pca_standardized(X, num_factors):\n",
    "    # 1. Since the data is already standardized, we don't need to center it.\n",
    "    #    Directly compute the covariance matrix from the standardized data.\n",
    "    covariance_matrix = np.cov(X, rowvar=False)\n",
    "\n",
    "    # 2. Perform Eigenvalue Decomposition on the covariance matrix\n",
    "    eigvals, eigvecs = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # 3. Sort the eigenvalues in descending order, and get the corresponding eigenvectors\n",
    "    sorted_indices = np.argsort(eigvals)[::-1]\n",
    "    eigvals_sorted = eigvals[sorted_indices]\n",
    "    eigvecs_sorted = eigvecs[:, sorted_indices]\n",
    "\n",
    "    # 4. Select the top 'num_factors' eigenvectors\n",
    "    selected_eigvecs = eigvecs_sorted[:, :num_factors]\n",
    "\n",
    "    # 5. Transform the data by projecting it onto the selected eigenvectors\n",
    "    F = np.dot(X, selected_eigvecs)\n",
    "\n",
    "    # Convert the results into a DataFrame with appropriate column names\n",
    "    F_df = pd.DataFrame(F, index=X.index, columns=[f'Factor_{i+1}' for i in range(num_factors)])\n",
    "\n",
    "    return F_df, eigvals_sorted[:num_factors], selected_eigvecs\n",
    "\n",
    "\n",
    "num_factors = 8\n",
    "F_train_12, eigvals, eigvecs = manual_pca_standardized(balanced, num_factors)\n",
    "F_train_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pure PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minindc(X):\n",
    "    ''' =========================================================================\n",
    "     takes np <-> returns np\n",
    "     DESCRIPTION\n",
    "     This function finds the index of the minimum value for each column of a\n",
    "     given matrix. The function assumes that the minimum value of each column\n",
    "     occurs only once within that column. The function returns an error if\n",
    "     this is not the case.\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     INPUT\n",
    "               x   = matrix\n",
    "\n",
    "     OUTPUT\n",
    "               pos = column vector with pos(i) containing the row number\n",
    "                     corresponding to the minimum value of x(:,i)\n",
    "\n",
    "     ========================================================================= '''\n",
    "\n",
    "    mins = X.argmin(axis=0) # returns the indices of the minimum values along each column, axis=0 specifies that the operation is done column-wise (for each column).\n",
    "    assert sum(X == X[mins]) == 1, 'Minimum value occurs more than once.'\n",
    "    return mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baing(X,kmax,jj):\n",
    "    #take in and return numpy arrays\n",
    "    ''' =========================================================================\n",
    "    DESCRIPTION\n",
    "    This function determines the number of factors to be selected for a given\n",
    "    dataset using one of three information criteria specified by the user.\n",
    "    The user also specifies the maximum number of factors to be selected.\n",
    "\n",
    "    -------------------------------------------------------------------------\n",
    "    INPUTS\n",
    "               X       = dataset (one series per column)\n",
    "               kmax    = an integer indicating the maximum number of factors\n",
    "                         to be estimated\n",
    "               jj      = an integer indicating the information criterion used\n",
    "                         for selecting the number of factors; it can take on\n",
    "                         the following values:\n",
    "                               1 (information criterion PC_p1)\n",
    "                               2 (information criterion PC_p2)\n",
    "                               3 (information criterion PC_p3)\n",
    "\n",
    "     OUTPUTS\n",
    "               ic1     = number of factors selected\n",
    "               chat    = values of X predicted by the factors\n",
    "               Fhat    = factors\n",
    "               eigval  = eivenvalues of X'*X (or X*X' if N>T)\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     SUBFUNCTIONS USED\n",
    "\n",
    "     minindc() - finds the index of the minimum value for each column of a given matrix\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     BREAKDOWN OF THE FUNCTION\n",
    "\n",
    "     Part 1: Setup.\n",
    "\n",
    "     Part 2: Calculate the overfitting penalty for each possible number of\n",
    "             factors to be selected (from 1 to kmax).\n",
    "\n",
    "     Part 3: Select the number of factors that minimizes the specified\n",
    "             information criterion by utilizing the overfitting penalties calculated in Part 2.\n",
    "\n",
    "     Part 4: Save other output variables to be returned by the function (chat,\n",
    "             Fhat, and eigval).\n",
    "\n",
    "    ========================================================================= '''\n",
    "    assert kmax <= X.shape[1] and  kmax >= 1 and np.floor(kmax) == kmax or kmax == 99, 'kmax is specified incorrectly'\n",
    "    assert jj in [1, 2, 3], 'jj is specified incorrectly'\n",
    "\n",
    "\n",
    "    #  PART 1: SETUP\n",
    "\n",
    "    T = X.shape[0]  # Number of observations per series (i.e. number of rows)\n",
    "    N = X.shape[1]  # Number of series (i.e. number of columns)\n",
    "    NT = N * T      # Total number of observations\n",
    "    NT1 = N + T     # Number of rows + columns\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 2: OVERFITTING PENALTY\n",
    "    #  Determine penalty for overfitting based on the selected information\n",
    "    #  criterion.\n",
    "\n",
    "    CT = np.zeros(kmax) # overfitting penalty\n",
    "    ii = np.arange(1, kmax + 1)     # Array containing possible number of factors that can be selected (1 to kmax)\n",
    "    GCT = min(N,T)                  # The smaller of N and T\n",
    "\n",
    "    # Calculate penalty based on criterion determined by jj.\n",
    "    if jj == 1:             # Criterion PC_p1\n",
    "        CT[:] = np.log(NT / NT1) * ii * (NT1 / NT)\n",
    "\n",
    "    elif jj == 2:             # Criterion PC_p2\n",
    "        CT[:] = np.log(min(N, T)) * ii * (NT1 / NT)\n",
    "\n",
    "    elif jj == 3:             # Criterion PC_p3\n",
    "        CT[:] = np.log(GCT) / GCT * ii\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 3: SELECT NUMBER OF FACTORS\n",
    "    #  Perform principal component analysis on the dataset and select the number\n",
    "    #  of factors that minimizes the specified information criterion.\n",
    "    #\n",
    "    #  -------------------------------------------------------------------------\n",
    "    #  RUN STANDARD PRINCIPAL COMPONENT ANALYSIS\n",
    "    #  Get components, loadings, and eigenvalues\n",
    "    # There are two different cases depending on whether T<N or T≥N. This distinction comes from computational efficiency and numerical stability when \n",
    "    # performing Principal Component Analysis (PCA) using Singular Value Decomposition (SVD).\n",
    "    # PCA is typically performed on the covariance matrix X′X or XX′. The choice of which matrix to use depends on the dimensions of X, specifically whether \n",
    "    # T (number of observations) is smaller or larger than N (number of variables). And therefore the order of the matrix in SVD is changed.\n",
    "\n",
    "    if T < N:\n",
    "        ev, eigval, V = np.linalg.svd(np.dot(X, X.T))       #  Singular value decomposition\n",
    "        Fhat0 = ev                               #  Components\n",
    "        Lambda0 = np.dot(X.T, Fhat0)                    #  Loadings\n",
    "    else:\n",
    "        ev, eigval, V = np.linalg.svd(np.dot(X.T, X))       #  Singular value decomposition\n",
    "        Lambda0 = ev                             #  Loadings\n",
    "        Fhat0 = np.dot(X, Lambda0)                      #  Components\n",
    "    #  -------------------------------------------------------------------------\n",
    "\n",
    "    # SELECT NUMBER OF FACTORS\n",
    "    # Preallocate memory\n",
    "    Sigma = np.zeros(kmax + 1)          # sum of squared residuals divided by NT, kmax factors + no factor\n",
    "    IC1 = np.zeros(kmax + 1)            # information criterion value, kmax factors + no factor\n",
    "\n",
    "    for i in range(0, kmax) :           # Loop through all possibilites for the number of factors\n",
    "        Fhat = Fhat0[:, :i+1]           # Identify factors as first i components\n",
    "        lambda_ = Lambda0[:, :i+1]       #     % Identify factor loadings as first i loadings\n",
    "\n",
    "        chat = np.dot(Fhat, lambda_.T)      #     % Predict X using i factors\n",
    "        ehat = X - chat                 # Residuals from predicting X using the factors\n",
    "        Sigma[i] = ((ehat*ehat/T).sum(axis = 0)).mean()    # Sum of squared residuals divided by NT\n",
    "\n",
    "        IC1[i] = np.log(Sigma[i]) + CT[i]      #  Value of the information criterion when using i factors\n",
    "\n",
    "\n",
    "    Sigma[kmax] = (X*X/T).sum(axis = 0).mean()  # Sum of squared residuals when using no factors to predict X (i.e. fitted values are set to 0)\n",
    "\n",
    "    IC1[kmax] =  np.log(Sigma[kmax]) # % Value of the information criterion when using no factors\n",
    "\n",
    "    ic1 = minindc(IC1) # % Number of factors that minimizes the information criterion\n",
    "    # Set ic1=0 if ic1>kmax (i.e. no factors are selected if the value of the\n",
    "    # information criterion is minimized when no factors are used)\n",
    "    ic1 = ic1 *(ic1 < kmax) # if = kmax -> 0\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 4: SAVE OTHER OUTPUT\n",
    "    #\n",
    "    #  Factors and loadings when number of factors set to kmax\n",
    "\n",
    "    Fhat = Fhat0[:, :kmax] # factors\n",
    "    Lambda = Lambda0[:, :kmax] #factor loadings\n",
    "\n",
    "    chat = np.dot(Fhat, Lambda.T) #     Predict X using kmax factors\n",
    "\n",
    "    return ic1+1, chat, Fhat, eigval\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_factor, estimated_data, factors_balance, pcs_variance = baing(balanced, 4, 2)\n",
    "factors_balance = pd.DataFrame(factors_balance, index = balanced.index)\n",
    "number_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-07-01</th>\n",
       "      <td>2.418711</td>\n",
       "      <td>-3.243823</td>\n",
       "      <td>6.297375</td>\n",
       "      <td>-1.318881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>3.093948</td>\n",
       "      <td>-2.015817</td>\n",
       "      <td>5.435545</td>\n",
       "      <td>-1.351130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>4.334594</td>\n",
       "      <td>-1.670850</td>\n",
       "      <td>4.241110</td>\n",
       "      <td>-2.391855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>2.085730</td>\n",
       "      <td>-2.882786</td>\n",
       "      <td>6.783883</td>\n",
       "      <td>0.681543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>2.933170</td>\n",
       "      <td>-2.492886</td>\n",
       "      <td>5.943360</td>\n",
       "      <td>-0.384075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>-2.086438</td>\n",
       "      <td>0.862803</td>\n",
       "      <td>-2.571406</td>\n",
       "      <td>2.535279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>-2.448567</td>\n",
       "      <td>-1.508989</td>\n",
       "      <td>-2.908778</td>\n",
       "      <td>0.852565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>-2.372798</td>\n",
       "      <td>-0.420062</td>\n",
       "      <td>-3.127862</td>\n",
       "      <td>1.259174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>-2.961295</td>\n",
       "      <td>-1.200742</td>\n",
       "      <td>-1.810127</td>\n",
       "      <td>2.287277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>-3.140649</td>\n",
       "      <td>-1.480209</td>\n",
       "      <td>-0.701923</td>\n",
       "      <td>4.017121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3\n",
       "sasdate                                           \n",
       "1962-07-01  2.418711 -3.243823  6.297375 -1.318881\n",
       "1962-08-01  3.093948 -2.015817  5.435545 -1.351130\n",
       "1962-09-01  4.334594 -1.670850  4.241110 -2.391855\n",
       "1962-10-01  2.085730 -2.882786  6.783883  0.681543\n",
       "1962-11-01  2.933170 -2.492886  5.943360 -0.384075\n",
       "...              ...       ...       ...       ...\n",
       "2019-10-01 -2.086438  0.862803 -2.571406  2.535279\n",
       "2019-11-01 -2.448567 -1.508989 -2.908778  0.852565\n",
       "2019-12-01 -2.372798 -0.420062 -3.127862  1.259174\n",
       "2020-01-01 -2.961295 -1.200742 -1.810127  2.287277\n",
       "2020-02-01 -3.140649 -1.480209 -0.701923  4.017121\n",
       "\n",
       "[692 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the lagged values, training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged versions of the factors with renamed columns regarding the original complete dataset\n",
    "factors_balance_0 = factors_balance.shift(6).add_suffix('_lag1')\n",
    "factors_balance_1 = factors_balance.shift(7).add_suffix('_lag2')\n",
    "factors_balance_2 = factors_balance.shift(8).add_suffix('_lag3')\n",
    "factors_balance_3 = factors_balance.shift(9).add_suffix('_lag4')\n",
    "\n",
    "# Create lagged versions of the target variable with renamed columns regarding the original complete data set\n",
    "Y_target = balanced['CMRMTSPLx']\n",
    "Y_target_1 = Y_target.shift(6).rename('Y_lag1')\n",
    "Y_target_2 = Y_target.shift(7).rename('Y_lag2')\n",
    "Y_target_3 = Y_target.shift(8).rename('Y_lag3')\n",
    "Y_target_4 = Y_target.shift(9).rename('Y_lag4')\n",
    "Y_target_5 = Y_target.shift(10).rename('Y_lag5')\n",
    "Y_target_6 = Y_target.shift(11).rename('Y_lag6')\n",
    "Y_target_7 = Y_target.shift(12).rename('Y_lag7')\n",
    "\n",
    "# ==============================================================================\n",
    "# Define train-test split \n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(balanced))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training data:\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_1_train = factors_balance_1.iloc[:train_size]\n",
    "factors_balance_2_train = factors_balance_2.iloc[:train_size]\n",
    "factors_balance_3_train = factors_balance_3.iloc[:train_size]\n",
    "\n",
    "Y_target_train = Y_target.iloc[:train_size]\n",
    "Y_train_1 = Y_target_1.iloc[:train_size]\n",
    "Y_train_2 = Y_target_2.iloc[:train_size]\n",
    "Y_train_3 = Y_target_3.iloc[:train_size]\n",
    "Y_train_4 = Y_target_4.iloc[:train_size]\n",
    "Y_train_5 = Y_target_5.iloc[:train_size]\n",
    "Y_train_6 = Y_target_6.iloc[:train_size]\n",
    "Y_train_7 = Y_target_7.iloc[:train_size]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Testing data:\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]\n",
    "factors_balance_1_test = factors_balance_1.iloc[train_size:]\n",
    "factors_balance_2_test = factors_balance_2.iloc[train_size:]\n",
    "factors_balance_3_test = factors_balance_3.iloc[train_size:]\n",
    "\n",
    "Y_target_test = Y_target.iloc[train_size:]\n",
    "Y_test_1 = Y_target_1.iloc[train_size:]\n",
    "Y_test_2 = Y_target_2.iloc[train_size:]\n",
    "Y_test_3 = Y_target_3.iloc[train_size:]\n",
    "Y_test_4 = Y_target_4.iloc[train_size:]\n",
    "Y_test_5 = Y_target_5.iloc[train_size:]\n",
    "Y_test_6 = Y_target_6.iloc[train_size:]\n",
    "Y_test_7 = Y_target_7.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI-AR, Lag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def best_bic_model(Y_train, factors_balance_0, factors_lags, Y_lags):\n",
    "\n",
    "    \"\"\"=========================================================================\n",
    "    Finds the best regression model by minimizing BIC.\n",
    "    \n",
    "    ----------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "    \n",
    "                Y_train (pd.Series): Target variable (dependent variable)\n",
    "                factors_balance_0 (pd.DataFrame): Baseline explanatory variables (8 factors)\n",
    "                factors_lags (list of pd.DataFrame): List of lagged factor datasets\n",
    "                Y_lags (list of pd.Series): List of lagged Y_train series\n",
    "    \n",
    "    ----------------------------------------------------------------------------    \n",
    "    Returns:\n",
    "                best_model: The best model in order to use it later for predictions\n",
    "                best_bic: The BIC value of the best model\n",
    "                best_vars: The specification of the best model\n",
    "    =========================================================================\"\"\"\n",
    "\n",
    "    # Store all possible lagged variables in a single dictionary\n",
    "    all_lagged_vars = {'factors_balance_0': factors_balance_0}  # Start with baseline model\n",
    "    all_lagged_vars.update({f'factors_balance_{i+1}': df for i, df in enumerate(factors_lags)})  # Factor lags\n",
    "    all_lagged_vars.update({f'Y_train_{i+1}': Y_lag for i, Y_lag in enumerate(Y_lags)})  # Y lags\n",
    "\n",
    "    # Generate all possible combinations of ONLY lagged variables\n",
    "    variable_names = list(all_lagged_vars.keys()) # these are the names of the data frames\n",
    "    variable_names.remove('factors_balance_0')  # Remove base variables from combination set\n",
    "    all_combinations = []\n",
    "    for r in range(0, len(variable_names) + 1):  # Start from 0 so we can test the base model (factors_balance_0), this line is used to find all the possible combinations because it generates all possible groups of size r\n",
    "        for subset in itertools.combinations(variable_names, r): # Create all the specifications of size r and store them in a list (thanks to 'extend')\n",
    "            all_combinations.append(('factors_balance_0',) + subset)  # Always include base factors\n",
    "\n",
    "    best_bic = np.inf\n",
    "    best_model = None\n",
    "    best_vars = None\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # Concatenate selected variables into a single DataFrame\n",
    "        selected_vars = [all_lagged_vars[var] for var in combination]\n",
    "        X = pd.concat(selected_vars, axis=1).dropna()  # Merge and drop NaN rows # axis=1 tells pandas to concatenate them horizontally (column-wise), ie. combine side by side (adds new columns)\n",
    "        \n",
    "        Y_train_filtered = Y_train.loc[X.index]  # Align Y_train with X\n",
    "\n",
    "        # Add constant term\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        # Fit the regression model\n",
    "        model = sm.OLS(Y_train_filtered, X).fit()\n",
    "\n",
    "        # Compute BIC\n",
    "        bic = model.bic\n",
    "\n",
    "        # Check if this model has the lowest BIC\n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best_model = model\n",
    "            best_vars = combination\n",
    "\n",
    "    return best_model, best_bic, best_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "factors_lags = [factors_balance_1_train, factors_balance_2_train, factors_balance_3_train]\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_Lag_model, DI_AR_Lag_bic, DI_AR_Lag_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('factors_balance_0', 'Y_train_6')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 29 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>3.59e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:23:25</td>     <th>  Log-Likelihood:    </th> <td> -746.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   611</td>      <th>  AIC:               </th> <td>   1505.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   605</td>      <th>  BIC:               </th> <td>   1531.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>    0.0214</td> <td>    0.034</td> <td>    0.636</td> <td> 0.525</td> <td>   -0.045</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th> <td>   -0.0142</td> <td>    0.008</td> <td>   -1.750</td> <td> 0.081</td> <td>   -0.030</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th> <td>   -0.0189</td> <td>    0.010</td> <td>   -1.812</td> <td> 0.070</td> <td>   -0.039</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th> <td>    0.0404</td> <td>    0.012</td> <td>    3.357</td> <td> 0.001</td> <td>    0.017</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th> <td>   -0.0324</td> <td>    0.014</td> <td>   -2.353</td> <td> 0.019</td> <td>   -0.059</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th> <td>    0.1227</td> <td>    0.040</td> <td>    3.060</td> <td> 0.002</td> <td>    0.044</td> <td>    0.201</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.457</td> <th>  Durbin-Watson:     </th> <td>   2.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  24.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.048</td> <th>  Prob(JB):          </th> <td>3.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.984</td> <th>  Cond. No.          </th> <td>    5.02</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.053   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.045   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     6.788   \\\\\n",
       "\\textbf{Date:}             & Sat, 29 Mar 2025 & \\textbf{  Prob (F-statistic):} &  3.59e-06   \\\\\n",
       "\\textbf{Time:}             &     13:23:25     & \\textbf{  Log-Likelihood:    } &   -746.49   \\\\\n",
       "\\textbf{No. Observations:} &         611      & \\textbf{  AIC:               } &     1505.   \\\\\n",
       "\\textbf{Df Residuals:}     &         605      & \\textbf{  BIC:               } &     1531.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &       0.0214  &        0.034     &     0.636  &         0.525        &       -0.045    &        0.088     \\\\\n",
       "\\textbf{0\\_lag1} &      -0.0142  &        0.008     &    -1.750  &         0.081        &       -0.030    &        0.002     \\\\\n",
       "\\textbf{1\\_lag1} &      -0.0189  &        0.010     &    -1.812  &         0.070        &       -0.039    &        0.002     \\\\\n",
       "\\textbf{2\\_lag1} &       0.0404  &        0.012     &     3.357  &         0.001        &        0.017    &        0.064     \\\\\n",
       "\\textbf{3\\_lag1} &      -0.0324  &        0.014     &    -2.353  &         0.019        &       -0.059    &       -0.005     \\\\\n",
       "\\textbf{Y\\_lag6} &       0.1227  &        0.040     &     3.060  &         0.002        &        0.044    &        0.201     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13.457 & \\textbf{  Durbin-Watson:     } &    2.415  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   24.884  \\\\\n",
       "\\textbf{Skew:}          & -0.048 & \\textbf{  Prob(JB):          } & 3.95e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.984 & \\textbf{  Cond. No.          } &     5.02  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.053\n",
       "Model:                            OLS   Adj. R-squared:                  0.045\n",
       "Method:                 Least Squares   F-statistic:                     6.788\n",
       "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           3.59e-06\n",
       "Time:                        13:23:25   Log-Likelihood:                -746.49\n",
       "No. Observations:                 611   AIC:                             1505.\n",
       "Df Residuals:                     605   BIC:                             1531.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0214      0.034      0.636      0.525      -0.045       0.088\n",
       "0_lag1        -0.0142      0.008     -1.750      0.081      -0.030       0.002\n",
       "1_lag1        -0.0189      0.010     -1.812      0.070      -0.039       0.002\n",
       "2_lag1         0.0404      0.012      3.357      0.001       0.017       0.064\n",
       "3_lag1        -0.0324      0.014     -2.353      0.019      -0.059      -0.005\n",
       "Y_lag6         0.1227      0.040      3.060      0.002       0.044       0.201\n",
       "==============================================================================\n",
       "Omnibus:                       13.457   Durbin-Watson:                   2.415\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               24.884\n",
       "Skew:                          -0.048   Prob(JB):                     3.95e-06\n",
       "Kurtosis:                       3.984   Cond. No.                         5.02\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.sandwich_covariance as sw\n",
    "\n",
    "def direct_forecast(model, factors_lags_test, Y_lags_test, Y_target_test):\n",
    "\n",
    "    \"\"\"============================================================================\n",
    "    Perform direct forecasts using the estimated regression model.\n",
    "\n",
    "    -----------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "        model (statsmodels OLS object): Estimated model from training.\n",
    "        factors_lags_test (list of pd.DataFrame): List of test lags for factor balance (including factors_balance_0_test).\n",
    "        Y_lags_test (list of pd.Series): List of test lags for Y.\n",
    "        Y_target_test (pd.Series): True values of Y in the test set.\n",
    "\n",
    "    -----------------------------------------------------------------------------\n",
    "    Returns:\n",
    "        pd.Series: Forecasted values indexed like Y_target_test.\n",
    "        float: Mean Squared Error (printed).\n",
    "    ============================================================================\"\"\"\n",
    "\n",
    "    # Merge all test variables into a single DataFrame\n",
    "    X_test = pd.concat(factors_lags_test + Y_lags_test, axis=1).dropna()  # Ensure correct alignment\n",
    "    X_test = sm.add_constant(X_test)  # Add constant term\n",
    "\n",
    "    # Ensure test data columns match model's coefficients\n",
    "    X_test = X_test[model.params.index] # model.params.index gives the names of the variables used in training\n",
    "\n",
    "    # Make predictions\n",
    "    forecasts = model.predict(X_test)\n",
    "\n",
    "    # Compute forecast errors\n",
    "    errors = Y_target_test.loc[X_test.index] - forecasts\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = np.mean(errors ** 2)\n",
    "\n",
    "    # Compute Newey-West standard error for MSE:\n",
    "    T = len(errors)  # Sample size\n",
    "    lag_length = int(np.floor(4 * (T / 100) ** (2 / 9)))  # Automatic lag selection\n",
    "    # Step 1: Regress squared errors on a constant\n",
    "    squared_errors = errors**2\n",
    "    X_const = sm.add_constant(np.ones(T))  # Constant-only regression, the constant term in this regression reflects the mean squared error (MSE)\n",
    "    hac_model = sm.OLS(squared_errors, X_const).fit(cov_type='HAC', cov_kwds={'maxlags': lag_length})\n",
    "    # Step 2: Extract standard error of the constant term (which is MSE)\n",
    "    newey_west_se = np.sqrt(hac_model.bse.iloc[0])  # Standard error of MSE\n",
    "    # Round results\n",
    "    newey_west_se = round(newey_west_se, 2)\n",
    "\n",
    "    # Return forecasts as Pandas Series\n",
    "    return pd.Series(forecasts, index=X_test.index), mse, newey_west_se\n",
    "\n",
    "# Function Call:\n",
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_6]\n",
    "\n",
    "DI_AR_Lag_forecast, DI_AR_Lag_MSE, DI_AR_Lag_se = direct_forecast(DI_AR_Lag_model, factors_lags_test, Y_lags_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145\n",
      "0.16\n"
     ]
    }
   ],
   "source": [
    "print(f\"{DI_AR_Lag_MSE:.3f}\")\n",
    "print(f\"{DI_AR_Lag_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI-AR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "number_factor, estimated_data, factors_balance, pcs_variance = baing(balanced, 12, 2)\n",
    "factors_balance = pd.DataFrame(factors_balance, index = balanced.index)\n",
    "print(number_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute again because the factors have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_balance_0 = factors_balance.shift(6).add_suffix('_lag1')\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_model, DI_AR_bic, DI_AR_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 29 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>7.90e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:43:02</td>     <th>  Log-Likelihood:    </th> <td> -739.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   611</td>      <th>  AIC:               </th> <td>   1506.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   597</td>      <th>  BIC:               </th> <td>   1568.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    0.0182</td> <td>    0.034</td> <td>    0.533</td> <td> 0.594</td> <td>   -0.049</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.0132</td> <td>    0.008</td> <td>   -1.604</td> <td> 0.109</td> <td>   -0.029</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>   -0.0178</td> <td>    0.010</td> <td>   -1.717</td> <td> 0.087</td> <td>   -0.038</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>    0.0407</td> <td>    0.012</td> <td>    3.384</td> <td> 0.001</td> <td>    0.017</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.0332</td> <td>    0.014</td> <td>   -2.417</td> <td> 0.016</td> <td>   -0.060</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>    0.0448</td> <td>    0.015</td> <td>    3.036</td> <td> 0.002</td> <td>    0.016</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.0023</td> <td>    0.016</td> <td>   -0.141</td> <td> 0.888</td> <td>   -0.034</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>    0.0109</td> <td>    0.018</td> <td>    0.596</td> <td> 0.552</td> <td>   -0.025</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>    0.0404</td> <td>    0.021</td> <td>    1.886</td> <td> 0.060</td> <td>   -0.002</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>    0.0208</td> <td>    0.024</td> <td>    0.854</td> <td> 0.394</td> <td>   -0.027</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>    0.0022</td> <td>    0.024</td> <td>    0.092</td> <td> 0.927</td> <td>   -0.046</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>    0.0089</td> <td>    0.026</td> <td>    0.344</td> <td> 0.731</td> <td>   -0.042</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>   -0.0085</td> <td>    0.027</td> <td>   -0.316</td> <td> 0.752</td> <td>   -0.061</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th>  <td>    0.1319</td> <td>    0.041</td> <td>    3.248</td> <td> 0.001</td> <td>    0.052</td> <td>    0.212</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.562</td> <th>  Durbin-Watson:     </th> <td>   2.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  25.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.011</td> <th>  Prob(JB):          </th> <td>2.78e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.002</td> <th>  Cond. No.          </th> <td>    5.17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.076   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.056   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3.769   \\\\\n",
       "\\textbf{Date:}             & Sat, 29 Mar 2025 & \\textbf{  Prob (F-statistic):} &  7.90e-06   \\\\\n",
       "\\textbf{Time:}             &     11:43:02     & \\textbf{  Log-Likelihood:    } &   -739.07   \\\\\n",
       "\\textbf{No. Observations:} &         611      & \\textbf{  AIC:               } &     1506.   \\\\\n",
       "\\textbf{Df Residuals:}     &         597      & \\textbf{  BIC:               } &     1568.   \\\\\n",
       "\\textbf{Df Model:}         &          13      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &       0.0182  &        0.034     &     0.533  &         0.594        &       -0.049    &        0.085     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.0132  &        0.008     &    -1.604  &         0.109        &       -0.029    &        0.003     \\\\\n",
       "\\textbf{1\\_lag1}  &      -0.0178  &        0.010     &    -1.717  &         0.087        &       -0.038    &        0.003     \\\\\n",
       "\\textbf{2\\_lag1}  &       0.0407  &        0.012     &     3.384  &         0.001        &        0.017    &        0.064     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.0332  &        0.014     &    -2.417  &         0.016        &       -0.060    &       -0.006     \\\\\n",
       "\\textbf{4\\_lag1}  &       0.0448  &        0.015     &     3.036  &         0.002        &        0.016    &        0.074     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.0023  &        0.016     &    -0.141  &         0.888        &       -0.034    &        0.029     \\\\\n",
       "\\textbf{6\\_lag1}  &       0.0109  &        0.018     &     0.596  &         0.552        &       -0.025    &        0.047     \\\\\n",
       "\\textbf{7\\_lag1}  &       0.0404  &        0.021     &     1.886  &         0.060        &       -0.002    &        0.082     \\\\\n",
       "\\textbf{8\\_lag1}  &       0.0208  &        0.024     &     0.854  &         0.394        &       -0.027    &        0.069     \\\\\n",
       "\\textbf{9\\_lag1}  &       0.0022  &        0.024     &     0.092  &         0.927        &       -0.046    &        0.050     \\\\\n",
       "\\textbf{10\\_lag1} &       0.0089  &        0.026     &     0.344  &         0.731        &       -0.042    &        0.059     \\\\\n",
       "\\textbf{11\\_lag1} &      -0.0085  &        0.027     &    -0.316  &         0.752        &       -0.061    &        0.044     \\\\\n",
       "\\textbf{Y\\_lag6}  &       0.1319  &        0.041     &     3.248  &         0.001        &        0.052    &        0.212     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13.562 & \\textbf{  Durbin-Watson:     } &    2.455  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   25.583  \\\\\n",
       "\\textbf{Skew:}          &  0.011 & \\textbf{  Prob(JB):          } & 2.78e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.002 & \\textbf{  Cond. No.          } &     5.17  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.076\n",
       "Model:                            OLS   Adj. R-squared:                  0.056\n",
       "Method:                 Least Squares   F-statistic:                     3.769\n",
       "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           7.90e-06\n",
       "Time:                        11:43:02   Log-Likelihood:                -739.07\n",
       "No. Observations:                 611   AIC:                             1506.\n",
       "Df Residuals:                     597   BIC:                             1568.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0182      0.034      0.533      0.594      -0.049       0.085\n",
       "0_lag1        -0.0132      0.008     -1.604      0.109      -0.029       0.003\n",
       "1_lag1        -0.0178      0.010     -1.717      0.087      -0.038       0.003\n",
       "2_lag1         0.0407      0.012      3.384      0.001       0.017       0.064\n",
       "3_lag1        -0.0332      0.014     -2.417      0.016      -0.060      -0.006\n",
       "4_lag1         0.0448      0.015      3.036      0.002       0.016       0.074\n",
       "5_lag1        -0.0023      0.016     -0.141      0.888      -0.034       0.029\n",
       "6_lag1         0.0109      0.018      0.596      0.552      -0.025       0.047\n",
       "7_lag1         0.0404      0.021      1.886      0.060      -0.002       0.082\n",
       "8_lag1         0.0208      0.024      0.854      0.394      -0.027       0.069\n",
       "9_lag1         0.0022      0.024      0.092      0.927      -0.046       0.050\n",
       "10_lag1        0.0089      0.026      0.344      0.731      -0.042       0.059\n",
       "11_lag1       -0.0085      0.027     -0.316      0.752      -0.061       0.044\n",
       "Y_lag6         0.1319      0.041      3.248      0.001       0.052       0.212\n",
       "==============================================================================\n",
       "Omnibus:                       13.562   Durbin-Watson:                   2.455\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               25.583\n",
       "Skew:                           0.011   Prob(JB):                     2.78e-06\n",
       "Kurtosis:                       4.002   Cond. No.                         5.17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.146\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_6]\n",
    "DI_AR_forecast, DI_AR_MSE, DI_AR_se = direct_forecast(DI_AR_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_AR_MSE:.3f}\")\n",
    "print(f\"{DI_AR_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same number of factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = []\n",
    "\n",
    "DI_model, DI_bic, DI_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 29 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>0.000126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:43:09</td>     <th>  Log-Likelihood:    </th> <td> -750.81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   616</td>      <th>  AIC:               </th> <td>   1528.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   603</td>      <th>  BIC:               </th> <td>   1585.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    0.0243</td> <td>    0.034</td> <td>    0.711</td> <td> 0.477</td> <td>   -0.043</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.0121</td> <td>    0.008</td> <td>   -1.464</td> <td> 0.144</td> <td>   -0.028</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>   -0.0262</td> <td>    0.010</td> <td>   -2.578</td> <td> 0.010</td> <td>   -0.046</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>    0.0390</td> <td>    0.012</td> <td>    3.290</td> <td> 0.001</td> <td>    0.016</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.0309</td> <td>    0.014</td> <td>   -2.237</td> <td> 0.026</td> <td>   -0.058</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>    0.0416</td> <td>    0.015</td> <td>    2.803</td> <td> 0.005</td> <td>    0.012</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.0047</td> <td>    0.016</td> <td>   -0.290</td> <td> 0.772</td> <td>   -0.037</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>    0.0129</td> <td>    0.018</td> <td>    0.704</td> <td> 0.482</td> <td>   -0.023</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>    0.0357</td> <td>    0.021</td> <td>    1.661</td> <td> 0.097</td> <td>   -0.007</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>    0.0267</td> <td>    0.024</td> <td>    1.099</td> <td> 0.272</td> <td>   -0.021</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>    0.0007</td> <td>    0.025</td> <td>    0.029</td> <td> 0.977</td> <td>   -0.047</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>    0.0005</td> <td>    0.026</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.050</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>   -0.0085</td> <td>    0.027</td> <td>   -0.316</td> <td> 0.752</td> <td>   -0.062</td> <td>    0.044</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.092</td> <th>  Durbin-Watson:     </th> <td>   2.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  26.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.033</td> <th>  Prob(JB):          </th> <td>1.50e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.020</td> <th>  Cond. No.          </th> <td>    4.32</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.061   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.043   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3.288   \\\\\n",
       "\\textbf{Date:}             & Sat, 29 Mar 2025 & \\textbf{  Prob (F-statistic):} &  0.000126   \\\\\n",
       "\\textbf{Time:}             &     11:43:09     & \\textbf{  Log-Likelihood:    } &   -750.81   \\\\\n",
       "\\textbf{No. Observations:} &         616      & \\textbf{  AIC:               } &     1528.   \\\\\n",
       "\\textbf{Df Residuals:}     &         603      & \\textbf{  BIC:               } &     1585.   \\\\\n",
       "\\textbf{Df Model:}         &          12      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &       0.0243  &        0.034     &     0.711  &         0.477        &       -0.043    &        0.092     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.0121  &        0.008     &    -1.464  &         0.144        &       -0.028    &        0.004     \\\\\n",
       "\\textbf{1\\_lag1}  &      -0.0262  &        0.010     &    -2.578  &         0.010        &       -0.046    &       -0.006     \\\\\n",
       "\\textbf{2\\_lag1}  &       0.0390  &        0.012     &     3.290  &         0.001        &        0.016    &        0.062     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.0309  &        0.014     &    -2.237  &         0.026        &       -0.058    &       -0.004     \\\\\n",
       "\\textbf{4\\_lag1}  &       0.0416  &        0.015     &     2.803  &         0.005        &        0.012    &        0.071     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.0047  &        0.016     &    -0.290  &         0.772        &       -0.037    &        0.027     \\\\\n",
       "\\textbf{6\\_lag1}  &       0.0129  &        0.018     &     0.704  &         0.482        &       -0.023    &        0.049     \\\\\n",
       "\\textbf{7\\_lag1}  &       0.0357  &        0.021     &     1.661  &         0.097        &       -0.007    &        0.078     \\\\\n",
       "\\textbf{8\\_lag1}  &       0.0267  &        0.024     &     1.099  &         0.272        &       -0.021    &        0.074     \\\\\n",
       "\\textbf{9\\_lag1}  &       0.0007  &        0.025     &     0.029  &         0.977        &       -0.047    &        0.049     \\\\\n",
       "\\textbf{10\\_lag1} &       0.0005  &        0.026     &     0.019  &         0.985        &       -0.050    &        0.051     \\\\\n",
       "\\textbf{11\\_lag1} &      -0.0085  &        0.027     &    -0.316  &         0.752        &       -0.062    &        0.044     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 14.092 & \\textbf{  Durbin-Watson:     } &    2.467  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   26.823  \\\\\n",
       "\\textbf{Skew:}          &  0.033 & \\textbf{  Prob(JB):          } & 1.50e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.020 & \\textbf{  Cond. No.          } &     4.32  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.061\n",
       "Model:                            OLS   Adj. R-squared:                  0.043\n",
       "Method:                 Least Squares   F-statistic:                     3.288\n",
       "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           0.000126\n",
       "Time:                        11:43:09   Log-Likelihood:                -750.81\n",
       "No. Observations:                 616   AIC:                             1528.\n",
       "Df Residuals:                     603   BIC:                             1585.\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0243      0.034      0.711      0.477      -0.043       0.092\n",
       "0_lag1        -0.0121      0.008     -1.464      0.144      -0.028       0.004\n",
       "1_lag1        -0.0262      0.010     -2.578      0.010      -0.046      -0.006\n",
       "2_lag1         0.0390      0.012      3.290      0.001       0.016       0.062\n",
       "3_lag1        -0.0309      0.014     -2.237      0.026      -0.058      -0.004\n",
       "4_lag1         0.0416      0.015      2.803      0.005       0.012       0.071\n",
       "5_lag1        -0.0047      0.016     -0.290      0.772      -0.037       0.027\n",
       "6_lag1         0.0129      0.018      0.704      0.482      -0.023       0.049\n",
       "7_lag1         0.0357      0.021      1.661      0.097      -0.007       0.078\n",
       "8_lag1         0.0267      0.024      1.099      0.272      -0.021       0.074\n",
       "9_lag1         0.0007      0.025      0.029      0.977      -0.047       0.049\n",
       "10_lag1        0.0005      0.026      0.019      0.985      -0.050       0.051\n",
       "11_lag1       -0.0085      0.027     -0.316      0.752      -0.062       0.044\n",
       "==============================================================================\n",
       "Omnibus:                       14.092   Durbin-Watson:                   2.467\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               26.823\n",
       "Skew:                           0.033   Prob(JB):                     1.50e-06\n",
       "Kurtosis:                       4.020   Cond. No.                         4.32\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.152\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = []\n",
    "DI_forecast, DI_MSE, DI_se = direct_forecast(DI_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_MSE:.3f}\")\n",
    "print(f\"{DI_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from group_lasso import GroupLasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress the ConvergenceWarning from scikit-learn\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def time_series_regression(dependent_var, regressors, regression_type, groups=None, alpha_range=None, l1_ratio_range=None):\n",
    "    \"\"\"\n",
    "    Perform LASSO, ElasticNet, or Group LASSO regression on time series data with hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - dependent_var (pd.Series): The dependent variable (target time series).\n",
    "    - regressors (pd.DataFrame): The explanatory variables (time series).\n",
    "    - regression_type (int): The type of regression to perform.\n",
    "        - 1 = LASSO\n",
    "        - 2 = ElasticNet\n",
    "        - 3 = Group LASSO\n",
    "    - groups (list, optional): List indicating the group of variables for Group LASSO (only required for group lasso).\n",
    "    - alpha_range (list, optional): List of values to try for the `alpha` parameter (regularization strength).\n",
    "    - l1_ratio_range (list, optional): List of values to try for the `l1_ratio` parameter (ElasticNet mixing parameter).\n",
    "\n",
    "    Returns:\n",
    "    - model: The fitted regression model.\n",
    "    \"\"\"\n",
    "    # Ensure that regressors are a DataFrame and dependent_var is a Series\n",
    "    X = regressors.copy()\n",
    "    y = dependent_var.copy()\n",
    "\n",
    "    # Add constant term ONLY for LASSO and ElasticNet\n",
    "    if regression_type in [1, 2]:  \n",
    "        X = sm.add_constant(X)  # Adds intercept column\n",
    "\n",
    "    # Perform LASSO or ElasticNet or Group LASSO based on regression_type\n",
    "    if regression_type == 1:  # LASSO\n",
    "        print(\"Performing LASSO Regression...\")\n",
    "        model = Lasso()\n",
    "        \n",
    "        # Hyperparameter tuning using GridSearchCV\n",
    "        if alpha_range is not None:\n",
    "            param_grid = {'alpha': alpha_range}\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "            grid_search.fit(X, y)\n",
    "            model = grid_search.best_estimator_\n",
    "\n",
    "    elif regression_type == 2:  # ElasticNet\n",
    "        print(\"Performing ElasticNet Regression...\")\n",
    "        model = ElasticNet()\n",
    "\n",
    "        # Ensure hyperparameter tuning is happening\n",
    "        if alpha_range is not None and l1_ratio_range is not None:\n",
    "            param_grid = {'alpha': alpha_range, 'l1_ratio': l1_ratio_range}\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "            grid_search.fit(X, y)\n",
    "            model = grid_search.best_estimator_\n",
    "\n",
    "            print(f\"Best Elastic Net Alpha: {grid_search.best_params_['alpha']}\")\n",
    "            print(f\"Best Elastic Net l1_ratio: {grid_search.best_params_['l1_ratio']}\")\n",
    "\n",
    "    elif regression_type == 3:  # Group LASSO\n",
    "        if groups is None:\n",
    "            raise ValueError(\"For Group LASSO, 'groups' must be provided.\")\n",
    "        print(\"Performing Group LASSO Regression...\")\n",
    "        model = GroupLasso(groups=groups, fit_intercept=False, supress_warning=True) # Set fit_intercept=False when the data is already deamened otherwise set it to True\n",
    "        \n",
    "        # Hyperparameter tuning using GridSearchCV\n",
    "        if alpha_range is not None:\n",
    "            param_grid = {'l1_reg': alpha_range}\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "            grid_search.fit(X, y)\n",
    "            model = grid_search.best_estimator_\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression_type. Use 1 for LASSO, 2 for ElasticNet, or 3 for Group LASSO.\")\n",
    "    \n",
    "    print(f\"Best model parameters: {model.get_params()}\")\n",
    "    return model\n",
    "\n",
    "# Penalization parameters\n",
    "alpha_range = np.logspace(-5, 5, 50)\n",
    "l1_ratio_range = np.linspace(0.1, 1.0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_lagged = balanced.shift(6)\n",
    "balanced_lagged_train = balanced_lagged.iloc[:train_size].dropna()\n",
    "balanced_lagged_test = balanced_lagged.iloc[train_size:]\n",
    "Y_target_train_adjusted = Y_target_train.loc[balanced_lagged_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LASSO Regression...\n",
      "Best model parameters: {'alpha': np.float64(0.04714866363457394), 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=1, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def direct_forecast_2(model, regressor, target):\n",
    "    \"\"\"\n",
    "    Perform direct forecasting using a trained model (LASSO, ElasticNet, or Group LASSO).\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained regression model (LASSO, ElasticNet, or Group LASSO).\n",
    "    - balanced_lagged_test (pd.DataFrame): The test set regressors (lagged factors).\n",
    "    - Y_target_test (pd.Series): The true values of the dependent variable for the test set.\n",
    "    \n",
    "    Returns:\n",
    "    - forecasted_values (pd.Series): Forecasted values indexed the same as Y_target_test.\n",
    "    - mse (float): Mean squared error of the forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add constant to the test data (intercept term for regression)\n",
    "    X_test = sm.add_constant(regressor)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    forecasts = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE (Mean Squared Error)\n",
    "    mse = mean_squared_error(Y_target_test, forecasts)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
    "\n",
    "    # Return the forecasted values as a Pandas Series\n",
    "    forecasted_values = pd.Series(forecasts, index=target.index)\n",
    "    \n",
    "    return forecasted_values, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.160\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_lasso, mse_lasso = direct_forecast_2(model_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ElasticNet Regression...\n",
      "Best Elastic Net Alpha: 0.1206792640639329\n",
      "Best Elastic Net l1_ratio: 0.3571428571428572\n",
      "Best model parameters: {'alpha': np.float64(0.1206792640639329), 'copy_X': True, 'fit_intercept': True, 'l1_ratio': np.float64(0.3571428571428572), 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_elastic_net = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=2, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.160\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_elastic_net, mse_elastic_net = direct_forecast_2(model_elastic_net, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some variables in the dataset were not assigned to any group.\n"
     ]
    }
   ],
   "source": [
    "def construct_groups(balanced_lagged_train):\n",
    "    \"\"\"\n",
    "    Construct the 'groups' array for Group LASSO based on predefined economic indicator groups.\n",
    "\n",
    "    Parameters:\n",
    "    - balanced_lagged_train (pd.DataFrame): DataFrame containing the regressors.\n",
    "\n",
    "    Returns:\n",
    "    - groups (np.array): An array of group assignments corresponding to the columns of balanced_lagged_train.\n",
    "    \"\"\"\n",
    "    # Define groups with variable names\n",
    "    group_dict = {\n",
    "        1: [\"RPI\", \"W875RX1\", \"INDPRO\", \"IPFPNSS\", \"IPFINAL\", \"IPCONGD\", \"IPDCONGD\", \"IPNCONGD\", \"IPBUSEQ\",\n",
    "            \"IPMAT\", \"IPDMAT\", \"IPNMAT\", \"IPMANSICS\", \"IPB51222s\", \"IPFUELS\", \"CUMFNS\"],\n",
    "        2: [\"HWI\", \"HWIURATIO\", \"CLF16OV\", \"CE16OV\", \"UNRATE\", \"UEMPMEAN\", \"UEMPLT5\", \"UEMP5TO14\", \"UEMP15OV\",\n",
    "            \"UEMP15T26\", \"UEMP27OV\", \"CLAIMSx\", \"PAYEMS\", \"USGOOD\", \"CES1021000001\", \"USCONS\", \"MANEMP\",\n",
    "            \"DMANEMP\", \"NDMANEMP\", \"SRVPRD\", \"USTPU\", \"USWTRADE\", \"USTRADE\", \"USFIRE\", \"USGOVT\",\n",
    "            \"CES0600000007\", \"AWOTMAN\", \"AWHMAN\", \"CES0600000008\", \"CES2000000008\", \"CES3000000008\"],\n",
    "        3: [\"HOUST\", \"HOUSTNE\", \"HOUSTMW\", \"HOUSTS\", \"HOUSTW\", \"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\", \"PERMITW\"],\n",
    "        4: [\"DPCERA3M086SBEA\", \"CMRMTSPLx\", \"RETAILx\", \"ACOGNO\", \"AMDMNOx\", \"ANDENOx\", \"AMDMUOx\", \"BUSINVx\",\n",
    "            \"ISRATIOx\", \"UMCSENTx\"],\n",
    "        5: [\"M1SL\", \"M2SL\", \"M2REAL\", \"BOGMBASE\", \"TOTRESNS\", \"NONBORRES\", \"BUSLOANS\", \"REALLN\", \"NONREVSL\",\n",
    "            \"CONSPI\", \"DTCOLNVHFNM\", \"DTCTHFNM\", \"INVEST\"],\n",
    "        6: [\"FEDFUNDS\", \"CP3Mx\", \"TB3MS\", \"TB6MS\", \"GS1\", \"GS5\", \"GS10\", \"AAA\", \"BAA\", \"COMPAPFFx\",\n",
    "            \"TB3SMFFM\", \"TB6SMFFM\", \"T1YFFM\", \"T5YFFM\", \"T10YFFM\", \"AAAFFM\", \"BAAFFM\", \"TWEXAFEGSMTHx\",\n",
    "            \"EXSZUSx\", \"EXJPUSx\", \"EXUSUKx\", \"EXCAUSx\"],\n",
    "        7: [\"WPSFD49207\", \"WPSFD49502\", \"WPSID61\", \"WPSID62\", \"OILPRICEx\", \"PPICMM\", \"CPIAUCSL\", \"CPIAPPSL\",\n",
    "            \"CPITRNSL\", \"CPIMEDSL\", \"CUSR0000SAC\", \"CUSR0000SAD\", \"CUSR0000SAS\", \"CPIULFSL\", \"CUSR0000SA0L2\",\n",
    "            \"CUSR0000SA0L5\", \"PCEPI\", \"DDURRG3M086SBEA\", \"DNDGRG3M086SBEA\", \"DSERRG3M086SBEA\"],\n",
    "        8: [\"S&P 500\", \"S&P div yield\", \"S&P PE ratio\", \"VIXCLSx\"]\n",
    "    }\n",
    "\n",
    "    # Extract the columns of the dataset\n",
    "    dataset_columns = balanced_lagged_train.columns.tolist()\n",
    "\n",
    "    # Initialize an empty array to store group assignments\n",
    "    groups = np.zeros(len(dataset_columns), dtype=int)\n",
    "\n",
    "    # Assign each variable to a group if it exists in `balanced_lagged_train`\n",
    "    for group_num, variables in group_dict.items():\n",
    "        for var in variables:\n",
    "            if var in dataset_columns:\n",
    "                groups[dataset_columns.index(var)] = group_num\n",
    "\n",
    "    # Ensure all variables are assigned a group\n",
    "    if np.any(groups == 0):\n",
    "        print(\"Warning: Some variables in the dataset were not assigned to any group.\")\n",
    "\n",
    "    return groups\n",
    "\n",
    "# function call\n",
    "groups = construct_groups(balanced_lagged_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Group  Count  Proportion\n",
      "0      0      1    0.009434\n",
      "1      1     14    0.132075\n",
      "2      2     30    0.283019\n",
      "3      3     10    0.094340\n",
      "4      4      6    0.056604\n",
      "5      5      7    0.066038\n",
      "6      6     17    0.160377\n",
      "7      7     17    0.160377\n",
      "8      8      4    0.037736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_group_proportions(groups):\n",
    "    \"\"\"\n",
    "    Compute and display the proportion of variables assigned to each group.\n",
    "\n",
    "    Parameters:\n",
    "    - groups (np.array): Array of group assignments for variables.\n",
    "\n",
    "    Returns:\n",
    "    - group_proportions (pd.DataFrame): DataFrame showing group counts and proportions.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each group\n",
    "    unique_groups, counts = np.unique(groups, return_counts=True)\n",
    "\n",
    "    # Compute proportions\n",
    "    total_vars = len(groups)\n",
    "    proportions = counts / total_vars\n",
    "\n",
    "    # Create a DataFrame for readability\n",
    "    group_proportions = pd.DataFrame({\n",
    "        'Group': unique_groups,\n",
    "        'Count': counts,\n",
    "        'Proportion': proportions\n",
    "    })\n",
    "\n",
    "    # Sort by group number\n",
    "    group_proportions = group_proportions.sort_values(by='Group')\n",
    "\n",
    "    return group_proportions\n",
    "\n",
    "# Compute and display proportions\n",
    "group_proportions = compute_group_proportions(groups)\n",
    "print(group_proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one variable has no group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Group LASSO Regression...\n",
      "Best model parameters: {'fit_intercept': False, 'frobenius_lipschitz': False, 'group_reg': 0.05, 'groups': array([1, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 8, 8,\n",
      "       8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 5, 8]), 'l1_reg': np.float64(1e-05), 'n_iter': 100, 'old_regularisation': False, 'random_state': None, 'scale_reg': 'group_size', 'subsampling_scheme': None, 'supress_warning': True, 'tol': 1e-05, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_group_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=3, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.152\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_group_lasso, mse_group_lasso = direct_forecast_2(model_group_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecast combining regression** with Heteroskedasticity and Autocorrelation Robust (HAC) standard errors using the Newey-West estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_combining_regression(forecast_1, forecast_2, y_actual, max_lags=5):\n",
    "    \"\"\"\n",
    "    Performs a forecast combination regression:\n",
    "    \n",
    "        Y_target_test = alpha * forecast_1 + (1 - alpha) * forecast_2 + error\n",
    "    \n",
    "    with heteroskedasticity and autocorrelation robust (HAC) standard errors.\n",
    "\n",
    "    Parameters:\n",
    "    - forecast_1 (pd.DataFrame or pd.Series): First forecast (e.g., from Group LASSO).\n",
    "    - forecast_2 (pd.DataFrame or pd.Series): Second forecast (e.g., from DI_AR_Lag model).\n",
    "    - y_actual (pd.DataFrame or pd.Series): Actual target values.\n",
    "    - max_lags (int, optional): Number of lags for HAC standard errors (default: 5 -> already seen on internet by finance people).\n",
    "    \n",
    "    Returns:\n",
    "    - alpha (float): Estimated weight for forecast_1.\n",
    "    - p_value (float): P-value of alpha.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure inputs are Pandas Series\n",
    "    forecast_1 = forecast_1.squeeze()\n",
    "    forecast_2 = forecast_2.squeeze()\n",
    "    y_actual = y_actual.squeeze()\n",
    "    \n",
    "    # Create the usual regression structure: Y = alpha * forecast_1 + (1 - alpha) * forecast_2\n",
    "    X = forecast_1.to_frame(name=\"forecast_1\")  # Independent variable: forecast_1\n",
    "    X[\"forecast_2\"] = forecast_2  # Independent variable: forecast_2\n",
    "    \n",
    "    # Add constant (intercept)\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit OLS regression with HAC robust standard errors\n",
    "    model = sm.OLS(y_actual, X).fit(cov_type='HAC', cov_kwds={'maxlags': max_lags})\n",
    "    \n",
    "    # Extract alpha estimate and p-value (for forecast_1)\n",
    "    alpha = model.params[\"forecast_1\"]\n",
    "    p_value = model.pvalues[\"forecast_1\"]\n",
    "    \n",
    "    return alpha, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.30\n",
      "P-value: 0.68\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_AR_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.24\n",
      "P-value: 0.77\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.49\n",
      "P-value: 0.65\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.37\n",
      "P-value: 0.72\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_elastic_net, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 109.19\n",
      "P-value: 0.01\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_group_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The stacked balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_factor, estimated_data, factors_stack_balance, pcs_variance = baing(stacked_balanced, 4, 2)\n",
    "factors_stack_balance= pd.DataFrame(factors_stack_balance, index = stacked_balanced.index)\n",
    "number_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1962-08-01</th>\n",
       "      <td>4.235530</td>\n",
       "      <td>-3.190213</td>\n",
       "      <td>8.678066</td>\n",
       "      <td>0.435747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-09-01</th>\n",
       "      <td>5.461861</td>\n",
       "      <td>-2.265202</td>\n",
       "      <td>7.412822</td>\n",
       "      <td>-0.453765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-10-01</th>\n",
       "      <td>4.848141</td>\n",
       "      <td>-2.722581</td>\n",
       "      <td>8.013966</td>\n",
       "      <td>1.069665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-11-01</th>\n",
       "      <td>3.944725</td>\n",
       "      <td>-3.244271</td>\n",
       "      <td>8.990376</td>\n",
       "      <td>2.110172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962-12-01</th>\n",
       "      <td>4.189962</td>\n",
       "      <td>-2.890795</td>\n",
       "      <td>8.880881</td>\n",
       "      <td>1.744444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>-3.607227</td>\n",
       "      <td>0.088312</td>\n",
       "      <td>-3.839854</td>\n",
       "      <td>3.237828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>-3.330456</td>\n",
       "      <td>-0.860262</td>\n",
       "      <td>-3.976584</td>\n",
       "      <td>1.678136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>-3.519861</td>\n",
       "      <td>-1.802092</td>\n",
       "      <td>-4.197273</td>\n",
       "      <td>1.090795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>-3.834596</td>\n",
       "      <td>-1.577499</td>\n",
       "      <td>-3.644279</td>\n",
       "      <td>1.759192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>-4.186292</td>\n",
       "      <td>-2.133589</td>\n",
       "      <td>-2.573945</td>\n",
       "      <td>3.564036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3\n",
       "sasdate                                           \n",
       "1962-08-01  4.235530 -3.190213  8.678066  0.435747\n",
       "1962-09-01  5.461861 -2.265202  7.412822 -0.453765\n",
       "1962-10-01  4.848141 -2.722581  8.013966  1.069665\n",
       "1962-11-01  3.944725 -3.244271  8.990376  2.110172\n",
       "1962-12-01  4.189962 -2.890795  8.880881  1.744444\n",
       "...              ...       ...       ...       ...\n",
       "2019-10-01 -3.607227  0.088312 -3.839854  3.237828\n",
       "2019-11-01 -3.330456 -0.860262 -3.976584  1.678136\n",
       "2019-12-01 -3.519861 -1.802092 -4.197273  1.090795\n",
       "2020-01-01 -3.834596 -1.577499 -3.644279  1.759192\n",
       "2020-02-01 -4.186292 -2.133589 -2.573945  3.564036\n",
       "\n",
       "[691 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_stack_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the lagged values, training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged versions of the factors with renamed columns regarding the original complete dataset\n",
    "factors_balance_0 = factors_stack_balance.shift(6).add_suffix('_lag1')\n",
    "factors_balance_1 = factors_stack_balance.shift(7).add_suffix('_lag2')\n",
    "factors_balance_2 = factors_stack_balance.shift(8).add_suffix('_lag3')\n",
    "factors_balance_3 = factors_stack_balance.shift(9).add_suffix('_lag4')\n",
    "\n",
    "Y_target = stacked_balanced['CMRMTSPLx']\n",
    "Y_target_1 = Y_target.shift(6).rename('Y_lag1')\n",
    "Y_target_2 = Y_target.shift(7).rename('Y_lag2')\n",
    "Y_target_3 = Y_target.shift(8).rename('Y_lag3')\n",
    "Y_target_4 = Y_target.shift(9).rename('Y_lag4')\n",
    "Y_target_5 = Y_target.shift(10).rename('Y_lag5')\n",
    "Y_target_6 = Y_target.shift(11).rename('Y_lag6')\n",
    "Y_target_7 = Y_target.shift(12).rename('Y_lag7')\n",
    "\n",
    "# ==============================================================================\n",
    "# Define train-test split \n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(stacked_balanced))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training data:\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_1_train = factors_balance_1.iloc[:train_size]\n",
    "factors_balance_2_train = factors_balance_2.iloc[:train_size]\n",
    "factors_balance_3_train = factors_balance_3.iloc[:train_size]\n",
    "\n",
    "Y_target_train = Y_target.iloc[:train_size]\n",
    "Y_train_1 = Y_target_1.iloc[:train_size]\n",
    "Y_train_2 = Y_target_2.iloc[:train_size]\n",
    "Y_train_3 = Y_target_3.iloc[:train_size]\n",
    "Y_train_4 = Y_target_4.iloc[:train_size]\n",
    "Y_train_5 = Y_target_5.iloc[:train_size]\n",
    "Y_train_6 = Y_target_6.iloc[:train_size]\n",
    "Y_train_7 = Y_target_7.iloc[:train_size]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Testing data:\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]\n",
    "factors_balance_1_test = factors_balance_1.iloc[train_size:]\n",
    "factors_balance_2_test = factors_balance_2.iloc[train_size:]\n",
    "factors_balance_3_test = factors_balance_3.iloc[train_size:]\n",
    "\n",
    "Y_target_test = Y_target.iloc[train_size:]\n",
    "Y_test_1 = Y_target_1.iloc[train_size:]\n",
    "Y_test_2 = Y_target_2.iloc[train_size:]\n",
    "Y_test_3 = Y_target_3.iloc[train_size:]\n",
    "Y_test_4 = Y_target_4.iloc[train_size:]\n",
    "Y_test_5 = Y_target_5.iloc[train_size:]\n",
    "Y_test_6 = Y_target_6.iloc[train_size:]\n",
    "Y_test_7 = Y_target_7.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "factors_lags = [factors_balance_1_train, factors_balance_2_train, factors_balance_3_train]\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_Lag_model, DI_AR_Lag_bic, DI_AR_Lag_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('factors_balance_0', 'Y_train_6')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 29 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>3.12e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:44:18</td>     <th>  Log-Likelihood:    </th> <td> -742.04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   610</td>      <th>  AIC:               </th> <td>   1496.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   604</td>      <th>  BIC:               </th> <td>   1523.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>    0.0199</td> <td>    0.034</td> <td>    0.590</td> <td> 0.555</td> <td>   -0.046</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th> <td>   -0.0088</td> <td>    0.006</td> <td>   -1.505</td> <td> 0.133</td> <td>   -0.020</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th> <td>   -0.0130</td> <td>    0.008</td> <td>   -1.725</td> <td> 0.085</td> <td>   -0.028</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th> <td>    0.0363</td> <td>    0.009</td> <td>    4.173</td> <td> 0.000</td> <td>    0.019</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th> <td>   -0.0256</td> <td>    0.011</td> <td>   -2.430</td> <td> 0.015</td> <td>   -0.046</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th> <td>    0.1193</td> <td>    0.040</td> <td>    2.978</td> <td> 0.003</td> <td>    0.041</td> <td>    0.198</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.988</td> <th>  Durbin-Watson:     </th> <td>   2.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  26.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.045</td> <th>  Prob(JB):          </th> <td>1.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.016</td> <th>  Cond. No.          </th> <td>    7.04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.061   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.054   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     7.911   \\\\\n",
       "\\textbf{Date:}             & Sat, 29 Mar 2025 & \\textbf{  Prob (F-statistic):} &  3.12e-07   \\\\\n",
       "\\textbf{Time:}             &     11:44:18     & \\textbf{  Log-Likelihood:    } &   -742.04   \\\\\n",
       "\\textbf{No. Observations:} &         610      & \\textbf{  AIC:               } &     1496.   \\\\\n",
       "\\textbf{Df Residuals:}     &         604      & \\textbf{  BIC:               } &     1523.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &       0.0199  &        0.034     &     0.590  &         0.555        &       -0.046    &        0.086     \\\\\n",
       "\\textbf{0\\_lag1} &      -0.0088  &        0.006     &    -1.505  &         0.133        &       -0.020    &        0.003     \\\\\n",
       "\\textbf{1\\_lag1} &      -0.0130  &        0.008     &    -1.725  &         0.085        &       -0.028    &        0.002     \\\\\n",
       "\\textbf{2\\_lag1} &       0.0363  &        0.009     &     4.173  &         0.000        &        0.019    &        0.053     \\\\\n",
       "\\textbf{3\\_lag1} &      -0.0256  &        0.011     &    -2.430  &         0.015        &       -0.046    &       -0.005     \\\\\n",
       "\\textbf{Y\\_lag6} &       0.1193  &        0.040     &     2.978  &         0.003        &        0.041    &        0.198     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13.988 & \\textbf{  Durbin-Watson:     } &    2.425  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   26.424  \\\\\n",
       "\\textbf{Skew:}          & -0.045 & \\textbf{  Prob(JB):          } & 1.83e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.016 & \\textbf{  Cond. No.          } &     7.04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.061\n",
       "Model:                            OLS   Adj. R-squared:                  0.054\n",
       "Method:                 Least Squares   F-statistic:                     7.911\n",
       "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           3.12e-07\n",
       "Time:                        11:44:18   Log-Likelihood:                -742.04\n",
       "No. Observations:                 610   AIC:                             1496.\n",
       "Df Residuals:                     604   BIC:                             1523.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0199      0.034      0.590      0.555      -0.046       0.086\n",
       "0_lag1        -0.0088      0.006     -1.505      0.133      -0.020       0.003\n",
       "1_lag1        -0.0130      0.008     -1.725      0.085      -0.028       0.002\n",
       "2_lag1         0.0363      0.009      4.173      0.000       0.019       0.053\n",
       "3_lag1        -0.0256      0.011     -2.430      0.015      -0.046      -0.005\n",
       "Y_lag6         0.1193      0.040      2.978      0.003       0.041       0.198\n",
       "==============================================================================\n",
       "Omnibus:                       13.988   Durbin-Watson:                   2.425\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               26.424\n",
       "Skew:                          -0.045   Prob(JB):                     1.83e-06\n",
       "Kurtosis:                       4.016   Cond. No.                         7.04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call:\n",
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_6]\n",
    "\n",
    "DI_AR_Lag_forecast, DI_AR_Lag_MSE, DI_AR_Lag_se = direct_forecast(DI_AR_Lag_model, factors_lags_test, Y_lags_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.142\n",
      "0.16\n"
     ]
    }
   ],
   "source": [
    "print(f\"{DI_AR_Lag_MSE:.3f}\")\n",
    "print(f\"{DI_AR_Lag_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI-AR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "number_factor, estimated_data, factors_stack_balance, pcs_variance = baing(stacked_balanced, 12, 2)\n",
    "factors_stack_balance = pd.DataFrame(factors_stack_balance, index = stacked_balanced.index)\n",
    "print(number_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_balance_0 = factors_stack_balance.shift(6).add_suffix('_lag1')\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_model, DI_AR_bic, DI_AR_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 29 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>1.94e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:44:40</td>     <th>  Log-Likelihood:    </th> <td> -734.51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   610</td>      <th>  AIC:               </th> <td>   1499.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   595</td>      <th>  BIC:               </th> <td>   1565.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    0.0189</td> <td>    0.034</td> <td>    0.556</td> <td> 0.579</td> <td>   -0.048</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.0076</td> <td>    0.006</td> <td>   -1.298</td> <td> 0.195</td> <td>   -0.019</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>   -0.0235</td> <td>    0.009</td> <td>   -2.737</td> <td> 0.006</td> <td>   -0.040</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>    0.0364</td> <td>    0.009</td> <td>    4.184</td> <td> 0.000</td> <td>    0.019</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.0328</td> <td>    0.011</td> <td>   -3.018</td> <td> 0.003</td> <td>   -0.054</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>    0.0136</td> <td>    0.011</td> <td>    1.252</td> <td> 0.211</td> <td>   -0.008</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.0053</td> <td>    0.014</td> <td>   -0.388</td> <td> 0.698</td> <td>   -0.032</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>   -0.0153</td> <td>    0.016</td> <td>   -0.983</td> <td> 0.326</td> <td>   -0.046</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>   -0.0113</td> <td>    0.018</td> <td>   -0.642</td> <td> 0.521</td> <td>   -0.046</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>   -0.0042</td> <td>    0.018</td> <td>   -0.233</td> <td> 0.816</td> <td>   -0.040</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>    0.0200</td> <td>    0.020</td> <td>    0.981</td> <td> 0.327</td> <td>   -0.020</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>    0.0419</td> <td>    0.020</td> <td>    2.139</td> <td> 0.033</td> <td>    0.003</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>    0.0336</td> <td>    0.022</td> <td>    1.555</td> <td> 0.121</td> <td>   -0.009</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag2</th>  <td>   -0.1555</td> <td>    0.057</td> <td>   -2.721</td> <td> 0.007</td> <td>   -0.268</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th>  <td>    0.1059</td> <td>    0.042</td> <td>    2.546</td> <td> 0.011</td> <td>    0.024</td> <td>    0.188</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.914</td> <th>  Durbin-Watson:     </th> <td>   2.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  29.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.039</td> <th>  Prob(JB):          </th> <td>4.55e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.069</td> <th>  Cond. No.          </th> <td>    10.7</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.084   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.063   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3.915   \\\\\n",
       "\\textbf{Date:}             & Sat, 29 Mar 2025 & \\textbf{  Prob (F-statistic):} &  1.94e-06   \\\\\n",
       "\\textbf{Time:}             &     11:44:40     & \\textbf{  Log-Likelihood:    } &   -734.51   \\\\\n",
       "\\textbf{No. Observations:} &         610      & \\textbf{  AIC:               } &     1499.   \\\\\n",
       "\\textbf{Df Residuals:}     &         595      & \\textbf{  BIC:               } &     1565.   \\\\\n",
       "\\textbf{Df Model:}         &          14      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &       0.0189  &        0.034     &     0.556  &         0.579        &       -0.048    &        0.086     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.0076  &        0.006     &    -1.298  &         0.195        &       -0.019    &        0.004     \\\\\n",
       "\\textbf{1\\_lag1}  &      -0.0235  &        0.009     &    -2.737  &         0.006        &       -0.040    &       -0.007     \\\\\n",
       "\\textbf{2\\_lag1}  &       0.0364  &        0.009     &     4.184  &         0.000        &        0.019    &        0.054     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.0328  &        0.011     &    -3.018  &         0.003        &       -0.054    &       -0.011     \\\\\n",
       "\\textbf{4\\_lag1}  &       0.0136  &        0.011     &     1.252  &         0.211        &       -0.008    &        0.035     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.0053  &        0.014     &    -0.388  &         0.698        &       -0.032    &        0.022     \\\\\n",
       "\\textbf{6\\_lag1}  &      -0.0153  &        0.016     &    -0.983  &         0.326        &       -0.046    &        0.015     \\\\\n",
       "\\textbf{7\\_lag1}  &      -0.0113  &        0.018     &    -0.642  &         0.521        &       -0.046    &        0.023     \\\\\n",
       "\\textbf{8\\_lag1}  &      -0.0042  &        0.018     &    -0.233  &         0.816        &       -0.040    &        0.031     \\\\\n",
       "\\textbf{9\\_lag1}  &       0.0200  &        0.020     &     0.981  &         0.327        &       -0.020    &        0.060     \\\\\n",
       "\\textbf{10\\_lag1} &       0.0419  &        0.020     &     2.139  &         0.033        &        0.003    &        0.080     \\\\\n",
       "\\textbf{11\\_lag1} &       0.0336  &        0.022     &     1.555  &         0.121        &       -0.009    &        0.076     \\\\\n",
       "\\textbf{Y\\_lag2}  &      -0.1555  &        0.057     &    -2.721  &         0.007        &       -0.268    &       -0.043     \\\\\n",
       "\\textbf{Y\\_lag6}  &       0.1059  &        0.042     &     2.546  &         0.011        &        0.024    &        0.188     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 14.914 & \\textbf{  Durbin-Watson:     } &    2.451  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   29.207  \\\\\n",
       "\\textbf{Skew:}          & -0.039 & \\textbf{  Prob(JB):          } & 4.55e-07  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.069 & \\textbf{  Cond. No.          } &     10.7  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.084\n",
       "Model:                            OLS   Adj. R-squared:                  0.063\n",
       "Method:                 Least Squares   F-statistic:                     3.915\n",
       "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           1.94e-06\n",
       "Time:                        11:44:40   Log-Likelihood:                -734.51\n",
       "No. Observations:                 610   AIC:                             1499.\n",
       "Df Residuals:                     595   BIC:                             1565.\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0189      0.034      0.556      0.579      -0.048       0.086\n",
       "0_lag1        -0.0076      0.006     -1.298      0.195      -0.019       0.004\n",
       "1_lag1        -0.0235      0.009     -2.737      0.006      -0.040      -0.007\n",
       "2_lag1         0.0364      0.009      4.184      0.000       0.019       0.054\n",
       "3_lag1        -0.0328      0.011     -3.018      0.003      -0.054      -0.011\n",
       "4_lag1         0.0136      0.011      1.252      0.211      -0.008       0.035\n",
       "5_lag1        -0.0053      0.014     -0.388      0.698      -0.032       0.022\n",
       "6_lag1        -0.0153      0.016     -0.983      0.326      -0.046       0.015\n",
       "7_lag1        -0.0113      0.018     -0.642      0.521      -0.046       0.023\n",
       "8_lag1        -0.0042      0.018     -0.233      0.816      -0.040       0.031\n",
       "9_lag1         0.0200      0.020      0.981      0.327      -0.020       0.060\n",
       "10_lag1        0.0419      0.020      2.139      0.033       0.003       0.080\n",
       "11_lag1        0.0336      0.022      1.555      0.121      -0.009       0.076\n",
       "Y_lag2        -0.1555      0.057     -2.721      0.007      -0.268      -0.043\n",
       "Y_lag6         0.1059      0.042      2.546      0.011       0.024       0.188\n",
       "==============================================================================\n",
       "Omnibus:                       14.914   Durbin-Watson:                   2.451\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               29.207\n",
       "Skew:                          -0.039   Prob(JB):                     4.55e-07\n",
       "Kurtosis:                       4.069   Cond. No.                         10.7\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.139\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_2, Y_test_6]\n",
    "DI_AR_forecast, DI_AR_MSE, DI_AR_se = direct_forecast(DI_AR_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_AR_MSE:.3f}\")\n",
    "print(f\"{DI_AR_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization in original scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Undo Standardization ----\n",
    "Z_forecast = DI_AR_forecast * std_diff + mean_diff\n",
    "Z_observed = Y_target_test * std_diff + mean_diff  # Undo standardization for Y_target_test\n",
    "\n",
    "# ---- Undo Differencing ----\n",
    "# Get the last observed log-transformed value from training data\n",
    "Y_last = np.log(memo.loc['2014-04-01'])  # The last value before forecasting\n",
    "\n",
    "# Reverse differencing for forecast\n",
    "Y_forecast = np.cumsum(np.insert(Z_forecast, 0, Y_last))  # Insert last known value\n",
    "Y_forecast = Y_forecast[1:]  # Remove the first value (it was just for initialization)\n",
    "\n",
    "# Reverse differencing for observed data\n",
    "Y_observed = np.cumsum(np.insert(Z_observed, 0, Y_last))  # Using same last value\n",
    "Y_observed = Y_observed[1:]\n",
    "\n",
    "# ---- Undo Log Transformation ----\n",
    "X_forecast = np.exp(Y_forecast)  # Convert back to original scale\n",
    "X_observed = np.exp(Y_observed)  # Convert observed back to original scale\n",
    "\n",
    "# ---- Compare Forecasted vs Observed ----\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Observed': X_observed,\n",
    "    'Forecasted': X_forecast\n",
    "}, index=Y_target_test.index)\n",
    "\n",
    "# ---- Plot the results ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(comparison_df.index, comparison_df['Observed'], label='Observed', marker='o')\n",
    "plt.plot(comparison_df.index, comparison_df['Forecasted'], label='Forecasted', linestyle='dashed', marker='x')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Original Scale Value')\n",
    "plt.title('6 months ahead, Dependent variable: Real Manu. and Trade Industries Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = []\n",
    "\n",
    "DI_model, DI_bic, DI_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>0.000466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:37:08</td>     <th>  Log-Likelihood:    </th> <td> -751.29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   615</td>      <th>  AIC:               </th> <td>   1529.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   602</td>      <th>  BIC:               </th> <td>   1586.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>    0.0191</td> <td>    0.034</td> <td>    0.555</td> <td> 0.579</td> <td>   -0.049</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.0078</td> <td>    0.006</td> <td>   -1.309</td> <td> 0.191</td> <td>   -0.019</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>   -0.0182</td> <td>    0.007</td> <td>   -2.461</td> <td> 0.014</td> <td>   -0.033</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>    0.0335</td> <td>    0.009</td> <td>    3.889</td> <td> 0.000</td> <td>    0.017</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.0250</td> <td>    0.011</td> <td>   -2.332</td> <td> 0.020</td> <td>   -0.046</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>    0.0154</td> <td>    0.011</td> <td>    1.429</td> <td> 0.154</td> <td>   -0.006</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.0046</td> <td>    0.014</td> <td>   -0.333</td> <td> 0.740</td> <td>   -0.032</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>   -0.0055</td> <td>    0.015</td> <td>   -0.374</td> <td> 0.709</td> <td>   -0.035</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>    0.0103</td> <td>    0.017</td> <td>    0.610</td> <td> 0.542</td> <td>   -0.023</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>   -0.0150</td> <td>    0.018</td> <td>   -0.840</td> <td> 0.401</td> <td>   -0.050</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>   -0.0012</td> <td>    0.018</td> <td>   -0.067</td> <td> 0.947</td> <td>   -0.037</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>    0.0282</td> <td>    0.020</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.010</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>    0.0060</td> <td>    0.020</td> <td>    0.295</td> <td> 0.768</td> <td>   -0.034</td> <td>    0.046</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.825</td> <th>  Durbin-Watson:     </th> <td>   2.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  26.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.003</td> <th>  Prob(JB):          </th> <td>1.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.013</td> <th>  Cond. No.          </th> <td>    6.04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.056   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.037   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     2.979   \\\\\n",
       "\\textbf{Date:}             & Fri, 28 Mar 2025 & \\textbf{  Prob (F-statistic):} &  0.000466   \\\\\n",
       "\\textbf{Time:}             &     18:37:08     & \\textbf{  Log-Likelihood:    } &   -751.29   \\\\\n",
       "\\textbf{No. Observations:} &         615      & \\textbf{  AIC:               } &     1529.   \\\\\n",
       "\\textbf{Df Residuals:}     &         602      & \\textbf{  BIC:               } &     1586.   \\\\\n",
       "\\textbf{Df Model:}         &          12      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &       0.0191  &        0.034     &     0.555  &         0.579        &       -0.049    &        0.087     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.0078  &        0.006     &    -1.309  &         0.191        &       -0.019    &        0.004     \\\\\n",
       "\\textbf{1\\_lag1}  &      -0.0182  &        0.007     &    -2.461  &         0.014        &       -0.033    &       -0.004     \\\\\n",
       "\\textbf{2\\_lag1}  &       0.0335  &        0.009     &     3.889  &         0.000        &        0.017    &        0.050     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.0250  &        0.011     &    -2.332  &         0.020        &       -0.046    &       -0.004     \\\\\n",
       "\\textbf{4\\_lag1}  &       0.0154  &        0.011     &     1.429  &         0.154        &       -0.006    &        0.037     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.0046  &        0.014     &    -0.333  &         0.740        &       -0.032    &        0.023     \\\\\n",
       "\\textbf{6\\_lag1}  &      -0.0055  &        0.015     &    -0.374  &         0.709        &       -0.035    &        0.024     \\\\\n",
       "\\textbf{7\\_lag1}  &       0.0103  &        0.017     &     0.610  &         0.542        &       -0.023    &        0.043     \\\\\n",
       "\\textbf{8\\_lag1}  &      -0.0150  &        0.018     &    -0.840  &         0.401        &       -0.050    &        0.020     \\\\\n",
       "\\textbf{9\\_lag1}  &      -0.0012  &        0.018     &    -0.067  &         0.947        &       -0.037    &        0.035     \\\\\n",
       "\\textbf{10\\_lag1} &       0.0282  &        0.020     &     1.443  &         0.149        &       -0.010    &        0.067     \\\\\n",
       "\\textbf{11\\_lag1} &       0.0060  &        0.020     &     0.295  &         0.768        &       -0.034    &        0.046     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 13.825 & \\textbf{  Durbin-Watson:     } &    2.474  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   26.291  \\\\\n",
       "\\textbf{Skew:}          &  0.003 & \\textbf{  Prob(JB):          } & 1.95e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.013 & \\textbf{  Cond. No.          } &     6.04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.056\n",
       "Model:                            OLS   Adj. R-squared:                  0.037\n",
       "Method:                 Least Squares   F-statistic:                     2.979\n",
       "Date:                Fri, 28 Mar 2025   Prob (F-statistic):           0.000466\n",
       "Time:                        18:37:08   Log-Likelihood:                -751.29\n",
       "No. Observations:                 615   AIC:                             1529.\n",
       "Df Residuals:                     602   BIC:                             1586.\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0191      0.034      0.555      0.579      -0.049       0.087\n",
       "0_lag1        -0.0078      0.006     -1.309      0.191      -0.019       0.004\n",
       "1_lag1        -0.0182      0.007     -2.461      0.014      -0.033      -0.004\n",
       "2_lag1         0.0335      0.009      3.889      0.000       0.017       0.050\n",
       "3_lag1        -0.0250      0.011     -2.332      0.020      -0.046      -0.004\n",
       "4_lag1         0.0154      0.011      1.429      0.154      -0.006       0.037\n",
       "5_lag1        -0.0046      0.014     -0.333      0.740      -0.032       0.023\n",
       "6_lag1        -0.0055      0.015     -0.374      0.709      -0.035       0.024\n",
       "7_lag1         0.0103      0.017      0.610      0.542      -0.023       0.043\n",
       "8_lag1        -0.0150      0.018     -0.840      0.401      -0.050       0.020\n",
       "9_lag1        -0.0012      0.018     -0.067      0.947      -0.037       0.035\n",
       "10_lag1        0.0282      0.020      1.443      0.149      -0.010       0.067\n",
       "11_lag1        0.0060      0.020      0.295      0.768      -0.034       0.046\n",
       "==============================================================================\n",
       "Omnibus:                       13.825   Durbin-Watson:                   2.474\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               26.291\n",
       "Skew:                           0.003   Prob(JB):                     1.95e-06\n",
       "Kurtosis:                       4.013   Cond. No.                         6.04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.141\n",
      "0.16\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = []\n",
    "DI_forecast, DI_MSE, DI_se = direct_forecast(DI_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_MSE:.3f}\")\n",
    "print(f\"{DI_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_lagged = stacked_balanced.shift(6)\n",
    "balanced_lagged_train = balanced_lagged.iloc[:train_size].dropna()\n",
    "balanced_lagged_test = balanced_lagged.iloc[train_size:]\n",
    "Y_target_train_adjusted = Y_target_train.loc[balanced_lagged_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LASSO Regression...\n",
      "Best model parameters: {'alpha': np.float64(0.04714866363457394), 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=1, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.159\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_lasso, mse_lasso = direct_forecast_2(model_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ElasticNet Regression...\n",
      "Best Elastic Net Alpha: 0.49417133613238384\n",
      "Best Elastic Net l1_ratio: 0.1\n",
      "Best model parameters: {'alpha': np.float64(0.49417133613238384), 'copy_X': True, 'fit_intercept': True, 'l1_ratio': np.float64(0.1), 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_elastic_net = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=2, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.160\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_elastic_net, mse_elastic_net = direct_forecast_2(model_elastic_net, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_groups_stacked(balanced_lagged_train):\n",
    "    \"\"\"\n",
    "    Construct the 'groups' array for Group LASSO, ensuring that lagged variables\n",
    "    are included in the same groups as their original counterparts.\n",
    "\n",
    "    Parameters:\n",
    "    - balanced_lagged_train (pd.DataFrame): DataFrame containing the regressors.\n",
    "\n",
    "    Returns:\n",
    "    - groups (np.array): An array of group assignments corresponding to the columns of balanced_lagged_train.\n",
    "    \"\"\"\n",
    "    # Define groups with original variable names\n",
    "    group_dict = {\n",
    "        1: [\"RPI\", \"W875RX1\", \"INDPRO\", \"IPFPNSS\", \"IPFINAL\", \"IPCONGD\", \"IPDCONGD\", \"IPNCONGD\", \"IPBUSEQ\",\n",
    "            \"IPMAT\", \"IPDMAT\", \"IPNMAT\", \"IPMANSICS\", \"IPB51222s\", \"IPFUELS\", \"CUMFNS\"],\n",
    "        2: [\"HWI\", \"HWIURATIO\", \"CLF16OV\", \"CE16OV\", \"UNRATE\", \"UEMPMEAN\", \"UEMPLT5\", \"UEMP5TO14\", \"UEMP15OV\",\n",
    "            \"UEMP15T26\", \"UEMP27OV\", \"CLAIMSx\", \"PAYEMS\", \"USGOOD\", \"CES1021000001\", \"USCONS\", \"MANEMP\",\n",
    "            \"DMANEMP\", \"NDMANEMP\", \"SRVPRD\", \"USTPU\", \"USWTRADE\", \"USTRADE\", \"USFIRE\", \"USGOVT\",\n",
    "            \"CES0600000007\", \"AWOTMAN\", \"AWHMAN\", \"CES0600000008\", \"CES2000000008\", \"CES3000000008\"],\n",
    "        3: [\"HOUST\", \"HOUSTNE\", \"HOUSTMW\", \"HOUSTS\", \"HOUSTW\", \"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\", \"PERMITW\"],\n",
    "        4: [\"DPCERA3M086SBEA\", \"CMRMTSPLx\", \"RETAILx\", \"ACOGNO\", \"AMDMNOx\", \"ANDENOx\", \"AMDMUOx\", \"BUSINVx\",\n",
    "            \"ISRATIOx\", \"UMCSENTx\"],\n",
    "        5: [\"M1SL\", \"M2SL\", \"M2REAL\", \"BOGMBASE\", \"TOTRESNS\", \"NONBORRES\", \"BUSLOANS\", \"REALLN\", \"NONREVSL\",\n",
    "            \"CONSPI\", \"DTCOLNVHFNM\", \"DTCTHFNM\", \"INVEST\"],\n",
    "        6: [\"FEDFUNDS\", \"CP3Mx\", \"TB3MS\", \"TB6MS\", \"GS1\", \"GS5\", \"GS10\", \"AAA\", \"BAA\", \"COMPAPFFx\",\n",
    "            \"TB3SMFFM\", \"TB6SMFFM\", \"T1YFFM\", \"T5YFFM\", \"T10YFFM\", \"AAAFFM\", \"BAAFFM\", \"TWEXAFEGSMTHx\",\n",
    "            \"EXSZUSx\", \"EXJPUSx\", \"EXUSUKx\", \"EXCAUSx\"],\n",
    "        7: [\"WPSFD49207\", \"WPSFD49502\", \"WPSID61\", \"WPSID62\", \"OILPRICEx\", \"PPICMM\", \"CPIAUCSL\", \"CPIAPPSL\",\n",
    "            \"CPITRNSL\", \"CPIMEDSL\", \"CUSR0000SAC\", \"CUSR0000SAD\", \"CUSR0000SAS\", \"CPIULFSL\", \"CUSR0000SA0L2\",\n",
    "            \"CUSR0000SA0L5\", \"PCEPI\", \"DDURRG3M086SBEA\", \"DNDGRG3M086SBEA\", \"DSERRG3M086SBEA\"],\n",
    "        8: [\"S&P 500\", \"S&P div yield\", \"S&P PE ratio\", \"VIXCLSx\"]\n",
    "    }\n",
    "\n",
    "    # Extract dataset columns\n",
    "    dataset_columns = balanced_lagged_train.columns.tolist()\n",
    "\n",
    "    # Initialize an empty array for group assignments\n",
    "    groups = np.zeros(len(dataset_columns), dtype=int)\n",
    "\n",
    "    # Map original variables to groups\n",
    "    variable_to_group = {}\n",
    "    for group_num, variables in group_dict.items():\n",
    "        for var in variables:\n",
    "            variable_to_group[var] = group_num\n",
    "\n",
    "    # Assign each variable and its lag to the correct group\n",
    "    for i, var in enumerate(dataset_columns):\n",
    "        base_var = var.replace(\"_lag\", \"\")  # Remove \"_lag\" to get the base variable name\n",
    "        if base_var in variable_to_group:\n",
    "            groups[i] = variable_to_group[base_var]\n",
    "\n",
    "    # Ensure all variables are assigned a group\n",
    "    if np.any(groups == 0):\n",
    "        print(\"Warning: Some variables in the dataset were not assigned to any group.\")\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some variables in the dataset were not assigned to any group.\n"
     ]
    }
   ],
   "source": [
    "# Function call\n",
    "groups = construct_groups_stacked(balanced_lagged_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Group  Count  Proportion\n",
      "0      0      2    0.009434\n",
      "1      1     28    0.132075\n",
      "2      2     60    0.283019\n",
      "3      3     20    0.094340\n",
      "4      4     12    0.056604\n",
      "5      5     14    0.066038\n",
      "6      6     34    0.160377\n",
      "7      7     34    0.160377\n",
      "8      8      8    0.037736\n"
     ]
    }
   ],
   "source": [
    "group_proportions = compute_group_proportions(groups)\n",
    "print(group_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Group LASSO Regression...\n",
      "Best model parameters: {'fit_intercept': False, 'frobenius_lipschitz': False, 'group_reg': 0.05, 'groups': array([1, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 8, 8,\n",
      "       8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 5, 8, 1, 4, 4, 4,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 8, 8, 8, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 5, 8]), 'l1_reg': np.float64(1.5998587196060572e-05), 'n_iter': 100, 'old_regularisation': False, 'random_state': None, 'scale_reg': 'group_size', 'subsampling_scheme': None, 'supress_warning': True, 'tol': 1e-05, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_group_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=3, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.152\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_group_lasso, mse_group_lasso = direct_forecast_2(model_group_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecast combining regression** with Heteroskedasticity and Autocorrelation Robust (HAC) standard errors using the Newey-West estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.75\n",
      "P-value: 0.23\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_AR_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 1.46\n",
      "P-value: 0.05\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.94\n",
      "P-value: 0.13\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 1.17\n",
      "P-value: 0.25\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_elastic_net, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 201.70\n",
      "P-value: 0.23\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_group_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X2,DEMEAN):\n",
    "    # take in pandas <-> return numpy\n",
    "    '''\n",
    "     =========================================================================\n",
    "     DESCRIPTION\n",
    "     This function transforms a given set of series based upon the input\n",
    "     variable DEMEAN. The following transformations are possible:\n",
    "\n",
    "       1) No transformation.\n",
    "\n",
    "       2) Each series is demeaned only (i.e. each series is rescaled to have a\n",
    "       mean of 0).\n",
    "\n",
    "       3) Each series is demeaned and standardized (i.e. each series is\n",
    "       rescaled to have a mean of 0 and a standard deviation of 1).\n",
    "\n",
    "       4) Each series is recursively demeaned and then standardized. For a\n",
    "       given series x(t), where t=1,...,T, the recursively demeaned series\n",
    "       x'(t) is calculated as x'(t) = x(t) - mean(x(1:t)). After the\n",
    "       recursively demeaned series x'(t) is calculated, it is standardized by\n",
    "       dividing x'(t) by the standard deviation of the original series x. Note\n",
    "       that this transformation does not rescale the original series to have a\n",
    "       specified mean or standard deviation.\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     INPUTS\n",
    "               X2      = set of series to be transformed (one series per\n",
    "                         column); no missing values;\n",
    "               DEMEAN  = an integer indicating the type of transformation\n",
    "                         performed on each series in x2; it can take on the\n",
    "                         following values:\n",
    "                               0 (no transformation)\n",
    "                               1 (demean only)\n",
    "                               2 (demean and standardize)\n",
    "                               3 (recursively demean and then standardize)\n",
    "\n",
    "     OUTPUTS\n",
    "               X22     = transformed dataset\n",
    "               mut     = matrix containing the values subtracted from X2\n",
    "                         during the transformation\n",
    "               sdt     = matrix containing the values that X2 was divided by\n",
    "                         during the transformation\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "    '''\n",
    "    assert DEMEAN in [0, 1, 2, 3], 'DEMEAN value incorrectly set, must be in [0, 1, 2, 3]'\n",
    "    mut = X2 * 0        # initialize values at no tranformation, i.e. DEMEAN = 0\n",
    "    std = X2 * 0 + 1\n",
    "\n",
    "    if DEMEAN == 1:   # Each series is demeaned only\n",
    "        mut = X2*0 + X2.mean()\n",
    "\n",
    "    elif DEMEAN == 2:   # Each series is demeaned and standardized\n",
    "        mut = X2 * 0 + X2.mean()\n",
    "        std = X2 * 0 + X2.std()\n",
    "\n",
    "    elif DEMEAN == 3:   # Each series is recursively demeaned and then standardized\n",
    "        for t in range(0, len(X2)):\n",
    "            mut.loc[X2.index[t], X2.columns] = X2.iloc[:t+1, :].mean()\n",
    "        std = X2 * 0 + X2.std()\n",
    "\n",
    "    X22 = (X2 - mut) / std\n",
    "\n",
    "    return X22.values, mut.values, std.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minindc(X):\n",
    "    ''' =========================================================================\n",
    "     takes np <-> returns np\n",
    "     DESCRIPTION\n",
    "     This function finds the index of the minimum value for each column of a\n",
    "     given matrix. The function assumes that the minimum value of each column\n",
    "     occurs only once within that column. The function returns an error if\n",
    "     this is not the case.\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     INPUT\n",
    "               x   = matrix\n",
    "\n",
    "     OUTPUT\n",
    "               pos = column vector with pos(i) containing the row number\n",
    "                     corresponding to the minimum value of x(:,i)\n",
    "\n",
    "     ========================================================================= '''\n",
    "\n",
    "    mins = X.argmin(axis=0) # returns the indices of the minimum values along each column, axis=0 specifies that the operation is done column-wise (for each column).\n",
    "    assert sum(X == X[mins]) == 1, 'Minimum value occurs more than once.'\n",
    "    return mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function pc2(X, nfac) performs PCA on a dataset X, where each column represents a variable or time series. The goal is to find the **principal components (factors)** and their **loadings (eigenvectors)** for the dataset. The function also calculates eigenvalues associated with the factors.\n",
    "\n",
    "This code has been designed to perform the EM algorithm, so the loadings are scaled by sqrt(N). This is not the usual PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc2(X,nfac):\n",
    "    '''' =========================================================================\n",
    "     DESCRIPTION\n",
    "     This function runs principal component analysis.\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     INPUTS\n",
    "               X      = dataset (one series per column)\n",
    "               nfac   = number of factors to be selected\n",
    "\n",
    "     OUTPUTS\n",
    "               chat  f = values of X predicted by the factors\n",
    "               fhat   = factors scaled by (1/sqrt(N)) where N is the number of\n",
    "                        series\n",
    "               lambda = factor loadings scaled by number of series\n",
    "               ss     = eigenvalues of X'*X\n",
    "\n",
    "     ========================================================================= '''\n",
    "\n",
    "    N = X.shape[1]  # Number of series in X (i.e. number of columns)\n",
    "    # The rows of vh are the eigenvectors of A'A and the columns of u are the eigenvectors of AA'.\n",
    "    # In both cases the corresponding (possibly non-zero) eigenvalues are given by s**2.\n",
    "    U, S, Vh = np.linalg.svd(X.T@X) # Singular value decomposition: X'*X = U*S*V where V=U'\n",
    "\n",
    "    lambda_ = U[:, :nfac]*np.sqrt(N)   # Factor loadings (eigenvectors) scaled by sqrt(N)\n",
    "    fhat = np.dot(X, lambda_)*(1/N)  # fhat represents the factors ie. the principal components (principal component scores). It is computed by multiplying the dataset X by the factor loadings (eigenvectors) lambda_.  Factors scaled by 1/sqrt(N) to preserve the correct scaling of the factors (since lambda is scaled by sqrt(N)).\n",
    "    # These factors are the projections of the data onto the directions defined by the eigenvectors (principal components).\n",
    "\n",
    "    chat = np.dot(fhat, lambda_.T) # Estimate initial dataset X using the factors (note that U'=inv(U))\n",
    "    # This is the reconstructed data X based on the selected principal components (factors). It is computed by multiplying fhat (the factors) by the transpose of lambda_ (the factor loadings).\n",
    "    # chat is an approximation of the original dataset X, using the first nfac principal components. This allows for dimensionality reduction if nfac is less than the total number of variables in X.\n",
    "\n",
    "    ss = S                          #  a vector of singular values of X'*X\n",
    "    # They are the square roots of the eigenvalues.\n",
    "    # These singular values (S) give the amount of variance explained by each principal component.\n",
    "\n",
    "    return chat, fhat, lambda_, ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal number of factors using PCp (the best is PCp2, so set jj =2) criteria from Bai and Ng (2002):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baing(X,kmax,jj):\n",
    "    #take in and return numpy arrays\n",
    "    ''' =========================================================================\n",
    "    DESCRIPTION\n",
    "    This function determines the number of factors to be selected for a given\n",
    "    dataset using one of three information criteria specified by the user.\n",
    "    The user also specifies the maximum number of factors to be selected.\n",
    "\n",
    "    -------------------------------------------------------------------------\n",
    "    INPUTS\n",
    "               X       = dataset (one series per column)\n",
    "               kmax    = an integer indicating the maximum number of factors\n",
    "                         to be estimated\n",
    "               jj      = an integer indicating the information criterion used\n",
    "                         for selecting the number of factors; it can take on\n",
    "                         the following values:\n",
    "                               1 (information criterion PC_p1)\n",
    "                               2 (information criterion PC_p2)\n",
    "                               3 (information criterion PC_p3)\n",
    "\n",
    "     OUTPUTS\n",
    "               ic1     = number of factors selected\n",
    "               chat    = values of X predicted by the factors\n",
    "               Fhat    = factors\n",
    "               eigval  = eivenvalues of X'*X (or X*X' if N>T)\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     SUBFUNCTIONS USED\n",
    "\n",
    "     minindc() - finds the index of the minimum value for each column of a given matrix\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     BREAKDOWN OF THE FUNCTION\n",
    "\n",
    "     Part 1: Setup.\n",
    "\n",
    "     Part 2: Calculate the overfitting penalty for each possible number of\n",
    "             factors to be selected (from 1 to kmax).\n",
    "\n",
    "     Part 3: Select the number of factors that minimizes the specified\n",
    "             information criterion by utilizing the overfitting penalties calculated in Part 2.\n",
    "\n",
    "     Part 4: Save other output variables to be returned by the function (chat,\n",
    "             Fhat, and eigval).\n",
    "\n",
    "    ========================================================================= '''\n",
    "    assert kmax <= X.shape[1] and  kmax >= 1 and np.floor(kmax) == kmax or kmax == 99, 'kmax is specified incorrectly'\n",
    "    assert jj in [1, 2, 3], 'jj is specified incorrectly'\n",
    "\n",
    "\n",
    "    #  PART 1: SETUP\n",
    "\n",
    "    T = X.shape[0]  # Number of observations per series (i.e. number of rows)\n",
    "    N = X.shape[1]  # Number of series (i.e. number of columns)\n",
    "    NT = N * T      # Total number of observations\n",
    "    NT1 = N + T     # Number of rows + columns\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 2: OVERFITTING PENALTY\n",
    "    #  Determine penalty for overfitting based on the selected information\n",
    "    #  criterion.\n",
    "\n",
    "    CT = np.zeros(kmax) # overfitting penalty\n",
    "    ii = np.arange(1, kmax + 1)     # Array containing possible number of factors that can be selected (1 to kmax)\n",
    "    GCT = min(N,T)                  # The smaller of N and T\n",
    "\n",
    "    # Calculate penalty based on criterion determined by jj.\n",
    "    if jj == 1:             # Criterion PC_p1\n",
    "        CT[:] = np.log(NT / NT1) * ii * (NT1 / NT)\n",
    "\n",
    "    elif jj == 2:             # Criterion PC_p2\n",
    "        CT[:] = np.log(min(N, T)) * ii * (NT1 / NT)\n",
    "\n",
    "    elif jj == 3:             # Criterion PC_p3\n",
    "        CT[:] = np.log(GCT) / GCT * ii\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 3: SELECT NUMBER OF FACTORS\n",
    "    #  Perform principal component analysis on the dataset and select the number\n",
    "    #  of factors that minimizes the specified information criterion.\n",
    "    #\n",
    "    #  -------------------------------------------------------------------------\n",
    "    #  RUN PRINCIPAL COMPONENT ANALYSIS\n",
    "    #  Get components, loadings, and eigenvalues\n",
    "    # There are two different cases depending on whether T<N or T≥N. This distinction comes from computational efficiency and numerical stability when \n",
    "    # performing Principal Component Analysis (PCA) using Singular Value Decomposition (SVD).\n",
    "    # PCA is typically performed on the covariance matrix X′X or XX′. The choice of which matrix to use depends on the dimensions of X, specifically whether \n",
    "    # T (number of observations) is smaller or larger than N (number of variables). And therefore the order of the matrix in SVD is changed.\n",
    "\n",
    "    if T < N:\n",
    "        ev, eigval, V = np.linalg.svd(np.dot(X, X.T))       #  Singular value decomposition\n",
    "        Fhat0 = ev*np.sqrt(T)                               #  Components\n",
    "        Lambda0 = np.dot(X.T, Fhat0) / T                    #  Loadings\n",
    "    else:\n",
    "        ev, eigval, V = np.linalg.svd(np.dot(X.T, X))       #  Singular value decomposition\n",
    "        Lambda0 = ev*np.sqrt(N)                             #  Loadings\n",
    "        Fhat0 = np.dot(X, Lambda0) / N                      #  Components\n",
    "    #  -------------------------------------------------------------------------\n",
    "\n",
    "    # SELECT NUMBER OF FACTORS\n",
    "    # Preallocate memory\n",
    "    Sigma = np.zeros(kmax + 1)          # sum of squared residuals divided by NT, kmax factors + no factor\n",
    "    IC1 = np.zeros(kmax + 1)            # information criterion value, kmax factors + no factor\n",
    "\n",
    "    for i in range(0, kmax) :           # Loop through all possibilites for the number of factors\n",
    "        Fhat = Fhat0[:, :i+1]           # Identify factors as first i components\n",
    "        lambda_ = Lambda0[:, :i+1]       #     % Identify factor loadings as first i loadings\n",
    "\n",
    "        chat = np.dot(Fhat, lambda_.T)      #     % Predict X using i factors\n",
    "        ehat = X - chat                 # Residuals from predicting X using the factors\n",
    "        Sigma[i] = ((ehat*ehat/T).sum(axis = 0)).mean()    # Sum of squared residuals divided by NT\n",
    "\n",
    "        IC1[i] = np.log(Sigma[i]) + CT[i]      #  Value of the information criterion when using i factors\n",
    "\n",
    "\n",
    "    Sigma[kmax] = (X*X/T).sum(axis = 0).mean()  # Sum of squared residuals when using no factors to predict X (i.e. fitted values are set to 0)\n",
    "\n",
    "    IC1[kmax] =  np.log(Sigma[kmax]) # % Value of the information criterion when using no factors\n",
    "\n",
    "    ic1 = minindc(IC1) # % Number of factors that minimizes the information criterion\n",
    "    # Set ic1=0 if ic1>kmax (i.e. no factors are selected if the value of the\n",
    "    # information criterion is minimized when no factors are used)\n",
    "    ic1 = ic1 *(ic1 < kmax) # if = kmax -> 0\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 4: SAVE OTHER OUTPUT\n",
    "    #\n",
    "    #  Factors and loadings when number of factors set to kmax\n",
    "\n",
    "    Fhat = Fhat0[:, :kmax] # factors\n",
    "    Lambda = Lambda0[:, :kmax] #factor loadings\n",
    "\n",
    "    chat = np.dot(Fhat, Lambda.T) #     Predict X using kmax factors\n",
    "\n",
    "    return ic1+1, chat, Fhat, eigval\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factors obtained using the Expectation-Maximization (EM) algorithm (where DEMEAN should be set to 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors_em(X, kmax, jj, DEMEAN):\n",
    "    ''' =========================================================================\n",
    "     DESCRIPTION\n",
    "     This program estimates a set of factors for a given dataset using\n",
    "     principal component analysis. The number of factors estimated is\n",
    "     determined by an information criterion specified by the user. Missing\n",
    "     values in the original dataset are handled using an iterative\n",
    "     expectation-maximization (EM) algorithm.\n",
    "\n",
    "     -------------------------------------------------------------------------\n",
    "     INPUTS\n",
    "               x       = dataset (one series per column)\n",
    "               kmax    = an integer indicating the maximum number of factors\n",
    "                         to be estimated; if set to 99, the number of factors\n",
    "                         selected is forced to equal 8\n",
    "               jj      = an integer indicating the information criterion used \n",
    "                         for selecting the number of factors; it can take on \n",
    "                         the following values:\n",
    "                               1 (information criterion PC_p1)\n",
    "                               2 (information criterion PC_p2)\n",
    "                               3 (information criterion PC_p3)      \n",
    "               DEMEAN  = an integer indicating the type of transformation\n",
    "                         performed on each series in x before the factors are\n",
    "                         estimated; it can take on the following values:\n",
    "                               0 (no transformation)\n",
    "                               1 (demean only)\n",
    "                               2 (demean and standardize)\n",
    "                               3 (recursively demean and then standardize) \n",
    "    \n",
    "     OUTPUTS\n",
    "               ehat    = difference between x and values of x predicted by\n",
    "                         the factors\n",
    "               Fhat    = set of factors\n",
    "               lamhat  = factor loadings\n",
    "               ve2     = eigenvalues of x3'*x3 (where x3 is the dataset x post\n",
    "                         transformation and with missing values filled in)\n",
    "               x2      = x with missing values replaced from the EM algorithm\n",
    "    \n",
    "     -------------------------------------------------------------------------\n",
    "     SUBFUNCTIONS\n",
    "    \n",
    "     baing() - selects number of factors\n",
    "     pc2() - runs principal component analysis\n",
    "     minindc() - finds the index of the minimum value for each column of a\n",
    "           given matrix\n",
    "     transform_data() - performs data transformation\n",
    "    \n",
    "     -------------------------------------------------------------------------'''\n",
    "\n",
    "    # BREAKDOWN OF THE FUNCTION\n",
    "    # Part 1: Check that inputs are specified correctly.\n",
    "    # Part 2: Setup.\n",
    "    # Part 3: Initialize the EM algorithm -- fill in missing values with\n",
    "    #         unconditional mean and estimate factors using the updated\n",
    "    #         dataset.\n",
    "\n",
    "    # Part 4: Perform the EM algorithm -- update missing values using factors,\n",
    "    #         construct a new set of factors from the updated dataset, and\n",
    "    #         repeat until the factor estimates do not change.\n",
    "    #\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Details for the three possible information criteria can be found in the\n",
    "    # paper \"Determining the Number of Factors in Approximate Factor Models\" by\n",
    "    # Bai and Ng (2002).\n",
    "\n",
    "    # The EM algorithm is essentially the one given in the paper \"Macroeconomic\n",
    "    # Forecasting Using Diffusion Indexes\" by Stock and Watson (2002). The\n",
    "    # algorithm is initialized by filling in missing values with the\n",
    "    # unconditional mean of the series, demeaning and standardizing the updated\n",
    "    # dataset, estimating factors from this demeaned and standardized dataset,\n",
    "    # and then using these factors to predict the dataset. The algorithm then\n",
    "    # proceeds as follows: update missing values using values predicted by the\n",
    "    # latest set of factors, demean and standardize the updated dataset,\n",
    "    # estimate a new set of factors using the demeaned and standardized updated\n",
    "    # dataset, and repeat the process until the factor estimates do not change.\n",
    "\n",
    "    # =========================================================================\n",
    "    # PART 1: CHECKS\n",
    "\n",
    "    # Check that x is not missing values for an entire row\n",
    "    assert (X.isna().sum(axis=1) == X.shape[1]).sum() == 0, 'X contains entire rows of missing values'\n",
    "\n",
    "    # Check that x is not missing values for an entire column\n",
    "    assert (X.isna().sum(axis=0) == X.shape[0]).sum() == 0, 'X contains entire columns of missing values'\n",
    "\n",
    "    # Check that kmax is an integer between 1 and the number of columns of x, or 99\n",
    "    assert kmax <= X.shape[1] and  kmax >= 1 and np.floor(kmax) == kmax or kmax == 99, 'kmax is specified incorrectly'\n",
    "\n",
    "    # Check that jj is one of 1, 2, 3\n",
    "    assert jj in [1, 2, 3], 'jj is specified incorrectly'\n",
    "\n",
    "    # Check that DEMEAN is one of 0, 1, 2, 3\n",
    "    assert DEMEAN in [0, 1, 2, 3], 'DEMEAN value incorrectly set, must be in [0, 1, 2, 3]'\n",
    "\n",
    "    # =========================================================================\n",
    "    # PART 2: SETUP\n",
    "\n",
    "\n",
    "    maxit = 50          # Maximum number of iterations for the EM algorithm\n",
    "    T = X.shape[0]      # Number of observations per series in x (i.e. number of rows)\n",
    "    N = X.shape[1]      # Number of series in x (i.e. number of columns)\n",
    "\n",
    "\n",
    "    err = 99999         # Set error to arbitrarily high number\n",
    "    it = 0              # Set iteration counter to 0\n",
    "    X1 = X.isna()       # Locate missing values in x\n",
    "\n",
    "    #  =========================================================================\n",
    "    #  PART 3: INITIALIZE EM ALGORITHM\n",
    "    #  Fill in missing values for each series with the unconditional mean of\n",
    "    #  that series. Demean and standardize the updated dataset. Estimate factors\n",
    "    #  using the demeaned and standardized dataset, and use these factors to\n",
    "    #  predict the original dataset.\n",
    "    #  Get unconditional mean of the non-missing values of each series\n",
    "\n",
    "\n",
    "\n",
    "    mut = (X*0).fillna(0) + X.mean(axis = 0)            # Get unconditional mean of the non-missing values of each series\n",
    "                                                        # mut has no missing values (na)\n",
    "    X2 = X.fillna(mut)                                  # Replace missing values with unconditional mean\n",
    "\n",
    "    #  Demean and standardize data using subfunction transform_data()\n",
    "    #    x3  = transformed dataset\n",
    "    #    mut = matrix containing the values subtracted from x2 during the\n",
    "    #         transformation, TYPE OF DEMEANING USED - CAN BE SIMPLE COLUMN MEAN\n",
    "    #    sdt = matrix containing the values that x2 was divided by during the\n",
    "    #          transformation\n",
    "\n",
    "    X3, mut, std = transform_data(X2, DEMEAN)       # these are numpy arrays, DEMEAN = 2\n",
    "    #  If input 'kmax' is not set to 99, use subfunction baing() to determine\n",
    "    #  the number of factors to estimate. Otherwise, set number of factors equal to 8\n",
    "    if kmax != 99:\n",
    "        icstar, _, _, _  = baing(X3, kmax, jj)\n",
    "    else:\n",
    "        icstar = 8\n",
    "\n",
    "\n",
    "    # Run principal components on updated dataset using subfunction pc2()\n",
    "    #    chat   = values of x3 predicted by the factors\n",
    "    #    Fhat   = factors scaled by (1/sqrt(N)) where N is the number of series\n",
    "    #    lamhat = factor loadings scaled by number of series\n",
    "    #    ve2    = eigenvalues of x3'*x3\n",
    "\n",
    "    chat, Fhat, lamhat, ve2  = pc2(X3, icstar)\n",
    "    chat0 = chat        # Save predicted series values\n",
    "\n",
    "    #  =========================================================================\n",
    "    # PART 4: PERFORM EM ALGORITHM\n",
    "    # Update missing values using values predicted by the latest set of\n",
    "    # factors. Demean and standardize the updated dataset. Estimate a new set\n",
    "    # of factors using the updated dataset. Repeat the process until the factor\n",
    "    # estimates do not change.\n",
    "    #\n",
    "    # Run while error is large and have yet to exceed maximum number of\n",
    "    # iterations\n",
    "\n",
    "    while (err > 0.000001) and (it < maxit):\n",
    "\n",
    "    #    ---------------------------------------------------------------------\n",
    "        it += 1             #     Increase iteration counter by 1\n",
    "        print(f'Iteration {it}: obj {err} IC {icstar} \\n')      #     Display iteration counter, error, and number of factors\n",
    "\n",
    "    #      ---------------------------------------------------------------------\n",
    "    #     UPDATE MISSING VALUES\n",
    "    #     Replace missing observations with latest values predicted by the\n",
    "    #     factors (after undoing any transformation)\n",
    "\n",
    "        temp = X.fillna(0)*0 + chat*std + mut  # temp must not have na's in the df as it will keep them\n",
    "        X2 = X.fillna(temp)\n",
    "\n",
    "    #     ---------------------------------------------------------------------\n",
    "    #     ESTIMATE FACTORS\n",
    "    #     Demean/standardize new dataset and recalculate mut and sdt using\n",
    "    #     subfunction transform_data()\n",
    "    #       x3  = transformed dataset\n",
    "    #       mut = matrix containing the values subtracted from x2 during the\n",
    "    #             transformation\n",
    "    #        sdt = matrix containing the values that x2 was divided by during\n",
    "    #              the transformation\n",
    "\n",
    "        X3, mut, sdt = transform_data(X2, DEMEAN)\n",
    "\n",
    "    #     Determine number of factors to estimate for the new dataset using\n",
    "    #     subfunction baing() (or set to 8 if kmax equals 99)\n",
    "\n",
    "        if kmax != 99:\n",
    "            icstar, _, _, _ = baing(X3, kmax, jj)\n",
    "        else:\n",
    "            icstar = 8\n",
    "\n",
    "    #  Run principal components on the new dataset using subfunction pc2()\n",
    "    #        chat   = values of x3 predicted by the factors\n",
    "    #        Fhat   = factors scaled by (1/sqrt(N)) where N is the number of\n",
    "    #                 series\n",
    "    #        lamhat = factor loadings scaled by number of series\n",
    "    #        ve2    = eigenvalues of x3'*x3\n",
    "\n",
    "        chat, Fhat, lamhat, ve2  = pc2(X3, icstar)\n",
    "    #     ---------------------------------------------------------------------\n",
    "    #     CALCULATE NEW ERROR VALUE\n",
    "    #     Calculate difference between the predicted values of the new dataset\n",
    "    #     and the predicted values of the previous dataset\n",
    "        diff = chat - chat0\n",
    "    #     The error value is equal to the sum of the squared differences\n",
    "    #     between chat and chat0 divided by the sum of the squared values of chat0\n",
    "\n",
    "        v1 = diff.flatten(order = 'F')  # vectorise columns\n",
    "        v2 = chat0.flatten(order = 'F')\n",
    "\n",
    "        err = (np.dot(v1.T, v1) / np.dot(v2.T, v2))\n",
    "        chat0 = chat     #   Set chat0 equal to the current chat\n",
    "\n",
    "        if it == maxit:                     #  Produce warning if maximum number of iterations is reached\n",
    "            print('Maximum number of iterations reached in EM algorithm')\n",
    "\n",
    "    #  -------------------------------------------------------------------------\n",
    "    #  FINAL DIFFERENCE\n",
    "    #  Calculate the difference between the initial dataset and the values\n",
    "    #  predicted by the final set of factors\n",
    "    pred = chat*sdt + mut\n",
    "    ehat = X - pred\n",
    "    # ehat = X - chat*sdt + mut\n",
    "\n",
    "    return pred, ehat, Fhat, lamhat, ve2, X2\n",
    "\n",
    "# Parameter recomendation to use the function:\n",
    "# if __name__ == \"__main__\":\n",
    "#     X = pd.read_csv('../../data/2019-07-transformed-removed-outliers.csv', index_col=0)     # read in data\n",
    "#     kmax = 7\n",
    "#     jj = 2\n",
    "#     DEMEAN = 2\n",
    "\n",
    "# This is how you call the function:\n",
    "# pred, ehat, Fhat, lamhat, ve2, x2 = factors_em(data, kmax, jj, DEMEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: obj 99999 IC 4 \n",
      "\n",
      "Iteration 2: obj 0.0025317725751776736 IC 4 \n",
      "\n",
      "Iteration 3: obj 0.0004921392558116904 IC 4 \n",
      "\n",
      "Iteration 4: obj 0.000210655543513058 IC 4 \n",
      "\n",
      "Iteration 5: obj 0.00010112948959606032 IC 4 \n",
      "\n",
      "Iteration 6: obj 4.974121963350118e-05 IC 4 \n",
      "\n",
      "Iteration 7: obj 2.478728525915622e-05 IC 4 \n",
      "\n",
      "Iteration 8: obj 1.2497406647953186e-05 IC 4 \n",
      "\n",
      "Iteration 9: obj 6.38919359278039e-06 IC 4 \n",
      "\n",
      "Iteration 10: obj 3.3289651757412197e-06 IC 4 \n",
      "\n",
      "Iteration 11: obj 1.7820804613492796e-06 IC 4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction, error, factors_EM, loadings, ve2, new_data = factors_em(df, 4, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-01-01</th>\n",
       "      <td>0.322199</td>\n",
       "      <td>0.109461</td>\n",
       "      <td>-0.802535</td>\n",
       "      <td>0.224872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-02-01</th>\n",
       "      <td>0.471015</td>\n",
       "      <td>-0.030114</td>\n",
       "      <td>-0.738222</td>\n",
       "      <td>0.215911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-03-01</th>\n",
       "      <td>0.571404</td>\n",
       "      <td>-0.148511</td>\n",
       "      <td>-0.768551</td>\n",
       "      <td>0.170623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-04-01</th>\n",
       "      <td>0.673570</td>\n",
       "      <td>-0.204205</td>\n",
       "      <td>-0.690391</td>\n",
       "      <td>0.043746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-05-01</th>\n",
       "      <td>0.565122</td>\n",
       "      <td>-0.027223</td>\n",
       "      <td>-0.673164</td>\n",
       "      <td>0.094547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-01</th>\n",
       "      <td>-0.232212</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>0.219179</td>\n",
       "      <td>-0.167368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>-0.253665</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.251990</td>\n",
       "      <td>-0.201297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01</th>\n",
       "      <td>-0.146677</td>\n",
       "      <td>-0.153202</td>\n",
       "      <td>0.191247</td>\n",
       "      <td>-0.137116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-01</th>\n",
       "      <td>-0.084145</td>\n",
       "      <td>-0.204678</td>\n",
       "      <td>0.247372</td>\n",
       "      <td>-0.094116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>-0.028505</td>\n",
       "      <td>-0.155770</td>\n",
       "      <td>0.345603</td>\n",
       "      <td>-0.172430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3\n",
       "sasdate                                           \n",
       "1959-01-01  0.322199  0.109461 -0.802535  0.224872\n",
       "1959-02-01  0.471015 -0.030114 -0.738222  0.215911\n",
       "1959-03-01  0.571404 -0.148511 -0.768551  0.170623\n",
       "1959-04-01  0.673570 -0.204205 -0.690391  0.043746\n",
       "1959-05-01  0.565122 -0.027223 -0.673164  0.094547\n",
       "...              ...       ...       ...       ...\n",
       "2024-09-01 -0.232212  0.023076  0.219179 -0.167368\n",
       "2024-10-01 -0.253665 -0.000426  0.251990 -0.201297\n",
       "2024-11-01 -0.146677 -0.153202  0.191247 -0.137116\n",
       "2024-12-01 -0.084145 -0.204678  0.247372 -0.094116\n",
       "2025-01-01 -0.028505 -0.155770  0.345603 -0.172430\n",
       "\n",
       "[793 rows x 4 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_EM= pd.DataFrame(factors_EM, index = df.index)\n",
    "factors_EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged versions of the factors with renamed columns regarding the original complete data set\n",
    "factors_balance_0 = factors_EM.shift(6).add_suffix('_lag1')\n",
    "factors_balance_1 = factors_EM.shift(7).add_suffix('_lag2')\n",
    "factors_balance_2 = factors_EM.shift(8).add_suffix('_lag3')\n",
    "factors_balance_3 = factors_EM.shift(9).add_suffix('_lag4')\n",
    "\n",
    "# Create lagged versions of the target variable with renamed columns regarding the original complete data set\n",
    "Y_target = new_data['CMRMTSPLx']\n",
    "Y_target_1 = Y_target.shift(6).rename('Y_lag1')\n",
    "Y_target_2 = Y_target.shift(7).rename('Y_lag2')\n",
    "Y_target_3 = Y_target.shift(8).rename('Y_lag3')\n",
    "Y_target_4 = Y_target.shift(9).rename('Y_lag4')\n",
    "Y_target_5 = Y_target.shift(10).rename('Y_lag5')\n",
    "Y_target_6 = Y_target.shift(11).rename('Y_lag6')\n",
    "Y_target_7 = Y_target.shift(12).rename('Y_lag7')\n",
    "\n",
    "# ==============================================================================\n",
    "# Define train-test split \n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * len(df))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Training data:\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_1_train = factors_balance_1.iloc[:train_size]\n",
    "factors_balance_2_train = factors_balance_2.iloc[:train_size]\n",
    "factors_balance_3_train = factors_balance_3.iloc[:train_size]\n",
    "\n",
    "Y_target_train = Y_target.iloc[:train_size]\n",
    "Y_train_1 = Y_target_1.iloc[:train_size]\n",
    "Y_train_2 = Y_target_2.iloc[:train_size]\n",
    "Y_train_3 = Y_target_3.iloc[:train_size]\n",
    "Y_train_4 = Y_target_4.iloc[:train_size]\n",
    "Y_train_5 = Y_target_5.iloc[:train_size]\n",
    "Y_train_6 = Y_target_6.iloc[:train_size]\n",
    "Y_train_7 = Y_target_7.iloc[:train_size]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Testing data:\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]\n",
    "factors_balance_1_test = factors_balance_1.iloc[train_size:]\n",
    "factors_balance_2_test = factors_balance_2.iloc[train_size:]\n",
    "factors_balance_3_test = factors_balance_3.iloc[train_size:]\n",
    "\n",
    "Y_target_test = Y_target.iloc[train_size:]\n",
    "Y_test_1 = Y_target_1.iloc[train_size:]\n",
    "Y_test_2 = Y_target_2.iloc[train_size:]\n",
    "Y_test_3 = Y_target_3.iloc[train_size:]\n",
    "Y_test_4 = Y_target_4.iloc[train_size:]\n",
    "Y_test_5 = Y_target_5.iloc[train_size:]\n",
    "Y_test_6 = Y_target_6.iloc[train_size:]\n",
    "Y_test_7 = Y_target_7.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "factors_lags = [factors_balance_1_train, factors_balance_2_train, factors_balance_3_train]\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_Lag_model, DI_AR_Lag_bic, DI_AR_Lag_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('factors_balance_0', 'Y_train_1', 'Y_train_6', 'Y_train_7')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>8.01e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:43:36</td>     <th>  Log-Likelihood:    </th> <td> -874.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   701</td>      <th>  AIC:               </th> <td>   1764.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   693</td>      <th>  BIC:               </th> <td>   1800.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -0.0005</td> <td>    0.032</td> <td>   -0.017</td> <td> 0.987</td> <td>   -0.064</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th> <td>   -0.1696</td> <td>    0.089</td> <td>   -1.898</td> <td> 0.058</td> <td>   -0.345</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th> <td>    0.1089</td> <td>    0.118</td> <td>    0.924</td> <td> 0.356</td> <td>   -0.123</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th> <td>   -0.2587</td> <td>    0.107</td> <td>   -2.415</td> <td> 0.016</td> <td>   -0.469</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th> <td>    0.2505</td> <td>    0.147</td> <td>    1.703</td> <td> 0.089</td> <td>   -0.038</td> <td>    0.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag1</th> <td>    0.1217</td> <td>    0.043</td> <td>    2.835</td> <td> 0.005</td> <td>    0.037</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th> <td>    0.1334</td> <td>    0.038</td> <td>    3.521</td> <td> 0.000</td> <td>    0.059</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag7</th> <td>    0.0293</td> <td>    0.038</td> <td>    0.773</td> <td> 0.440</td> <td>   -0.045</td> <td>    0.104</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>23.740</td> <th>  Durbin-Watson:     </th> <td>   2.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  49.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.161</td> <th>  Prob(JB):          </th> <td>1.86e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.260</td> <th>  Cond. No.          </th> <td>    4.82</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.050   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.041   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     5.225   \\\\\n",
       "\\textbf{Date:}             & Fri, 28 Mar 2025 & \\textbf{  Prob (F-statistic):} &  8.01e-06   \\\\\n",
       "\\textbf{Time:}             &     18:43:36     & \\textbf{  Log-Likelihood:    } &   -874.00   \\\\\n",
       "\\textbf{No. Observations:} &         701      & \\textbf{  AIC:               } &     1764.   \\\\\n",
       "\\textbf{Df Residuals:}     &         693      & \\textbf{  BIC:               } &     1800.   \\\\\n",
       "\\textbf{Df Model:}         &           7      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &      -0.0005  &        0.032     &    -0.017  &         0.987        &       -0.064    &        0.063     \\\\\n",
       "\\textbf{0\\_lag1} &      -0.1696  &        0.089     &    -1.898  &         0.058        &       -0.345    &        0.006     \\\\\n",
       "\\textbf{1\\_lag1} &       0.1089  &        0.118     &     0.924  &         0.356        &       -0.123    &        0.341     \\\\\n",
       "\\textbf{2\\_lag1} &      -0.2587  &        0.107     &    -2.415  &         0.016        &       -0.469    &       -0.048     \\\\\n",
       "\\textbf{3\\_lag1} &       0.2505  &        0.147     &     1.703  &         0.089        &       -0.038    &        0.539     \\\\\n",
       "\\textbf{Y\\_lag1} &       0.1217  &        0.043     &     2.835  &         0.005        &        0.037    &        0.206     \\\\\n",
       "\\textbf{Y\\_lag6} &       0.1334  &        0.038     &     3.521  &         0.000        &        0.059    &        0.208     \\\\\n",
       "\\textbf{Y\\_lag7} &       0.0293  &        0.038     &     0.773  &         0.440        &       -0.045    &        0.104     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 23.740 & \\textbf{  Durbin-Watson:     } &    2.437  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   49.417  \\\\\n",
       "\\textbf{Skew:}          & -0.161 & \\textbf{  Prob(JB):          } & 1.86e-11  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.260 & \\textbf{  Cond. No.          } &     4.82  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.050\n",
       "Model:                            OLS   Adj. R-squared:                  0.041\n",
       "Method:                 Least Squares   F-statistic:                     5.225\n",
       "Date:                Fri, 28 Mar 2025   Prob (F-statistic):           8.01e-06\n",
       "Time:                        18:43:36   Log-Likelihood:                -874.00\n",
       "No. Observations:                 701   AIC:                             1764.\n",
       "Df Residuals:                     693   BIC:                             1800.\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0005      0.032     -0.017      0.987      -0.064       0.063\n",
       "0_lag1        -0.1696      0.089     -1.898      0.058      -0.345       0.006\n",
       "1_lag1         0.1089      0.118      0.924      0.356      -0.123       0.341\n",
       "2_lag1        -0.2587      0.107     -2.415      0.016      -0.469      -0.048\n",
       "3_lag1         0.2505      0.147      1.703      0.089      -0.038       0.539\n",
       "Y_lag1         0.1217      0.043      2.835      0.005       0.037       0.206\n",
       "Y_lag6         0.1334      0.038      3.521      0.000       0.059       0.208\n",
       "Y_lag7         0.0293      0.038      0.773      0.440      -0.045       0.104\n",
       "==============================================================================\n",
       "Omnibus:                       23.740   Durbin-Watson:                   2.437\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               49.417\n",
       "Skew:                          -0.161   Prob(JB):                     1.86e-11\n",
       "Kurtosis:                       4.260   Cond. No.                         4.82\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_Lag_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Call:\n",
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_1, Y_test_6, Y_test_7]\n",
    "\n",
    "DI_AR_Lag_forecast, DI_AR_Lag_MSE, DI_AR_Lag_se = direct_forecast(DI_AR_Lag_model, factors_lags_test, Y_lags_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.757\n",
      "1.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"{DI_AR_Lag_MSE:.3f}\")\n",
    "print(f\"{DI_AR_Lag_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI-AR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: obj 99999 IC 11 \n",
      "\n",
      "Iteration 2: obj 0.0314715521772786 IC 12 \n",
      "\n",
      "Iteration 3: obj 0.0014176281867848432 IC 12 \n",
      "\n",
      "Iteration 4: obj 0.000521235157445075 IC 12 \n",
      "\n",
      "Iteration 5: obj 0.00029427306473284615 IC 12 \n",
      "\n",
      "Iteration 6: obj 0.0001871384629042762 IC 12 \n",
      "\n",
      "Iteration 7: obj 0.0001235994515225643 IC 12 \n",
      "\n",
      "Iteration 8: obj 8.323060641058887e-05 IC 12 \n",
      "\n",
      "Iteration 9: obj 5.68392966538867e-05 IC 12 \n",
      "\n",
      "Iteration 10: obj 3.926701649208046e-05 IC 12 \n",
      "\n",
      "Iteration 11: obj 2.7401423426853256e-05 IC 12 \n",
      "\n",
      "Iteration 12: obj 1.9298025946677208e-05 IC 12 \n",
      "\n",
      "Iteration 13: obj 1.3711142503875397e-05 IC 12 \n",
      "\n",
      "Iteration 14: obj 9.826937632325056e-06 IC 12 \n",
      "\n",
      "Iteration 15: obj 7.10554172554484e-06 IC 12 \n",
      "\n",
      "Iteration 16: obj 5.1845588960052654e-06 IC 12 \n",
      "\n",
      "Iteration 17: obj 3.818429841591914e-06 IC 12 \n",
      "\n",
      "Iteration 18: obj 2.8394744068563416e-06 IC 12 \n",
      "\n",
      "Iteration 19: obj 2.1324312817387194e-06 IC 12 \n",
      "\n",
      "Iteration 20: obj 1.6175975752726948e-06 IC 12 \n",
      "\n",
      "Iteration 21: obj 1.239547761140906e-06 IC 12 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-01-01</th>\n",
       "      <td>0.337074</td>\n",
       "      <td>0.259493</td>\n",
       "      <td>-0.601780</td>\n",
       "      <td>-0.254682</td>\n",
       "      <td>0.133429</td>\n",
       "      <td>-0.176245</td>\n",
       "      <td>0.119199</td>\n",
       "      <td>-0.003612</td>\n",
       "      <td>-0.158703</td>\n",
       "      <td>0.097825</td>\n",
       "      <td>-0.028648</td>\n",
       "      <td>-0.068514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-02-01</th>\n",
       "      <td>0.467045</td>\n",
       "      <td>0.030410</td>\n",
       "      <td>-0.732714</td>\n",
       "      <td>-0.231480</td>\n",
       "      <td>0.112558</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.116651</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.124465</td>\n",
       "      <td>-0.024694</td>\n",
       "      <td>0.045301</td>\n",
       "      <td>0.086812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-03-01</th>\n",
       "      <td>0.558539</td>\n",
       "      <td>-0.074097</td>\n",
       "      <td>-0.771316</td>\n",
       "      <td>-0.199885</td>\n",
       "      <td>0.034734</td>\n",
       "      <td>0.141346</td>\n",
       "      <td>0.094365</td>\n",
       "      <td>0.075782</td>\n",
       "      <td>-0.006850</td>\n",
       "      <td>-0.019643</td>\n",
       "      <td>-0.125192</td>\n",
       "      <td>0.055999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-04-01</th>\n",
       "      <td>0.652390</td>\n",
       "      <td>-0.116903</td>\n",
       "      <td>-0.697884</td>\n",
       "      <td>-0.066040</td>\n",
       "      <td>0.081268</td>\n",
       "      <td>0.088826</td>\n",
       "      <td>0.147811</td>\n",
       "      <td>0.213209</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>-0.149813</td>\n",
       "      <td>0.059242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-05-01</th>\n",
       "      <td>0.548440</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>-0.669698</td>\n",
       "      <td>-0.114636</td>\n",
       "      <td>0.161208</td>\n",
       "      <td>0.065662</td>\n",
       "      <td>0.078385</td>\n",
       "      <td>0.088052</td>\n",
       "      <td>0.005547</td>\n",
       "      <td>0.034406</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.104428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-01</th>\n",
       "      <td>-0.222516</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.223589</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>-0.269808</td>\n",
       "      <td>0.227958</td>\n",
       "      <td>0.067117</td>\n",
       "      <td>-0.164296</td>\n",
       "      <td>-0.068867</td>\n",
       "      <td>-0.138303</td>\n",
       "      <td>-0.086685</td>\n",
       "      <td>-0.089568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>-0.253799</td>\n",
       "      <td>-0.028026</td>\n",
       "      <td>0.258609</td>\n",
       "      <td>0.196773</td>\n",
       "      <td>0.127103</td>\n",
       "      <td>0.073735</td>\n",
       "      <td>-0.045019</td>\n",
       "      <td>-0.171540</td>\n",
       "      <td>-0.197356</td>\n",
       "      <td>0.122581</td>\n",
       "      <td>-0.015612</td>\n",
       "      <td>-0.222725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-01</th>\n",
       "      <td>-0.147417</td>\n",
       "      <td>-0.170666</td>\n",
       "      <td>0.189869</td>\n",
       "      <td>0.139032</td>\n",
       "      <td>0.115131</td>\n",
       "      <td>0.130912</td>\n",
       "      <td>-0.098716</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>-0.118676</td>\n",
       "      <td>0.126470</td>\n",
       "      <td>0.036473</td>\n",
       "      <td>-0.198835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-01</th>\n",
       "      <td>-0.082989</td>\n",
       "      <td>-0.223450</td>\n",
       "      <td>0.237876</td>\n",
       "      <td>0.082914</td>\n",
       "      <td>-0.033790</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.060009</td>\n",
       "      <td>-0.084594</td>\n",
       "      <td>-0.101390</td>\n",
       "      <td>0.132551</td>\n",
       "      <td>0.171672</td>\n",
       "      <td>-0.052743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>-0.040166</td>\n",
       "      <td>-0.159699</td>\n",
       "      <td>0.344571</td>\n",
       "      <td>0.183642</td>\n",
       "      <td>0.153924</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>0.074898</td>\n",
       "      <td>0.077547</td>\n",
       "      <td>0.064631</td>\n",
       "      <td>0.122908</td>\n",
       "      <td>-0.018384</td>\n",
       "      <td>-0.122761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "sasdate                                                                  \n",
       "1959-01-01  0.337074  0.259493 -0.601780 -0.254682  0.133429 -0.176245   \n",
       "1959-02-01  0.467045  0.030410 -0.732714 -0.231480  0.112558  0.066960   \n",
       "1959-03-01  0.558539 -0.074097 -0.771316 -0.199885  0.034734  0.141346   \n",
       "1959-04-01  0.652390 -0.116903 -0.697884 -0.066040  0.081268  0.088826   \n",
       "1959-05-01  0.548440  0.043496 -0.669698 -0.114636  0.161208  0.065662   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-09-01 -0.222516  0.002791  0.223589  0.131862 -0.269808  0.227958   \n",
       "2024-10-01 -0.253799 -0.028026  0.258609  0.196773  0.127103  0.073735   \n",
       "2024-11-01 -0.147417 -0.170666  0.189869  0.139032  0.115131  0.130912   \n",
       "2024-12-01 -0.082989 -0.223450  0.237876  0.082914 -0.033790  0.076763   \n",
       "2025-01-01 -0.040166 -0.159699  0.344571  0.183642  0.153924 -0.001019   \n",
       "\n",
       "                  6         7         8         9         10        11  \n",
       "sasdate                                                                 \n",
       "1959-01-01  0.119199 -0.003612 -0.158703  0.097825 -0.028648 -0.068514  \n",
       "1959-02-01  0.116651  0.007326  0.124465 -0.024694  0.045301  0.086812  \n",
       "1959-03-01  0.094365  0.075782 -0.006850 -0.019643 -0.125192  0.055999  \n",
       "1959-04-01  0.147811  0.213209  0.066378  0.003598 -0.149813  0.059242  \n",
       "1959-05-01  0.078385  0.088052  0.005547  0.034406  0.040574  0.104428  \n",
       "...              ...       ...       ...       ...       ...       ...  \n",
       "2024-09-01  0.067117 -0.164296 -0.068867 -0.138303 -0.086685 -0.089568  \n",
       "2024-10-01 -0.045019 -0.171540 -0.197356  0.122581 -0.015612 -0.222725  \n",
       "2024-11-01 -0.098716 -0.039876 -0.118676  0.126470  0.036473 -0.198835  \n",
       "2024-12-01  0.060009 -0.084594 -0.101390  0.132551  0.171672 -0.052743  \n",
       "2025-01-01  0.074898  0.077547  0.064631  0.122908 -0.018384 -0.122761  \n",
       "\n",
       "[793 rows x 12 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction, error, factors_EM, loadings, ve2, new_data = factors_em(df, 12, 2, 2)\n",
    "factors_EM= pd.DataFrame(factors_EM, index = df.index)\n",
    "factors_EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_balance_0 = factors_EM.shift(6).add_suffix('_lag1')\n",
    "factors_balance_0_train = factors_balance_0.iloc[:train_size]\n",
    "factors_balance_0_test = factors_balance_0.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = [Y_train_1, Y_train_2, Y_train_3, Y_train_4, Y_train_5, Y_train_6, Y_train_7]\n",
    "\n",
    "DI_AR_model, DI_AR_bic, DI_AR_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>1.57e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:45:02</td>     <th>  Log-Likelihood:    </th> <td> -861.01</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   701</td>      <th>  AIC:               </th> <td>   1754.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   685</td>      <th>  BIC:               </th> <td>   1827.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    15</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   -0.0019</td> <td>    0.032</td> <td>   -0.060</td> <td> 0.952</td> <td>   -0.065</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.2024</td> <td>    0.094</td> <td>   -2.154</td> <td> 0.032</td> <td>   -0.387</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>    0.1978</td> <td>    0.129</td> <td>    1.533</td> <td> 0.126</td> <td>   -0.056</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>   -0.2226</td> <td>    0.109</td> <td>   -2.041</td> <td> 0.042</td> <td>   -0.437</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.1240</td> <td>    0.161</td> <td>   -0.769</td> <td> 0.442</td> <td>   -0.441</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>    0.0318</td> <td>    0.151</td> <td>    0.210</td> <td> 0.834</td> <td>   -0.265</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.6162</td> <td>    0.174</td> <td>   -3.550</td> <td> 0.000</td> <td>   -0.957</td> <td>   -0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>    0.1879</td> <td>    0.197</td> <td>    0.955</td> <td> 0.340</td> <td>   -0.198</td> <td>    0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>   -0.1717</td> <td>    0.230</td> <td>   -0.747</td> <td> 0.455</td> <td>   -0.623</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>    0.1789</td> <td>    0.252</td> <td>    0.710</td> <td> 0.478</td> <td>   -0.316</td> <td>    0.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>    0.3790</td> <td>    0.215</td> <td>    1.760</td> <td> 0.079</td> <td>   -0.044</td> <td>    0.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>   -0.6871</td> <td>    0.249</td> <td>   -2.763</td> <td> 0.006</td> <td>   -1.175</td> <td>   -0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>    0.0978</td> <td>    0.253</td> <td>    0.387</td> <td> 0.699</td> <td>   -0.399</td> <td>    0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag1</th>  <td>    0.1566</td> <td>    0.051</td> <td>    3.078</td> <td> 0.002</td> <td>    0.057</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag6</th>  <td>    0.1217</td> <td>    0.039</td> <td>    3.160</td> <td> 0.002</td> <td>    0.046</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Y_lag7</th>  <td>    0.0216</td> <td>    0.038</td> <td>    0.562</td> <td> 0.574</td> <td>   -0.054</td> <td>    0.097</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>23.803</td> <th>  Durbin-Watson:     </th> <td>   2.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  53.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.122</td> <th>  Prob(JB):          </th> <td>3.08e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.325</td> <th>  Cond. No.          </th> <td>    9.30</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.085   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.065   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     4.226   \\\\\n",
       "\\textbf{Date:}             & Fri, 28 Mar 2025 & \\textbf{  Prob (F-statistic):} &  1.57e-07   \\\\\n",
       "\\textbf{Time:}             &     18:45:02     & \\textbf{  Log-Likelihood:    } &   -861.01   \\\\\n",
       "\\textbf{No. Observations:} &         701      & \\textbf{  AIC:               } &     1754.   \\\\\n",
       "\\textbf{Df Residuals:}     &         685      & \\textbf{  BIC:               } &     1827.   \\\\\n",
       "\\textbf{Df Model:}         &          15      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &      -0.0019  &        0.032     &    -0.060  &         0.952        &       -0.065    &        0.061     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.2024  &        0.094     &    -2.154  &         0.032        &       -0.387    &       -0.018     \\\\\n",
       "\\textbf{1\\_lag1}  &       0.1978  &        0.129     &     1.533  &         0.126        &       -0.056    &        0.451     \\\\\n",
       "\\textbf{2\\_lag1}  &      -0.2226  &        0.109     &    -2.041  &         0.042        &       -0.437    &       -0.008     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.1240  &        0.161     &    -0.769  &         0.442        &       -0.441    &        0.193     \\\\\n",
       "\\textbf{4\\_lag1}  &       0.0318  &        0.151     &     0.210  &         0.834        &       -0.265    &        0.328     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.6162  &        0.174     &    -3.550  &         0.000        &       -0.957    &       -0.275     \\\\\n",
       "\\textbf{6\\_lag1}  &       0.1879  &        0.197     &     0.955  &         0.340        &       -0.198    &        0.574     \\\\\n",
       "\\textbf{7\\_lag1}  &      -0.1717  &        0.230     &    -0.747  &         0.455        &       -0.623    &        0.280     \\\\\n",
       "\\textbf{8\\_lag1}  &       0.1789  &        0.252     &     0.710  &         0.478        &       -0.316    &        0.673     \\\\\n",
       "\\textbf{9\\_lag1}  &       0.3790  &        0.215     &     1.760  &         0.079        &       -0.044    &        0.802     \\\\\n",
       "\\textbf{10\\_lag1} &      -0.6871  &        0.249     &    -2.763  &         0.006        &       -1.175    &       -0.199     \\\\\n",
       "\\textbf{11\\_lag1} &       0.0978  &        0.253     &     0.387  &         0.699        &       -0.399    &        0.594     \\\\\n",
       "\\textbf{Y\\_lag1}  &       0.1566  &        0.051     &     3.078  &         0.002        &        0.057    &        0.257     \\\\\n",
       "\\textbf{Y\\_lag6}  &       0.1217  &        0.039     &     3.160  &         0.002        &        0.046    &        0.197     \\\\\n",
       "\\textbf{Y\\_lag7}  &       0.0216  &        0.038     &     0.562  &         0.574        &       -0.054    &        0.097     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 23.803 & \\textbf{  Durbin-Watson:     } &    2.482  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   53.014  \\\\\n",
       "\\textbf{Skew:}          & -0.122 & \\textbf{  Prob(JB):          } & 3.08e-12  \\\\\n",
       "\\textbf{Kurtosis:}      &  4.325 & \\textbf{  Cond. No.          } &     9.30  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.085\n",
       "Model:                            OLS   Adj. R-squared:                  0.065\n",
       "Method:                 Least Squares   F-statistic:                     4.226\n",
       "Date:                Fri, 28 Mar 2025   Prob (F-statistic):           1.57e-07\n",
       "Time:                        18:45:02   Log-Likelihood:                -861.01\n",
       "No. Observations:                 701   AIC:                             1754.\n",
       "Df Residuals:                     685   BIC:                             1827.\n",
       "Df Model:                          15                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0019      0.032     -0.060      0.952      -0.065       0.061\n",
       "0_lag1        -0.2024      0.094     -2.154      0.032      -0.387      -0.018\n",
       "1_lag1         0.1978      0.129      1.533      0.126      -0.056       0.451\n",
       "2_lag1        -0.2226      0.109     -2.041      0.042      -0.437      -0.008\n",
       "3_lag1        -0.1240      0.161     -0.769      0.442      -0.441       0.193\n",
       "4_lag1         0.0318      0.151      0.210      0.834      -0.265       0.328\n",
       "5_lag1        -0.6162      0.174     -3.550      0.000      -0.957      -0.275\n",
       "6_lag1         0.1879      0.197      0.955      0.340      -0.198       0.574\n",
       "7_lag1        -0.1717      0.230     -0.747      0.455      -0.623       0.280\n",
       "8_lag1         0.1789      0.252      0.710      0.478      -0.316       0.673\n",
       "9_lag1         0.3790      0.215      1.760      0.079      -0.044       0.802\n",
       "10_lag1       -0.6871      0.249     -2.763      0.006      -1.175      -0.199\n",
       "11_lag1        0.0978      0.253      0.387      0.699      -0.399       0.594\n",
       "Y_lag1         0.1566      0.051      3.078      0.002       0.057       0.257\n",
       "Y_lag6         0.1217      0.039      3.160      0.002       0.046       0.197\n",
       "Y_lag7         0.0216      0.038      0.562      0.574      -0.054       0.097\n",
       "==============================================================================\n",
       "Omnibus:                       23.803   Durbin-Watson:                   2.482\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               53.014\n",
       "Skew:                          -0.122   Prob(JB):                     3.08e-12\n",
       "Kurtosis:                       4.325   Cond. No.                         9.30\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_AR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.774\n",
      "1.00\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = [Y_test_1, Y_test_6, Y_test_7]\n",
    "DI_AR_forecast, DI_AR_MSE, DI_AR_se = direct_forecast(DI_AR_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_AR_MSE:.3f}\")\n",
    "print(f\"{DI_AR_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: DI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "factors_lags = []\n",
    "Y_lags = []\n",
    "\n",
    "DI_model, DI_bic, DI_specification = best_bic_model(Y_target_train, factors_balance_0_train, factors_lags, Y_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>CMRMTSPLx</td>    <th>  R-squared:         </th> <td>   0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Mar 2025</td> <th>  Prob (F-statistic):</th> <td>0.000291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:46:07</td>     <th>  Log-Likelihood:    </th> <td> -907.84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   707</td>      <th>  AIC:               </th> <td>   1842.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   694</td>      <th>  BIC:               </th> <td>   1901.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   -0.0017</td> <td>    0.034</td> <td>   -0.049</td> <td> 0.961</td> <td>   -0.068</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>0_lag1</th>  <td>   -0.0757</td> <td>    0.089</td> <td>   -0.846</td> <td> 0.398</td> <td>   -0.251</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1_lag1</th>  <td>   -0.0624</td> <td>    0.109</td> <td>   -0.570</td> <td> 0.569</td> <td>   -0.277</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2_lag1</th>  <td>   -0.2935</td> <td>    0.108</td> <td>   -2.715</td> <td> 0.007</td> <td>   -0.506</td> <td>   -0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3_lag1</th>  <td>   -0.1982</td> <td>    0.150</td> <td>   -1.319</td> <td> 0.188</td> <td>   -0.493</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4_lag1</th>  <td>   -0.1254</td> <td>    0.151</td> <td>   -0.829</td> <td> 0.408</td> <td>   -0.423</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5_lag1</th>  <td>   -0.5121</td> <td>    0.175</td> <td>   -2.932</td> <td> 0.003</td> <td>   -0.855</td> <td>   -0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6_lag1</th>  <td>    0.0680</td> <td>    0.193</td> <td>    0.352</td> <td> 0.725</td> <td>   -0.311</td> <td>    0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7_lag1</th>  <td>   -0.2743</td> <td>    0.229</td> <td>   -1.196</td> <td> 0.232</td> <td>   -0.725</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8_lag1</th>  <td>   -0.0313</td> <td>    0.256</td> <td>   -0.122</td> <td> 0.903</td> <td>   -0.534</td> <td>    0.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9_lag1</th>  <td>    0.4030</td> <td>    0.227</td> <td>    1.777</td> <td> 0.076</td> <td>   -0.042</td> <td>    0.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10_lag1</th> <td>   -0.7378</td> <td>    0.261</td> <td>   -2.823</td> <td> 0.005</td> <td>   -1.251</td> <td>   -0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11_lag1</th> <td>    0.2282</td> <td>    0.262</td> <td>    0.870</td> <td> 0.385</td> <td>   -0.287</td> <td>    0.743</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>50.777</td> <th>  Durbin-Watson:     </th> <td>   2.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 175.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.244</td> <th>  Prob(JB):          </th> <td>8.27e-39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.391</td> <th>  Cond. No.          </th> <td>    8.76</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    CMRMTSPLx     & \\textbf{  R-squared:         } &     0.051   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.034   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     3.082   \\\\\n",
       "\\textbf{Date:}             & Fri, 28 Mar 2025 & \\textbf{  Prob (F-statistic):} &  0.000291   \\\\\n",
       "\\textbf{Time:}             &     18:46:07     & \\textbf{  Log-Likelihood:    } &   -907.84   \\\\\n",
       "\\textbf{No. Observations:} &         707      & \\textbf{  AIC:               } &     1842.   \\\\\n",
       "\\textbf{Df Residuals:}     &         694      & \\textbf{  BIC:               } &     1901.   \\\\\n",
       "\\textbf{Df Model:}         &          12      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &      -0.0017  &        0.034     &    -0.049  &         0.961        &       -0.068    &        0.064     \\\\\n",
       "\\textbf{0\\_lag1}  &      -0.0757  &        0.089     &    -0.846  &         0.398        &       -0.251    &        0.100     \\\\\n",
       "\\textbf{1\\_lag1}  &      -0.0624  &        0.109     &    -0.570  &         0.569        &       -0.277    &        0.152     \\\\\n",
       "\\textbf{2\\_lag1}  &      -0.2935  &        0.108     &    -2.715  &         0.007        &       -0.506    &       -0.081     \\\\\n",
       "\\textbf{3\\_lag1}  &      -0.1982  &        0.150     &    -1.319  &         0.188        &       -0.493    &        0.097     \\\\\n",
       "\\textbf{4\\_lag1}  &      -0.1254  &        0.151     &    -0.829  &         0.408        &       -0.423    &        0.172     \\\\\n",
       "\\textbf{5\\_lag1}  &      -0.5121  &        0.175     &    -2.932  &         0.003        &       -0.855    &       -0.169     \\\\\n",
       "\\textbf{6\\_lag1}  &       0.0680  &        0.193     &     0.352  &         0.725        &       -0.311    &        0.447     \\\\\n",
       "\\textbf{7\\_lag1}  &      -0.2743  &        0.229     &    -1.196  &         0.232        &       -0.725    &        0.176     \\\\\n",
       "\\textbf{8\\_lag1}  &      -0.0313  &        0.256     &    -0.122  &         0.903        &       -0.534    &        0.472     \\\\\n",
       "\\textbf{9\\_lag1}  &       0.4030  &        0.227     &     1.777  &         0.076        &       -0.042    &        0.848     \\\\\n",
       "\\textbf{10\\_lag1} &      -0.7378  &        0.261     &    -2.823  &         0.005        &       -1.251    &       -0.225     \\\\\n",
       "\\textbf{11\\_lag1} &       0.2282  &        0.262     &     0.870  &         0.385        &       -0.287    &        0.743     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 50.777 & \\textbf{  Durbin-Watson:     } &    2.517  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &  175.376  \\\\\n",
       "\\textbf{Skew:}          & -0.244 & \\textbf{  Prob(JB):          } & 8.27e-39  \\\\\n",
       "\\textbf{Kurtosis:}      &  5.391 & \\textbf{  Cond. No.          } &     8.76  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              CMRMTSPLx   R-squared:                       0.051\n",
       "Model:                            OLS   Adj. R-squared:                  0.034\n",
       "Method:                 Least Squares   F-statistic:                     3.082\n",
       "Date:                Fri, 28 Mar 2025   Prob (F-statistic):           0.000291\n",
       "Time:                        18:46:07   Log-Likelihood:                -907.84\n",
       "No. Observations:                 707   AIC:                             1842.\n",
       "Df Residuals:                     694   BIC:                             1901.\n",
       "Df Model:                          12                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0017      0.034     -0.049      0.961      -0.068       0.064\n",
       "0_lag1        -0.0757      0.089     -0.846      0.398      -0.251       0.100\n",
       "1_lag1        -0.0624      0.109     -0.570      0.569      -0.277       0.152\n",
       "2_lag1        -0.2935      0.108     -2.715      0.007      -0.506      -0.081\n",
       "3_lag1        -0.1982      0.150     -1.319      0.188      -0.493       0.097\n",
       "4_lag1        -0.1254      0.151     -0.829      0.408      -0.423       0.172\n",
       "5_lag1        -0.5121      0.175     -2.932      0.003      -0.855      -0.169\n",
       "6_lag1         0.0680      0.193      0.352      0.725      -0.311       0.447\n",
       "7_lag1        -0.2743      0.229     -1.196      0.232      -0.725       0.176\n",
       "8_lag1        -0.0313      0.256     -0.122      0.903      -0.534       0.472\n",
       "9_lag1         0.4030      0.227      1.777      0.076      -0.042       0.848\n",
       "10_lag1       -0.7378      0.261     -2.823      0.005      -1.251      -0.225\n",
       "11_lag1        0.2282      0.262      0.870      0.385      -0.287       0.743\n",
       "==============================================================================\n",
       "Omnibus:                       50.777   Durbin-Watson:                   2.517\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              175.376\n",
       "Skew:                          -0.244   Prob(JB):                     8.27e-39\n",
       "Kurtosis:                       5.391   Cond. No.                         8.76\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DI_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.712\n",
      "1.00\n"
     ]
    }
   ],
   "source": [
    "factors_lags_test = [factors_balance_0_test]\n",
    "Y_lags_test = []\n",
    "DI_forecast, DI_MSE, DI_se = direct_forecast(DI_model, factors_lags_test, Y_lags_test, Y_target_test)\n",
    "print(f\"{DI_MSE:.3f}\")\n",
    "print(f\"{DI_se:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_lagged = new_data.shift(6)\n",
    "balanced_lagged_train = balanced_lagged.iloc[:train_size].dropna()\n",
    "balanced_lagged_test = balanced_lagged.iloc[train_size:]\n",
    "Y_target_train_adjusted = Y_target_train.loc[balanced_lagged_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LASSO Regression...\n",
      "Best model parameters: {'alpha': np.float64(0.029470517025518096), 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=1, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.714\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_lasso, mse_lasso = direct_forecast_2(model_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ElasticNet Regression...\n",
      "Best Elastic Net Alpha: 0.30888435964774846\n",
      "Best Elastic Net l1_ratio: 0.1\n",
      "Best model parameters: {'alpha': np.float64(0.30888435964774846), 'copy_X': True, 'fit_intercept': True, 'l1_ratio': np.float64(0.1), 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_elastic_net = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=2, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.963\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_elastic_net, mse_elastic_net = direct_forecast_2(model_elastic_net, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some variables in the dataset were not assigned to any group.\n"
     ]
    }
   ],
   "source": [
    "# function call\n",
    "groups = construct_groups(balanced_lagged_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Group  Count  Proportion\n",
      "0      0      1    0.007937\n",
      "1      1     15    0.119048\n",
      "2      2     31    0.246032\n",
      "3      3     10    0.079365\n",
      "4      4     10    0.079365\n",
      "5      5     13    0.103175\n",
      "6      6     22    0.174603\n",
      "7      7     20    0.158730\n",
      "8      8      4    0.031746\n"
     ]
    }
   ],
   "source": [
    "group_proportions = compute_group_proportions(groups)\n",
    "print(group_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Group LASSO Regression...\n",
      "Best model parameters: {'fit_intercept': False, 'frobenius_lipschitz': False, 'group_reg': 0.05, 'groups': array([1, 1, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "       7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 4, 5, 5, 5, 8]), 'l1_reg': np.float64(1e-05), 'n_iter': 100, 'old_regularisation': False, 'random_state': None, 'scale_reg': 'group_size', 'subsampling_scheme': None, 'supress_warning': True, 'tol': 1e-05, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model_group_lasso = time_series_regression(dependent_var = Y_target_train_adjusted, regressors = balanced_lagged_train, \n",
    "                               regression_type=3, alpha_range=alpha_range, l1_ratio_range=l1_ratio_range, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.681\n"
     ]
    }
   ],
   "source": [
    "# Now use the model to forecast on the test data\n",
    "forecasted_group_lasso, mse_group_lasso = direct_forecast_2(model_group_lasso, balanced_lagged_test, Y_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecast combining regression** with Heteroskedasticity and Autocorrelation Robust (HAC) standard errors using the Newey-West estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: -0.17\n",
      "P-value: 0.56\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_AR_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.07\n",
      "P-value: 0.81\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(DI_forecast, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.04\n",
      "P-value: 0.93\n"
     ]
    }
   ],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_elastic_net, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hat, p_val = forecast_combining_regression(forecasted_group_lasso, DI_AR_Lag_forecast, Y_target_test)\n",
    "print(f\"Estimated alpha: {alpha_hat:.2f}\")\n",
    "print(f\"P-value: {p_val:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
